{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import glob\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import ast\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sentence_splitter import SentenceSplitter, split_text_into_sentences\n",
    "from transformers import PegasusForConditionalGeneration, PegasusTokenizerFast, PegasusTokenizer\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "import textwrap\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from imgaug import augmenters as iaa\n",
    "import imgaug as ia\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function below will sort the files in numberical order \n",
    "numbers = re.compile(r'(\\d+)')\n",
    "def numericalSort(value):\n",
    "    parts = numbers.split(value)\n",
    "    parts[1::2] = map(int, parts[1::2])\n",
    "    return parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "etd_path = sorted(glob.glob('/home/mchou001/etds_json/*.json'), key = numericalSort)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessor:\n",
    "    def __init__(self, json_files):\n",
    "        self.json_files = json_files\n",
    "        #self.dedication_ = dedication_\n",
    "        #self.acknowledgement_ = acknowledgement_\n",
    "        #self.general_abs = general_abs\n",
    "    \n",
    "    ## the function will return parsed lines from json files\n",
    "    def json_text_parser(self):\n",
    "        counter = 0\n",
    "        list_line = []\n",
    "        for filename in self.json_files:\n",
    "            with open(filename, encoding='utf-8', mode='r') as files:\n",
    "                data = json.loads(files.read())\n",
    "                counter = counter + 1\n",
    "                _text = []\n",
    "                for i in range(len(data)):\n",
    "                    text = data[i].get(\"Line_and_BB\")\n",
    "                    _text.append(text)\n",
    "                for i, data in enumerate(_text):\n",
    "                    line_text = [text['Line'].strip('\\n') for text in data]\n",
    "                    list_line.append(line_text)\n",
    "        \n",
    "        return list_line\n",
    "    \n",
    "    ## the function will return parsed bounding box information from json files\n",
    "    def json_bbox_parser(self):\n",
    "        counter = 0\n",
    "        list_bbox = []\n",
    "        for filename in self.json_files:\n",
    "            with open(filename, encoding='utf-8', mode='r') as files:\n",
    "                data = json.loads(files.read())\n",
    "                counter = counter + 1\n",
    "                _text = []\n",
    "                for i in range(len(data)):\n",
    "                    text = data[i].get(\"Line_and_BB\")\n",
    "                    _text.append(text)\n",
    "                for i, data in enumerate(_text):\n",
    "                    bbox_text = [bbox['Bounding Box'] for bbox in data]\n",
    "                    list_bbox.append(bbox_text)\n",
    "        \n",
    "        return list_bbox\n",
    "    \n",
    "    '''\n",
    "    We are using 'ast' module for preprocessing task of the class labels.\n",
    "    The ast module helps Python applications to process trees of the Python abstract syntax grammar.\n",
    "    We utilized the ast module to grammatically identify the sentences and applied 'sentence splitter' module \n",
    "    on paragraphs to get a list of sentences.\n",
    "    The preprocessing functions for each classes returns a list.\n",
    "    '''\n",
    "    \n",
    "    def preprocess_dedication(self, dedication_):\n",
    "        self.dedication_ = dedication_\n",
    "        text_dedication = ast.literal_eval(self.dedication_)\n",
    "        text_dedication = (\" \").join(text_dedication)\n",
    "        splitter = SentenceSplitter(language='en')\n",
    "        dedication_list = splitter.split(text_dedication)\n",
    "        return dedication_list    \n",
    "    \n",
    "    def preprocess_ack(self, acknowledgement_):\n",
    "        self.acknowledgement_ = acknowledgement_\n",
    "        text_ack = ast.literal_eval(self.acknowledgement_)\n",
    "        text_ack = (\" \").join(text_ack)\n",
    "        splitter = SentenceSplitter(language='en')\n",
    "        ack_list = splitter.split(text_ack)\n",
    "        return ack_list\n",
    "    \n",
    "    def preprocess_gabs(self, general_abs):\n",
    "        self.general_abs = general_abs\n",
    "        splitter = SentenceSplitter(language='en')\n",
    "        gabs_list = splitter.split(self.general_abs)\n",
    "        return gabs_list\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## loading the paraphrasing model\n",
    "\n",
    "#references: https://arxiv.org/abs/1912.08777\n",
    "\n",
    "model_name = 'tuner007/pegasus_paraphrase'\n",
    "torch_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
    "model = PegasusForConditionalGeneration.from_pretrained(model_name).to(torch_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Augmentation:\n",
    "    \n",
    "    def get_paraphrased_sentences(self, input_text, num_return_sequences):\n",
    "        self.input_text = input_text\n",
    "        self.num_return_sequences = num_return_sequences        \n",
    "        batch = tokenizer.prepare_seq2seq_batch([self.input_text],truncation=True,padding='longest',max_length=60, return_tensors=\"pt\").to(torch_device)\n",
    "        translated = model.generate(**batch,max_length=60,num_beams=10, num_return_sequences=self.num_return_sequences, temperature=1.5)\n",
    "        paraphrased_text = tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
    "        return paraphrased_text       \n",
    "    \n",
    "    def paraphrased_text(self, class_text_list):\n",
    "        self.class_text_list = class_text_list\n",
    "        phrases_text = []\n",
    "        for ele in self.class_text_list:\n",
    "            phrase = self.get_paraphrased_sentences(ele, 1)\n",
    "            phrases_text.append(phrase)\n",
    "        \n",
    "        para_phrases = [' '.join(x) for x in phrases_text]\n",
    "        paraphrase_ = [' '.join(x for x in para_phrases)] ## combine  the splitted lists into a paragraph\n",
    "        paraphrase_text = str(paraphrase_).strip('[]').strip(\"'\")\n",
    "        paraphrase_strip = paraphrase_text.strip('\"\"')\n",
    "        return paraphrase_strip\n",
    "    \n",
    "    def wrap_text(self, text):\n",
    "        self.text = text\n",
    "        new_phrase = textwrap.wrap(self.text, width=90)\n",
    "        string = ''\n",
    "        for ele in new_phrase[0:]:\n",
    "            string = string + ele + '\\n'\n",
    "        return string       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'Label-Dedication/03.png'\n",
    "W,H = (2360, 3200)\n",
    "def text_on_img(text, size):\n",
    "    font = ImageFont.truetype('NimbusMonoPS-Bold.otf', size)\n",
    "    image = Image.new(mode = \"RGB\", size = (W, H), color = \"white\")\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    w, h = draw.textsize(text)\n",
    "    ## Adjust the texual position in a page and draw the text on a image\n",
    "    draw.text(((W-w)/10,(H-h)/6), text, font=font, fill=(0,0,0), spacing=60) \n",
    "    image.save(file)\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mchou001/.local/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:3524: FutureWarning: \n",
      "`prepare_seq2seq_batch` is deprecated and will be removed in version 5 of HuggingFace Transformers. Use the regular\n",
      "`__call__` method to prepare your inputs and the tokenizer under the `as_target_tokenizer` context manager to prepare\n",
      "your targets.\n",
      "\n",
      "Here is a short example:\n",
      "\n",
      "model_inputs = tokenizer(src_texts, ...)\n",
      "with tokenizer.as_target_tokenizer():\n",
      "    labels = tokenizer(tgt_texts, ...)\n",
      "model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
      "\n",
      "See the documentation of your specific tokenizer for more details on the specific arguments to the tokenizer of choice.\n",
      "For a more complete example, see the implementation of `prepare_seq2seq_batch`.\n",
      "\n",
      "  warnings.warn(formatted_warning, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    parser = Preprocessor(etd_path)\n",
    "    \n",
    "    etd_lines = parser.json_text_parser()\n",
    "    etd_bbox = parser.json_bbox_parser()\n",
    "    labels_ = pd.read_csv(\"labels.csv\", encoding = 'utf-8')\n",
    "    etd_label = labels_['labels']\n",
    "    res_list = [list(item) for item in list(zip(etd_lines, etd_bbox, etd_label))]\n",
    "    dataframe = pd.DataFrame(res_list, columns = ['text', 'bbox', 'class'])\n",
    "    dataframe.to_csv('ETD_aug.csv', index = False)\n",
    "    df1 = pd.read_csv('ETD_aug.csv')\n",
    "    \n",
    "    df1.set_index(\"class\", inplace = True)\n",
    "    \n",
    "    augmentation = Augmentation()\n",
    "    \n",
    "    ## Dedication ###\n",
    "    label_dedication = df1.loc[\"Label-Dedication\"]\n",
    "    phrases_dedication = label_dedication['text']\n",
    "    dedication = parser.preprocess_dedication(phrases_dedication[2])  ## change the index values which corresponds to different ETD samples\n",
    "    dedication_paraphrase = augmentation.paraphrased_text(dedication)\n",
    "    dedication_text_wrap = augmentation.wrap_text(dedication_paraphrase)\n",
    "    dedication_title = \"\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tDedication\\n\" + dedication_text_wrap\n",
    "    \n",
    "    ## Acknowledgement ##\n",
    "    label_ack = df1.loc[\"Label-Acknowledgement\"]\n",
    "    phrases_ack = label_ack['text']\n",
    "    ack = parser.preprocess_ack(phrases_ack[0]) ## change the index values which corresponds to different ETD samples\n",
    "    ack_paraphrase = augmentation.paraphrased_text(ack)\n",
    "    ack_text_wrap = augmentation.wrap_text(ack_paraphrase)\n",
    "    ack_title = \"\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tACKNOWLEDGEMENTS\\n\" + ack_text_wrap\n",
    "    #print(ack_title)\n",
    "       \n",
    "    ## General Abstract ##\n",
    "    label_gabs = df1.loc[\"Label-GeneralAbstract\"]\n",
    "    phrase_gabs = label_gabs['text']\n",
    "    text0 = ast.literal_eval(phrase_gabs[0])\n",
    "    text1 = ast.literal_eval(phrase_gabs[1])\n",
    "    text2 = ast.literal_eval(phrase_gabs[2])\n",
    "    text3 = ast.literal_eval(phrase_gabs[3])\n",
    "    phrases_gabs = (\" \").join(text0) + (\" \").join(text1) + (\" \").join(text2) + (\" \").join(text3)\n",
    "    gabs = parser.preprocess_gabs(phrases_gabs)\n",
    "    gabs_paraphrase = augmentation.paraphrased_text(gabs)\n",
    "    gabs_text_wrap = augmentation.wrap_text(gabs_paraphrase)\n",
    "    gabs_title = \"\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tSUMMARY\\n\" + gabs_text_wrap\n",
    "    text_on_img(dedication_title,43)\n",
    "    \n",
    "    '''For now we need to change the directory '''\n",
    "    ##image augmentation\n",
    "    seq = iaa.Sequential([\n",
    "        iaa.Affine(rotate=(-5, 5)),\n",
    "        iaa.AdditiveGaussianNoise(scale=(10, 60)),\n",
    "        iaa.SaltAndPepper(p=0.1),\n",
    "        iaa.GaussianBlur(sigma=0.5),\n",
    "        iaa.LinearContrast(alpha=1),\n",
    "        iaa.PerspectiveTransform(scale=0.025, keep_size=True)\n",
    "    ], random_order = True)\n",
    "    \n",
    "    img_path_dedication = sorted(glob.glob('/home/mchou001/Label-Dedication/*.png'), key = numericalSort)\n",
    "    for n, images in enumerate(img_path_dedication[0:3]):\n",
    "        aug_image = seq(image=cv2.imread(images))\n",
    "        cv2.imwrite('/home/mchou001/Label-Dedication/aug_images/aug{}.png'.format(n),aug_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
