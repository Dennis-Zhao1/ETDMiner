{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import glob\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import ast\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sentence_splitter import SentenceSplitter, split_text_into_sentences\n",
    "from transformers import PegasusForConditionalGeneration, PegasusTokenizerFast, PegasusTokenizer\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "import textwrap\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from imgaug import augmenters as iaa\n",
    "import imgaug as ia\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function below will sort the files in numberical order \n",
    "numbers = re.compile(r'(\\d+)')\n",
    "def numericalSort(value):\n",
    "    parts = numbers.split(value)\n",
    "    parts[1::2] = map(int, parts[1::2])\n",
    "    return parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "etd_path = sorted(glob.glob('/home/mchou001/etds_json/*.json'), key = numericalSort)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessor:\n",
    "    \n",
    "    ## the function will return parsed lines from json files\n",
    "    def json_text_parser(self, json_files):\n",
    "        self.json_files = json_files\n",
    "        counter = 0\n",
    "        list_line = []\n",
    "        for filename in self.json_files:\n",
    "            with open(filename, encoding='utf-8', mode='r') as file:\n",
    "                data = json.loads(file.read())\n",
    "                counter = counter + 1\n",
    "                _text = []\n",
    "                for i in range(len(data)):\n",
    "                    text = data[i].get(\"Line_and_BB\")\n",
    "                    _text.append(text)\n",
    "                for i, data in enumerate(_text):\n",
    "                    line_text = [text['Line'].strip('\\n') for text in data]\n",
    "                    list_line.append(line_text)\n",
    "        \n",
    "        return list_line\n",
    "    \n",
    "    ## the function will return parsed bounding box information from json files\n",
    "    def json_bbox_parser(self, json_files):\n",
    "        self.json_files = json_files\n",
    "        counter = 0\n",
    "        list_bbox = []\n",
    "        for filename in self.json_files:\n",
    "            with open(filename, encoding='utf-8', mode='r') as file:\n",
    "                data = json.loads(file.read())\n",
    "                counter = counter + 1\n",
    "                _text = []\n",
    "                for i in range(len(data)):\n",
    "                    text = data[i].get(\"Line_and_BB\")\n",
    "                    _text.append(text)\n",
    "                for i, data in enumerate(_text):\n",
    "                    bbox_text = [bbox['Bounding Box'] for bbox in data]\n",
    "                    list_bbox.append(bbox_text)\n",
    "        \n",
    "        return list_bbox\n",
    "    \n",
    "    '''\n",
    "    We are using 'ast' module for preprocessing task of the class labels.\n",
    "    The ast module helps Python applications to process trees of the Python abstract syntax grammar.\n",
    "    We utilized the ast module to grammatically identify the sentences and applied 'sentence splitter' module \n",
    "    on paragraphs to get a list of sentences.\n",
    "    The preprocessing functions for each classes returns a list.\n",
    "    '''\n",
    "    \n",
    "    def preprocess_dedication(self, dedication_):\n",
    "        self.dedication_ = dedication_\n",
    "        text_dedication = ast.literal_eval(self.dedication_)\n",
    "        text_dedication = (\" \").join(text_dedication)\n",
    "        splitter = SentenceSplitter(language='en')\n",
    "        dedication_list = splitter.split(text_dedication)\n",
    "        return dedication_list    \n",
    "    \n",
    "    def preprocess_ack(self, acknowledgement_):\n",
    "        self.acknowledgement_ = acknowledgement_\n",
    "        text_ack = ast.literal_eval(self.acknowledgement_)\n",
    "        text_ack = (\" \").join(text_ack)\n",
    "        splitter = SentenceSplitter(language='en')\n",
    "        ack_list = splitter.split(text_ack)\n",
    "        return ack_list\n",
    "    \n",
    "    def preprocess_gabs(self, general_abs):\n",
    "        self.general_abs = general_abs\n",
    "        splitter = SentenceSplitter(language='en')\n",
    "        gabs_list = splitter.split(self.general_abs)\n",
    "        return gabs_list\n",
    "    \n",
    "    def preprocess_abs(self, abstract_):\n",
    "        self.abstract_ = abstract_\n",
    "        text_abstract = ast.literal_eval(self.abstract_)\n",
    "        text_abstract = (\" \").join(text_abstract)\n",
    "        splitter = SentenceSplitter(language='en')\n",
    "        abstract_list = splitter.split(text_abstract)\n",
    "        return abstract_list  \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## loading the paraphrasing model\n",
    "\n",
    "#references: https://arxiv.org/abs/1912.08777\n",
    "\n",
    "model_name = 'tuner007/pegasus_paraphrase'\n",
    "torch_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
    "model = PegasusForConditionalGeneration.from_pretrained(model_name).to(torch_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Augmentation:\n",
    "    \n",
    "    def get_paraphrased_sentences(self, input_text, num_return_sequences):\n",
    "        self.input_text = input_text\n",
    "        self.num_return_sequences = num_return_sequences        \n",
    "        batch = tokenizer.prepare_seq2seq_batch([self.input_text],truncation=True,padding='longest',max_length=60, return_tensors=\"pt\").to(torch_device)\n",
    "        translated = model.generate(**batch,max_length=60,num_beams=10, num_return_sequences=self.num_return_sequences, temperature=1.5)\n",
    "        paraphrased_text = tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
    "        return paraphrased_text       \n",
    "    \n",
    "    def paraphrased_text(self, class_text_list):\n",
    "        self.class_text_list = class_text_list\n",
    "        phrases_text = []\n",
    "        for ele in self.class_text_list:\n",
    "            phrase = self.get_paraphrased_sentences(ele, 1)\n",
    "            phrases_text.append(phrase)\n",
    "        \n",
    "        para_phrases = [' '.join(x) for x in phrases_text]\n",
    "        paraphrase_ = [' '.join(x for x in para_phrases)] ## combine  the splitted lists into a paragraph\n",
    "        paraphrase_text = str(paraphrase_).strip('[]').strip(\"'\")\n",
    "        paraphrase_strip = paraphrase_text.strip('\"\"')\n",
    "        return paraphrase_strip\n",
    "    \n",
    "    def wrap_text(self, text):\n",
    "        self.text = text\n",
    "        new_phrase = textwrap.wrap(self.text, width=90)\n",
    "        string = ''\n",
    "        for ele in new_phrase[0:]:\n",
    "            string = string + ele + '\\n'\n",
    "        return string       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    parser = Preprocessor()\n",
    "    \n",
    "    etd_lines = parser.json_text_parser(etd_path)\n",
    "    etd_bbox = parser.json_bbox_parser(etd_path)\n",
    "    labels_ = pd.read_csv(\"labels.csv\", encoding = 'utf-8')\n",
    "    etd_label = labels_['labels']\n",
    "    file_id = labels_['file_id']\n",
    "    res_list = [list(item) for item in list(zip(file_id, etd_lines, etd_bbox, etd_label))]\n",
    "    dataframe = pd.DataFrame(res_list, columns = ['file_idx', 'text', 'bbox', 'class'])\n",
    "    dataframe.to_csv('ETD_aug.csv', index = False)\n",
    "#     df1 = pd.read_csv('ETD_aug.csv')\n",
    "    \n",
    "#     df1.set_index(\"class\", inplace = True)\n",
    "    \n",
    "#     augmentation = Augmentation()\n",
    "    \n",
    "#     ## Dedication ###\n",
    "#     label_dedication = df1.loc[\"Label-Dedication\"]\n",
    "#     phrases_dedication = label_dedication['text']\n",
    "#     dedication = parser.preprocess_dedication(phrases_dedication[2])  ## change the index values which corresponds to different ETD samples\n",
    "#     dedication_paraphrase = augmentation.paraphrased_text(dedication)\n",
    "#     dedication_text_wrap = augmentation.wrap_text(dedication_paraphrase)\n",
    "#     dedication_title = \"\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tDedication\\n\" + dedication_text_wrap\n",
    "\n",
    "       \n",
    "#     ## General Abstract ##\n",
    "#     label_gabs = df1.loc[\"Label-GeneralAbstract\"]\n",
    "#     phrase_gabs = label_gabs['text']\n",
    "#     text0 = ast.literal_eval(phrase_gabs[0])\n",
    "#     text1 = ast.literal_eval(phrase_gabs[1])\n",
    "#     text2 = ast.literal_eval(phrase_gabs[2])\n",
    "#     text3 = ast.literal_eval(phrase_gabs[3])\n",
    "#     phrases_gabs = (\" \").join(text0) + (\" \").join(text1) + (\" \").join(text2) + (\" \").join(text3)\n",
    "#     gabs = parser.preprocess_gabs(phrases_gabs)\n",
    "#     gabs_paraphrase = augmentation.paraphrased_text(gabs)\n",
    "#     gabs_text_wrap = augmentation.wrap_text(gabs_paraphrase)\n",
    "#     gabs_title = \"\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tSUMMARY\\n\" + gabs_text_wrap\n",
    "    \n",
    "#     ## Abstract\n",
    "#     label_abs = df1.loc[\"Label-Abstract\"]\n",
    "#     phrases_abs = label_abs['text']\n",
    "#     abstract = parser.preprocess_abs(phrases_abs[775])\n",
    "#     abstract_paraphrase = augmentation.paraphrased_text(abstract)\n",
    "#     abstract_text_wrap = augmentation.wrap_text(abstract_paraphrase)\n",
    "#     #print(abstract_text_wrap)\n",
    "#     abs_title = \"\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tAbstract\\n\" + abstract_text_wrap\n",
    "    \n",
    "    \n",
    "    #text_on_img(abs_title,40) ## dedication -- 43, ack and abstract - 40, gabs -- 38\n",
    "    \n",
    "#    '''For now we need to change the directory for each class label'''\n",
    "    ##image augmentation\n",
    "#     seq = iaa.Sequential([\n",
    "#         iaa.Affine(rotate=(-6, 6)),\n",
    "#         iaa.AdditiveGaussianNoise(scale=(10, 50)),\n",
    "#         iaa.SaltAndPepper(p=0.1),\n",
    "#         iaa.GaussianBlur(sigma=0.5),\n",
    "#         iaa.LinearContrast(alpha=1),\n",
    "#         iaa.PerspectiveTransform(scale=0.025, keep_size=True)\n",
    "#     ], random_order = True)\n",
    "    \n",
    "#     img_path_dedication = sorted(glob.glob('/home/mchou001/Label-Abstract/*.png'), key = numericalSort)\n",
    "#     for n, images in enumerate(img_path_dedication[0:20]):\n",
    "#         for n in range(6):\n",
    "#             aug_image = seq(image=cv2.imread(images))\n",
    "#             cv2.imwrite('/home/mchou001/Label-Abstract/aug_images/aug{}.png'.format(n),aug_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
