{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae1b6492-f343-4b84-b0ea-665cf022568b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-12 15:06:10.958628: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-12 15:06:12.959653: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "Intel(R) Extension for Scikit-learn* enabled (https://github.com/intel/scikit-learn-intelex)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/_distributor_init.py:25: FutureWarning: \n",
      "USE_DAAL4PY_SKLEARN variable is deprecated for Intel(R) Extension for Scikit-learn\n",
      "and will be delete in the 2022.1 release.\n",
      "Please, use new construction of global patching:\n",
      "python sklearnex.glob patch_sklearn\n",
      "Read more: https://intel.github.io/scikit-learn-intelex/global_patching.html\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from transformers import LayoutLMv2FeatureExtractor, LayoutLMv2Tokenizer, LayoutLMv2Processor\n",
    "from datasets import Features, Sequence, ClassLabel, Value, Array2D, Array3D\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "import tqdm\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests, zipfile, io\n",
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from p_tqdm import p_map\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "import time\n",
    "import pytesseract\n",
    "from functools import partial\n",
    "import re\n",
    "import cv2\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a893115-eb2b-4326-9e41-6fa6dfda76d8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ArrowInvalid",
     "evalue": "Could not open Parquet input source '<Buffer>': Parquet magic bytes not found in footer. Either the file is corrupted or this is not a parquet file.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mArrowInvalid\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_17760/1026407109.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_parquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/home/mchou001/ResNET/rvl_cdip_100_examples_per_class/data/train-00000-of-00001-6c397619d2120c4e.parquet'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/parquet.py\u001b[0m in \u001b[0;36mread_parquet\u001b[0;34m(path, engine, columns, storage_options, use_nullable_dtypes, **kwargs)\u001b[0m\n\u001b[1;32m    498\u001b[0m         \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m         \u001b[0muse_nullable_dtypes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_nullable_dtypes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 500\u001b[0;31m         \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    501\u001b[0m     )\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/parquet.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, path, columns, use_nullable_dtypes, storage_options, **kwargs)\u001b[0m\n\u001b[1;32m    238\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m             result = self.api.parquet.read_table(\n\u001b[0;32m--> 240\u001b[0;31m                 \u001b[0mpath_or_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m             ).to_pandas(**to_pandas_kwargs)\n\u001b[1;32m    242\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmanager\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"array\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/envs/segment/lib/python3.7/site-packages/pyarrow/parquet/core.py\u001b[0m in \u001b[0;36mread_table\u001b[0;34m(source, columns, use_threads, metadata, schema, use_pandas_metadata, read_dictionary, memory_map, buffer_size, partitioning, filesystem, filters, use_legacy_dataset, ignore_prefixes, pre_buffer, coerce_int96_timestamp_unit, decryption_properties, thrift_string_size_limit, thrift_container_size_limit)\u001b[0m\n\u001b[1;32m   2950\u001b[0m                 \u001b[0mcoerce_int96_timestamp_unit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcoerce_int96_timestamp_unit\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2951\u001b[0m                 \u001b[0mthrift_string_size_limit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mthrift_string_size_limit\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2952\u001b[0;31m                 \u001b[0mthrift_container_size_limit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mthrift_container_size_limit\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2953\u001b[0m             )\n\u001b[1;32m   2954\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/envs/segment/lib/python3.7/site-packages/pyarrow/parquet/core.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path_or_paths, filesystem, filters, partitioning, read_dictionary, buffer_size, memory_map, ignore_prefixes, pre_buffer, coerce_int96_timestamp_unit, schema, decryption_properties, thrift_string_size_limit, thrift_container_size_limit, **kwargs)\u001b[0m\n\u001b[1;32m   2477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2478\u001b[0m             self._dataset = ds.FileSystemDataset(\n\u001b[0;32m-> 2479\u001b[0;31m                 \u001b[0;34m[\u001b[0m\u001b[0mfragment\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mschema\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfragment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mphysical_schema\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2480\u001b[0m                 \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparquet_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2481\u001b[0m                 \u001b[0mfilesystem\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfragment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilesystem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/envs/segment/lib/python3.7/site-packages/pyarrow/_dataset.pyx\u001b[0m in \u001b[0;36mpyarrow._dataset.Fragment.physical_schema.__get__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/envs/segment/lib/python3.7/site-packages/pyarrow/error.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.pyarrow_internal_check_status\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/envs/segment/lib/python3.7/site-packages/pyarrow/error.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.check_status\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mArrowInvalid\u001b[0m: Could not open Parquet input source '<Buffer>': Parquet magic bytes not found in footer. Either the file is corrupted or this is not a parquet file."
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# df = pd.read_parquet('/home/mchou001/ResNET/rvl_cdip_100_examples_per_class/data/train-00000-of-00001-6c397619d2120c4e.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b469ab3-d88e-444d-9430-571bce4bb9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df = pd.read_csv(\"ETD_aug.csv\")\n",
    "text_df = text_df['text']\n",
    "text_df.to_csv(\"ETD_text.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5bc9063d-f3cc-4d44-80b1-af8713749dfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda is available? True\n",
      "Device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "cuda_yes = torch.cuda.is_available()\n",
    "print('Cuda is available?', cuda_yes)\n",
    "device = torch.device(\"cuda:0\" if cuda_yes else \"cpu\")\n",
    "print('Device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b6cf46a-556e-4715-874c-171146985aff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img_path</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/home/mchou001/train/1/1.png</td>\n",
       "      <td>Label-TitlePage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/home/mchou001/train/1/1.png</td>\n",
       "      <td>Label-TitlePage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/home/mchou001/train/2/1.png</td>\n",
       "      <td>Label-TitlePage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/home/mchou001/train/3/1.png</td>\n",
       "      <td>Label-TitlePage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/home/mchou001/train/4/1.png</td>\n",
       "      <td>Label-TitlePage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>918</th>\n",
       "      <td>/home/mchou001/train/7/48.png</td>\n",
       "      <td>Label-Appendices</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>919</th>\n",
       "      <td>/home/mchou001/train/7/49.png</td>\n",
       "      <td>Label-Appendices</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>920</th>\n",
       "      <td>/home/mchou001/train/9/83.png</td>\n",
       "      <td>Label-Appendices</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>921</th>\n",
       "      <td>/home/mchou001/train/9/84.png</td>\n",
       "      <td>Label-Appendices</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>922</th>\n",
       "      <td>/home/mchou001/train/9/85.png</td>\n",
       "      <td>Label-Appendices</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>923 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          img_path             label\n",
       "0     /home/mchou001/train/1/1.png   Label-TitlePage\n",
       "1     /home/mchou001/train/1/1.png   Label-TitlePage\n",
       "2     /home/mchou001/train/2/1.png   Label-TitlePage\n",
       "3     /home/mchou001/train/3/1.png   Label-TitlePage\n",
       "4     /home/mchou001/train/4/1.png   Label-TitlePage\n",
       "..                             ...               ...\n",
       "918  /home/mchou001/train/7/48.png  Label-Appendices\n",
       "919  /home/mchou001/train/7/49.png  Label-Appendices\n",
       "920  /home/mchou001/train/9/83.png  Label-Appendices\n",
       "921  /home/mchou001/train/9/84.png  Label-Appendices\n",
       "922  /home/mchou001/train/9/85.png  Label-Appendices\n",
       "\n",
       "[923 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(\"train_1k.csv\")\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90269616-50ad-4f88-b55f-b6331ec85ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# numbers = re.compile(r'(\\d+)')\n",
    "# def numericalSort(value):\n",
    "#     parts = numbers.split(value)\n",
    "#     parts[1::2] = map(int, parts[1::2])\n",
    "#     return parts\n",
    "\n",
    "# test_img_path = sorted(glob.glob('/home/mchou001/test/**/*.png'), key = numericalSort)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67238c23-5715-425b-be89-c238a530e287",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_label = pd.read_csv(\"labels_test.csv\")\n",
    "# test_class = test_label['labels']\n",
    "# img_path_label_map = list(zip(test_img_path, test_class))\n",
    "# df_test = pd.DataFrame(img_path_label_map, columns = ['img_path', 'label'])\n",
    "# df_test.to_csv('test200ETDs.csv', encoding = 'utf-8', index= None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6baf2f5c-b529-456b-b41e-280fcf80233d",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = LayoutLMv2FeatureExtractor()\n",
    "tokenizer = LayoutLMv2Tokenizer.from_pretrained(\"microsoft/layoutlmv2-base-uncased\")\n",
    "processor = LayoutLMv2Processor(feature_extractor, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da5db416-1ccb-406b-a5f9-baca1b804d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_classes = np.asarray(train['label'])\n",
    "le = LabelEncoder()\n",
    "le.fit(list(y_classes))\n",
    "train_y = le.transform(list(y_classes))\n",
    "id2label = dict(zip(train_y, y_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be991685-cfec-4610-a75b-f7af8672576f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Label-TitlePage': 11,\n",
       " 'Label-Dedication': 5,\n",
       " 'Label-Acknowledgement': 1,\n",
       " 'Label-TableofContent': 10,\n",
       " 'Label-ListofTables': 8,\n",
       " 'Label-ListofFigures': 7,\n",
       " 'Label-Abstract': 0,\n",
       " 'Label-Chapters': 4,\n",
       " 'Label-ChapterAbstract': 3,\n",
       " 'Label-GeneralAbstract': 6,\n",
       " 'Label-ReferenceList': 9,\n",
       " 'Label-Appendices': 2}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label2id = {k: v for v, k in enumerate(train_y)}\n",
    "label2id = dict(zip(y_classes, train_y))\n",
    "label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "891e9b51-d0cb-459d-8ff7-e5b96bb88962",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Label-ReferenceList',\n",
       " 'Label-GeneralAbstract',\n",
       " 'Label-ChapterAbstract',\n",
       " 'Label-TableofContent',\n",
       " 'Label-Dedication',\n",
       " 'Label-Chapters',\n",
       " 'Label-Appendices',\n",
       " 'Label-Acknowledgement',\n",
       " 'Label-ListofFigures',\n",
       " 'Label-Abstract',\n",
       " 'Label-ListofTables',\n",
       " 'Label-TitlePage']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = list(set(y_classes))\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1cb37dc1-10fd-4383-aac1-8f4bd657aeb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['img_path', 'label'],\n",
       "    num_rows: 923\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = Dataset.from_pandas(train)\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "69d58dd1-eb7f-47a2-8dea-560b92d09552",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Label-ReferenceList      84\n",
       "Label-ListofFigures      78\n",
       "Label-TableofContent     77\n",
       "Label-Acknowledgement    77\n",
       "Label-Abstract           77\n",
       "Label-GeneralAbstract    77\n",
       "Label-Dedication         77\n",
       "Label-ChapterAbstract    77\n",
       "Label-TitlePage          77\n",
       "Label-Chapters           76\n",
       "Label-ListofTables       76\n",
       "Label-Appendices         70\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "24453c60-9367-46ee-8f14-4485646d8d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train = Features({\n",
    "    'image': Array3D(dtype=\"int64\", shape=(3, 224, 224)),\n",
    "    'input_ids': Sequence(feature=Value(dtype='int64')),\n",
    "    'attention_mask': Sequence(Value(dtype='int64')),\n",
    "    'token_type_ids': Sequence(Value(dtype='int64')),\n",
    "    'bbox': Array2D(dtype=\"int64\", shape=(512, 4)),\n",
    "    'labels': ClassLabel(num_classes=len(labels), names=labels),\n",
    "})\n",
    "\n",
    "def preprocess_data(examples):\n",
    "    \n",
    "    images = [Image.open(path) for path in examples['img_path']]\n",
    "    \n",
    "    encoded_inputs = processor(images, padding=\"max_length\", truncation=True)\n",
    "    \n",
    "    encoded_inputs[\"labels\"] = [label2id[label] for label in examples[\"label\"]]\n",
    "    \n",
    "    return encoded_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a031d4be-140d-4018-967b-e5e4a420406b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.fingerprint:Parameter 'function'=<function preprocess_data at 0x7f56f8f03950> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b11295c7dd14ea5a774f6ebd5dad594",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2999 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "encoded_train_dataset = train_data.map(preprocess_data,remove_columns=train_data.column_names,features=features_train, \n",
    "                                       batched = True, batch_size = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "4fe62789-bb63-4e16-a526-e0f0fd2bab00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['image', 'input_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels'],\n",
       "    num_rows: 2999\n",
       "})"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d74f92ed-3c8c-48dc-a6d0-7c229e25fe13",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(encoded_train_dataset, 'tensor1k.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3e1a4275-da54-40b7-827b-6b2fe3a70830",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_train = torch.load('tensor1k.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "521b3ae8-aad8-4149-bdc5-4f658c8e05f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_train.set_format(type=\"torch\", device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8cba63c3-3928-48f3-889a-80da60a69feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "CUDA_LAUNCH_BLOCKING=1\n",
    "train_dataloader = torch.utils.data.DataLoader(encoded_train, batch_size=4)\n",
    "batch_train = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7ccb6257-89ef-4bdb-aeef-7823616b6e33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image torch.Size([4, 3, 224, 224])\n",
      "input_ids torch.Size([4, 512])\n",
      "attention_mask torch.Size([4, 512])\n",
      "token_type_ids torch.Size([4, 512])\n",
      "bbox torch.Size([4, 512, 4])\n",
      "labels torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "for k,v in batch_train.items():\n",
    "    print(k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9628302f-eb9c-4c41-8f7f-16462ccb818f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[CLS] ppccedupes as a peppesfntaticn fop mata in a computer ppcgpaim fcr undepstatipe ic matupal language hy terry \\\\ iinograd p. a., the colorado college ( 1966 ) suppittter ¢ ( n paptial fulfillment cf the pequepemfnts fcp thf pfopee of poctopr nf philoscphy at the massachusetts imnstitutf of tfchnology august, 12790 signature of author.. wun. soe ee department of mat feratic tugust 2h, 1979 certified by.., woe we ee ee thests supervisor accepted hy * * e a 78 e « ° e. * ‘. '. a. a « ° chairman, denartmental corrittee on craduate students mass. inst. fece sep 29 1279 ven saies [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\""
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.tokenizer.decode(batch_train['input_ids'][2].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "83b46df0-4104-42c5-b2d5-d45664b11c4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Label-TitlePage'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2label[batch_train['labels'][1].item()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4cb3af2-7c2e-493c-99c7-fa4d086cd36d",
   "metadata": {},
   "source": [
    "## Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6e3e79c5-5d8d-44d9-9df4-86a351144909",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img_path</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/home/mchou001/test/494/2.png</td>\n",
       "      <td>Label-TitlePage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/home/mchou001/test/406/2.png</td>\n",
       "      <td>Label-TitlePage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/home/mchou001/test/406/3.png</td>\n",
       "      <td>Label-TitlePage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/home/mchou001/test/407/2.png</td>\n",
       "      <td>Label-TitlePage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/home/mchou001/test/407/3.png</td>\n",
       "      <td>Label-TitlePage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>519</th>\n",
       "      <td>/home/mchou001/test/411/265.png</td>\n",
       "      <td>Label-CurriculumVitae</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>520</th>\n",
       "      <td>/home/mchou001/test/412/105.png</td>\n",
       "      <td>Label-CurriculumVitae</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>521</th>\n",
       "      <td>/home/mchou001/test/413/170.png</td>\n",
       "      <td>Label-CurriculumVitae</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>522</th>\n",
       "      <td>/home/mchou001/test/414/170.png</td>\n",
       "      <td>Label-CurriculumVitae</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>523</th>\n",
       "      <td>/home/mchou001/test/415/185.png</td>\n",
       "      <td>Label-CurriculumVitae</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>524 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            img_path                  label\n",
       "0      /home/mchou001/test/494/2.png        Label-TitlePage\n",
       "1      /home/mchou001/test/406/2.png        Label-TitlePage\n",
       "2      /home/mchou001/test/406/3.png        Label-TitlePage\n",
       "3      /home/mchou001/test/407/2.png        Label-TitlePage\n",
       "4      /home/mchou001/test/407/3.png        Label-TitlePage\n",
       "..                               ...                    ...\n",
       "519  /home/mchou001/test/411/265.png  Label-CurriculumVitae\n",
       "520  /home/mchou001/test/412/105.png  Label-CurriculumVitae\n",
       "521  /home/mchou001/test/413/170.png  Label-CurriculumVitae\n",
       "522  /home/mchou001/test/414/170.png  Label-CurriculumVitae\n",
       "523  /home/mchou001/test/415/185.png  Label-CurriculumVitae\n",
       "\n",
       "[524 rows x 2 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv(\"test600.csv\")\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f1d9d437-1093-453b-8906-96e534bae4ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Label-TitlePage          134\n",
       "Label-Chapters            80\n",
       "Label-ChapterAbstract     71\n",
       "Label-Dedication          65\n",
       "Label-Abstract            46\n",
       "Label-Acknowledgement     33\n",
       "Label-GeneralAbstract     15\n",
       "Label-ListofFigures       15\n",
       "Label-ReferenceList       15\n",
       "Label-TableofContent      15\n",
       "Label-Appendices          15\n",
       "Label-ListofTables        15\n",
       "Label-CurriculumVitae      5\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "51d85366-3253-4d4d-8bed-9e0c950f60cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Label-ReferenceList',\n",
       " 'Label-GeneralAbstract',\n",
       " 'Label-ChapterAbstract',\n",
       " 'Label-TableofContent',\n",
       " 'Label-Dedication',\n",
       " 'Label-Chapters',\n",
       " 'Label-Appendices',\n",
       " 'Label-Acknowledgement',\n",
       " 'Label-ListofFigures',\n",
       " 'Label-CurriculumVitae',\n",
       " 'Label-Abstract',\n",
       " 'Label-ListofTables',\n",
       " 'Label-TitlePage']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_classes = np.asarray(test['label'])\n",
    "le = LabelEncoder()\n",
    "le.fit(list(test_classes))\n",
    "test_y = le.transform(list(test_classes))\n",
    "id2label_test = dict(zip(test_y, test_classes))\n",
    "label2id_test = {k: v for v, k in enumerate(test_y)}\n",
    "label2id_test = dict(zip(test_classes, test_y))\n",
    "labels_test = list(set(test_classes))\n",
    "labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "15fc9f76-b723-4d72-8b4c-4e1e27e2bd22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['img_path', 'label'],\n",
       "    num_rows: 524\n",
       "})"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = Dataset.from_pandas(test)\n",
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0c6439f0-865d-48b7-a9ff-e1513013a874",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_test = Features({\n",
    "    'image': Array3D(dtype=\"int64\", shape=(3, 224, 224)),\n",
    "    'input_ids': Sequence(feature=Value(dtype='int64')),\n",
    "    'attention_mask': Sequence(Value(dtype='int64')),\n",
    "    'token_type_ids': Sequence(Value(dtype='int64')),\n",
    "    'bbox': Array2D(dtype=\"int64\", shape=(512, 4)),\n",
    "    'labels_test': ClassLabel(num_classes=len(labels_test), names= labels_test),\n",
    "})\n",
    "\n",
    "def preprocess_test_data(examples):\n",
    "    \n",
    "    images = [Image.open(path) for path in examples['img_path']]\n",
    "    \n",
    "    encoded_inputs = processor(images, padding=\"max_length\", truncation=True)\n",
    "    \n",
    "    encoded_inputs[\"labels_test\"] = [label2id[label] for label in examples[\"label\"]]\n",
    "    \n",
    "    return encoded_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "daed5111-a05d-4b23-8bca-1463fc90f5ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fafe26361264e7caa7b8f240051e706",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/524 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "encoded_test_dataset = test_data.map(preprocess_test_data, remove_columns=train_data.column_names, features=features_test, \n",
    "                              batched=True, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1c29fa57-bc81-4b28-bf39-fdf43553ddc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['image', 'input_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels_test'],\n",
       "    num_rows: 524\n",
       "})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d5a18e2f-b826-4e39-a16c-e0f2b1d59e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(encoded_test_dataset, 'tensor_test180.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fa376016-2b62-44e9-9e69-9ceaa929cc50",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_test = torch.load('tensor_test180.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f77f5d37-5f54-462c-a97e-e20da4637afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_test.set_format(type=\"torch\", device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "79d5ba11-69d3-4a3e-bb20-9f9c9834d1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = torch.utils.data.DataLoader(encoded_test, batch_size=4)\n",
    "batch_test = next(iter(test_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b295270a-7df4-41d3-b84e-d9ce5201902d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image torch.Size([4, 3, 224, 224])\n",
      "input_ids torch.Size([4, 512])\n",
      "attention_mask torch.Size([4, 512])\n",
      "token_type_ids torch.Size([4, 512])\n",
      "bbox torch.Size([4, 512, 4])\n",
      "labels_test torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "for k,v in batch_test.items():\n",
    "    print(k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1df606e2-58d4-437c-bac7-4b0e410fcd6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] 72 - 18, 998 marvich, robert hyman, 1945 — reactions of molecular nitrogen with ‘ titanocene and its derivatives. ‘ the univereity of michigan, ph. d., 1971 chenistry, inorganic university microfilms, a xerox company, ann arbor, michigan. ‘ reproduced wih permission of the copyright owner. futher reproduction prohibited without permission. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.tokenizer.decode(batch_test['input_ids'][0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "97e9ae2d-3710-4036-bcdd-aeffe6b7e270",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Label-TitlePage'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2label_test[batch_test['labels_test'][0].item()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5ae213d9-2289-42ec-946d-edad90de785e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Whether to run training.\"\n",
    "do_train = True\n",
    "# \"Whether to run eval on the dev set.\"\n",
    "do_eval = True\n",
    "# \"Whether to run the model in inference mode on the test set.\"\n",
    "do_predict = True\n",
    "# Whether load checkpoint file before train model\n",
    "load_checkpoint = True\n",
    "batch_size = 12 #32\n",
    "# \"The initial learning rate for Adam.\"\n",
    "learning_rate0 = 5e-6 #0.000005\n",
    "weight_decay_finetune = 1e-5 #0.00001\n",
    "total_train_epochs = 10\n",
    "gradient_accumulation_steps = 1\n",
    "warmup_proportion = 0.1\n",
    "output_dir = '/home/mchou001/'\n",
    "model_scale = 'microsoft/layoutlmv2-base-uncased' ## try uncased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f1dcaeb9-1d09-4f9b-b43d-4b71ed3dbd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score(y_true, y_pred):\n",
    "\n",
    "    num_proposed = len(y_pred[y_pred>ignore_id])\n",
    "    num_correct = (np.logical_and(y_true==y_pred, y_true)).sum()\n",
    "    num_gold = len(y_true[y_true>ignore_id])\n",
    "\n",
    "    try:\n",
    "        precision = num_correct / num_proposed\n",
    "    except ZeroDivisionError:\n",
    "        precision = 1.0\n",
    "\n",
    "    try:\n",
    "        recall = num_correct / num_gold\n",
    "    except ZeroDivisionError:\n",
    "        recall = 1.0\n",
    "\n",
    "    try:\n",
    "        f1 = 2*precision*recall / (precision + recall)\n",
    "    except ZeroDivisionError:\n",
    "        if precision*recall==0:\n",
    "            f1=1.0\n",
    "        else:\n",
    "            f1=0\n",
    "\n",
    "    return precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d7db02d8-3eb7-4afb-a28c-e827e7628f47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Torch AMP is not available on this platform\n",
      "Some weights of the model checkpoint at microsoft/layoutlmv2-base-uncased were not used when initializing LayoutLMv2ForSequenceClassification: ['layoutlmv2.visual.backbone.bottom_up.res2.0.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.2.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.17.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.5.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.6.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.6.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.18.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.1.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.10.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.3.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.4.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.0.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.19.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.12.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.3.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.1.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.0.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.21.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.11.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.4.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.0.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.17.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.13.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.13.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.7.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.16.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.15.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.7.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.13.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.10.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.12.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.12.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.3.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.16.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.1.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.19.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.2.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.11.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.4.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.5.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.0.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.2.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.0.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.21.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.22.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.2.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.1.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.7.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.5.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.20.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.14.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.18.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.0.shortcut.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.17.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.16.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.11.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.10.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.2.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.9.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.22.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.0.shortcut.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.1.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.0.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.8.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.19.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.18.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.0.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.1.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.0.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.14.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.0.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.1.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.8.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.9.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.15.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.6.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.3.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.0.shortcut.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.20.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.stem.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.20.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.2.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.2.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.21.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.2.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.1.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.2.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.22.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.1.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.14.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.8.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.2.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.2.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.1.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.0.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.9.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.15.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.1.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.1.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.2.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.0.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.3.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.3.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.0.shortcut.norm.num_batches_tracked']\n",
      "- This IS expected if you are initializing LayoutLMv2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LayoutLMv2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of LayoutLMv2ForSequenceClassification were not initialized from the model checkpoint at microsoft/layoutlmv2-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import LayoutLMv2ForSequenceClassification\n",
    "import torch\n",
    "from torch.optim import AdamW, Adam\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "if load_checkpoint and os.path.exists(output_dir+'/layoutETD1k.pt'):\n",
    "    checkpoint = torch.load(output_dir+'/layoutETD1k.pt', map_location='cuda')\n",
    "    start_epoch = checkpoint['epoch']+1\n",
    "    valid_acc_prev = checkpoint['valid_acc']\n",
    "    #valid_f1_prev = checkpoint['valid_f1']\n",
    "    model = LayoutLMv2ForSequenceClassification.from_pretrained(model_scale, state_dict=checkpoint['model_state'], num_labels=len(labels))\n",
    "    print('Loaded the LayoutLM model, epoch:',checkpoint['epoch'],'valid acc:', checkpoint['valid_acc'])\n",
    "else:\n",
    "    start_epoch = 0\n",
    "    valid_acc_prev = 0\n",
    "    valid_f1_prev = 0\n",
    "    model = LayoutLMv2ForSequenceClassification.from_pretrained(model_scale, num_labels=len(labels))\n",
    "\n",
    "model.to(device)\n",
    "named_params = list(model.named_parameters())\n",
    "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in named_params if not any(nd in n for nd in no_decay)], 'weight_decay': weight_decay_finetune},\n",
    "    {'params': [p for n, p in named_params if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9e66b29f-1308-4266-820e-8ad6e6d0fbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, predict_dataloader, batch_size, epoch_th, dataset_name):\n",
    "    # print(\"***** Running prediction *****\")\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    total=0\n",
    "    correct=0\n",
    "    start = time.time()\n",
    "    with torch.no_grad():\n",
    "        for batch in predict_dataloader:\n",
    "            image = batch['image'].to(device)\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            bbox = batch['bbox'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            token_type_ids = batch['token_type_ids'].to(device)\n",
    "            labels = batch['labels_test'].to(device)\n",
    "            out_scores = model(image= image, input_ids=input_ids, bbox=bbox, attention_mask=attention_mask, token_type_ids=token_type_ids,\n",
    "                        labels=labels)\n",
    "            # out_scores = out_scores.detach().cpu().numpy()\n",
    "            predicted = out_scores.logits.argmax(-1)\n",
    "            valid_predicted = predicted\n",
    "            valid_label_ids = labels\n",
    "            # print(len(valid_label_ids),len(valid_predicted),len(valid_label_ids)==len(valid_predicted))\n",
    "            all_preds.extend(valid_predicted.tolist())\n",
    "            all_labels.extend(valid_label_ids.tolist())\n",
    "            total += len(valid_label_ids)\n",
    "            correct += valid_predicted.eq(valid_label_ids).sum().item()\n",
    "\n",
    "    test_acc = correct/total\n",
    "    #precision, recall, f1 = f1_score(np.array(all_labels), np.array(all_preds))\n",
    "    end = time.time()\n",
    "    print('Epoch:%d, Acc:%.2f on %s, Spend: %.3f minutes for evaluation' % (epoch_th, 100.*test_acc, dataset_name, (end-start)/60.0))\n",
    "    print('--------------------------------------------------------------')\n",
    "    return test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5ea3d8ad-1b55-406b-b3ea-1a7ecd240df1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0-0/231, Loss: 2.4879825115203857 \n",
      "Epoch:0-1/231, Loss: 2.2779130935668945 \n",
      "Epoch:0-2/231, Loss: 2.4338154792785645 \n",
      "Epoch:0-3/231, Loss: 2.0457537174224854 \n",
      "Epoch:0-4/231, Loss: 2.242124557495117 \n",
      "Epoch:0-5/231, Loss: 2.062376022338867 \n",
      "Epoch:0-6/231, Loss: 1.9513334035873413 \n",
      "Epoch:0-7/231, Loss: 1.8379333019256592 \n",
      "Epoch:0-8/231, Loss: 1.7097687721252441 \n",
      "Epoch:0-9/231, Loss: 1.653087854385376 \n",
      "Epoch:0-10/231, Loss: 1.6996227502822876 \n",
      "Epoch:0-11/231, Loss: 1.8437039852142334 \n",
      "Epoch:0-12/231, Loss: 1.6164402961730957 \n",
      "Epoch:0-13/231, Loss: 1.5403103828430176 \n",
      "Epoch:0-14/231, Loss: 1.40115487575531 \n",
      "Epoch:0-15/231, Loss: 1.410728931427002 \n",
      "Epoch:0-16/231, Loss: 1.2270996570587158 \n",
      "Epoch:0-17/231, Loss: 1.2943031787872314 \n",
      "Epoch:0-18/231, Loss: 1.1414142847061157 \n",
      "Epoch:0-19/231, Loss: 2.355823516845703 \n",
      "Epoch:0-20/231, Loss: 2.812464952468872 \n",
      "Epoch:0-21/231, Loss: 2.623180389404297 \n",
      "Epoch:0-22/231, Loss: 2.5611672401428223 \n",
      "Epoch:0-23/231, Loss: 2.629121780395508 \n",
      "Epoch:0-24/231, Loss: 2.5329177379608154 \n",
      "Epoch:0-25/231, Loss: 2.5196046829223633 \n",
      "Epoch:0-26/231, Loss: 2.3536252975463867 \n",
      "Epoch:0-27/231, Loss: 2.2905867099761963 \n",
      "Epoch:0-28/231, Loss: 2.305006980895996 \n",
      "Epoch:0-29/231, Loss: 2.0727272033691406 \n",
      "Epoch:0-30/231, Loss: 2.1201868057250977 \n",
      "Epoch:0-31/231, Loss: 1.9828932285308838 \n",
      "Epoch:0-32/231, Loss: 1.8045098781585693 \n",
      "Epoch:0-33/231, Loss: 1.7553768157958984 \n",
      "Epoch:0-34/231, Loss: 1.7607417106628418 \n",
      "Epoch:0-35/231, Loss: 1.6149704456329346 \n",
      "Epoch:0-36/231, Loss: 1.5193332433700562 \n",
      "Epoch:0-37/231, Loss: 1.4855103492736816 \n",
      "Epoch:0-38/231, Loss: 2.178286075592041 \n",
      "Epoch:0-39/231, Loss: 3.104327440261841 \n",
      "Epoch:0-40/231, Loss: 2.8915719985961914 \n",
      "Epoch:0-41/231, Loss: 2.8509838581085205 \n",
      "Epoch:0-42/231, Loss: 3.022371292114258 \n",
      "Epoch:0-43/231, Loss: 2.7556045055389404 \n",
      "Epoch:0-44/231, Loss: 2.650678873062134 \n",
      "Epoch:0-45/231, Loss: 2.690432071685791 \n",
      "Epoch:0-46/231, Loss: 2.5024125576019287 \n",
      "Epoch:0-47/231, Loss: 2.5295958518981934 \n",
      "Epoch:0-48/231, Loss: 2.345529317855835 \n",
      "Epoch:0-49/231, Loss: 2.4098377227783203 \n",
      "Epoch:0-50/231, Loss: 2.2601871490478516 \n",
      "Epoch:0-51/231, Loss: 2.245709180831909 \n",
      "Epoch:0-52/231, Loss: 2.1766722202301025 \n",
      "Epoch:0-53/231, Loss: 2.150690793991089 \n",
      "Epoch:0-54/231, Loss: 1.745638370513916 \n",
      "Epoch:0-55/231, Loss: 1.8280221223831177 \n",
      "Epoch:0-56/231, Loss: 1.8869253396987915 \n",
      "Epoch:0-57/231, Loss: 2.08406925201416 \n",
      "Epoch:0-58/231, Loss: 2.979748487472534 \n",
      "Epoch:0-59/231, Loss: 3.131866931915283 \n",
      "Epoch:0-60/231, Loss: 2.861727714538574 \n",
      "Epoch:0-61/231, Loss: 2.926988124847412 \n",
      "Epoch:0-62/231, Loss: 2.979703903198242 \n",
      "Epoch:0-63/231, Loss: 2.737130880355835 \n",
      "Epoch:0-64/231, Loss: 2.6592493057250977 \n",
      "Epoch:0-65/231, Loss: 2.60273814201355 \n",
      "Epoch:0-66/231, Loss: 2.7233710289001465 \n",
      "Epoch:0-67/231, Loss: 2.5454115867614746 \n",
      "Epoch:0-68/231, Loss: 2.3160760402679443 \n",
      "Epoch:0-69/231, Loss: 2.3219656944274902 \n",
      "Epoch:0-70/231, Loss: 2.307495594024658 \n",
      "Epoch:0-71/231, Loss: 2.3178868293762207 \n",
      "Epoch:0-72/231, Loss: 2.060750722885132 \n",
      "Epoch:0-73/231, Loss: 2.070117473602295 \n",
      "Epoch:0-74/231, Loss: 1.8696649074554443 \n",
      "Epoch:0-75/231, Loss: 1.7672631740570068 \n",
      "Epoch:0-76/231, Loss: 1.8849895000457764 \n",
      "Epoch:0-77/231, Loss: 3.4724509716033936 \n",
      "Epoch:0-78/231, Loss: 3.4543404579162598 \n",
      "Epoch:0-79/231, Loss: 3.400491237640381 \n",
      "Epoch:0-80/231, Loss: 3.4772374629974365 \n",
      "Epoch:0-81/231, Loss: 3.2455124855041504 \n",
      "Epoch:0-82/231, Loss: 3.3293890953063965 \n",
      "Epoch:0-83/231, Loss: 3.1863820552825928 \n",
      "Epoch:0-84/231, Loss: 3.1261277198791504 \n",
      "Epoch:0-85/231, Loss: 3.0562729835510254 \n",
      "Epoch:0-86/231, Loss: 3.0534589290618896 \n",
      "Epoch:0-87/231, Loss: 2.7987759113311768 \n",
      "Epoch:0-88/231, Loss: 2.615960121154785 \n",
      "Epoch:0-89/231, Loss: 2.623615264892578 \n",
      "Epoch:0-90/231, Loss: 2.5826847553253174 \n",
      "Epoch:0-91/231, Loss: 2.413872241973877 \n",
      "Epoch:0-92/231, Loss: 2.4801132678985596 \n",
      "Epoch:0-93/231, Loss: 2.370331287384033 \n",
      "Epoch:0-94/231, Loss: 2.2459754943847656 \n",
      "Epoch:0-95/231, Loss: 2.218944787979126 \n",
      "Epoch:0-96/231, Loss: 3.4801831245422363 \n",
      "Epoch:0-97/231, Loss: 3.486417293548584 \n",
      "Epoch:0-98/231, Loss: 3.599789619445801 \n",
      "Epoch:0-99/231, Loss: 3.5302138328552246 \n",
      "Epoch:0-100/231, Loss: 3.3565001487731934 \n",
      "Epoch:0-101/231, Loss: 3.1730167865753174 \n",
      "Epoch:0-102/231, Loss: 3.1095447540283203 \n",
      "Epoch:0-103/231, Loss: 3.254157066345215 \n",
      "Epoch:0-104/231, Loss: 2.962282180786133 \n",
      "Epoch:0-105/231, Loss: 3.129098415374756 \n",
      "Epoch:0-106/231, Loss: 2.757579803466797 \n",
      "Epoch:0-107/231, Loss: 2.844061851501465 \n",
      "Epoch:0-108/231, Loss: 2.5684642791748047 \n",
      "Epoch:0-109/231, Loss: 2.5966572761535645 \n",
      "Epoch:0-110/231, Loss: 2.4259324073791504 \n",
      "Epoch:0-111/231, Loss: 2.2873287200927734 \n",
      "Epoch:0-112/231, Loss: 2.1967997550964355 \n",
      "Epoch:0-113/231, Loss: 2.0712573528289795 \n",
      "Epoch:0-114/231, Loss: 2.164034366607666 \n",
      "Epoch:0-115/231, Loss: 2.684992790222168 \n",
      "Epoch:0-116/231, Loss: 3.174302577972412 \n",
      "Epoch:0-117/231, Loss: 3.071010112762451 \n",
      "Epoch:0-118/231, Loss: 3.117007255554199 \n",
      "Epoch:0-119/231, Loss: 3.108560800552368 \n",
      "Epoch:0-120/231, Loss: 2.941020965576172 \n",
      "Epoch:0-121/231, Loss: 2.957037925720215 \n",
      "Epoch:0-122/231, Loss: 2.750694990158081 \n",
      "Epoch:0-123/231, Loss: 2.6849217414855957 \n",
      "Epoch:0-124/231, Loss: 2.589026689529419 \n",
      "Epoch:0-125/231, Loss: 2.4106926918029785 \n",
      "Epoch:0-126/231, Loss: 2.3099939823150635 \n",
      "Epoch:0-127/231, Loss: 2.2577390670776367 \n",
      "Epoch:0-128/231, Loss: 2.2711706161499023 \n",
      "Epoch:0-129/231, Loss: 2.1184871196746826 \n",
      "Epoch:0-130/231, Loss: 2.1131250858306885 \n",
      "Epoch:0-131/231, Loss: 2.002664804458618 \n",
      "Epoch:0-132/231, Loss: 1.8283360004425049 \n",
      "Epoch:0-133/231, Loss: 1.6282719373703003 \n",
      "Epoch:0-134/231, Loss: 2.012214183807373 \n",
      "Epoch:0-135/231, Loss: 3.1519718170166016 \n",
      "Epoch:0-136/231, Loss: 3.1412270069122314 \n",
      "Epoch:0-137/231, Loss: 3.2458629608154297 \n",
      "Epoch:0-138/231, Loss: 3.2230398654937744 \n",
      "Epoch:0-139/231, Loss: 2.9315683841705322 \n",
      "Epoch:0-140/231, Loss: 2.8691811561584473 \n",
      "Epoch:0-141/231, Loss: 2.8375725746154785 \n",
      "Epoch:0-142/231, Loss: 2.6022772789001465 \n",
      "Epoch:0-143/231, Loss: 2.5421254634857178 \n",
      "Epoch:0-144/231, Loss: 2.5624330043792725 \n",
      "Epoch:0-145/231, Loss: 2.4748048782348633 \n",
      "Epoch:0-146/231, Loss: 2.1584205627441406 \n",
      "Epoch:0-147/231, Loss: 1.9636220932006836 \n",
      "Epoch:0-148/231, Loss: 2.002657175064087 \n",
      "Epoch:0-149/231, Loss: 1.9029362201690674 \n",
      "Epoch:0-150/231, Loss: 1.8419228792190552 \n",
      "Epoch:0-151/231, Loss: 1.6088283061981201 \n",
      "Epoch:0-152/231, Loss: 1.6893863677978516 \n",
      "Epoch:0-153/231, Loss: 1.6910606622695923 \n",
      "Epoch:0-154/231, Loss: 2.2525041103363037 \n",
      "Epoch:0-155/231, Loss: 2.532257318496704 \n",
      "Epoch:0-156/231, Loss: 2.298664093017578 \n",
      "Epoch:0-157/231, Loss: 2.444830894470215 \n",
      "Epoch:0-158/231, Loss: 2.4441561698913574 \n",
      "Epoch:0-159/231, Loss: 2.097163677215576 \n",
      "Epoch:0-160/231, Loss: 2.035071849822998 \n",
      "Epoch:0-161/231, Loss: 2.0428853034973145 \n",
      "Epoch:0-162/231, Loss: 1.8062162399291992 \n",
      "Epoch:0-163/231, Loss: 1.7175564765930176 \n",
      "Epoch:0-164/231, Loss: 1.713989496231079 \n",
      "Epoch:0-165/231, Loss: 1.4530452489852905 \n",
      "Epoch:0-166/231, Loss: 1.268369197845459 \n",
      "Epoch:0-167/231, Loss: 1.0695710182189941 \n",
      "Epoch:0-168/231, Loss: 1.043105125427246 \n",
      "Epoch:0-169/231, Loss: 1.1048377752304077 \n",
      "Epoch:0-170/231, Loss: 0.8707453012466431 \n",
      "Epoch:0-171/231, Loss: 1.000510334968567 \n",
      "Epoch:0-172/231, Loss: 0.8590287566184998 \n",
      "Epoch:0-173/231, Loss: 4.09763240814209 \n",
      "Epoch:0-174/231, Loss: 4.099360942840576 \n",
      "Epoch:0-175/231, Loss: 4.187197685241699 \n",
      "Epoch:0-176/231, Loss: 4.069828987121582 \n",
      "Epoch:0-177/231, Loss: 3.8698158264160156 \n",
      "Epoch:0-178/231, Loss: 3.8225114345550537 \n",
      "Epoch:0-179/231, Loss: 3.7817840576171875 \n",
      "Epoch:0-180/231, Loss: 3.823678731918335 \n",
      "Epoch:0-181/231, Loss: 3.67677903175354 \n",
      "Epoch:0-182/231, Loss: 3.3275208473205566 \n",
      "Epoch:0-183/231, Loss: 3.3765833377838135 \n",
      "Epoch:0-184/231, Loss: 3.075716018676758 \n",
      "Epoch:0-185/231, Loss: 3.0868067741394043 \n",
      "Epoch:0-186/231, Loss: 2.8035898208618164 \n",
      "Epoch:0-187/231, Loss: 2.7170000076293945 \n",
      "Epoch:0-188/231, Loss: 2.383087635040283 \n",
      "Epoch:0-189/231, Loss: 2.469264030456543 \n",
      "Epoch:0-190/231, Loss: 2.156257152557373 \n",
      "Epoch:0-191/231, Loss: 2.131963014602661 \n",
      "Epoch:0-192/231, Loss: 3.045508623123169 \n",
      "Epoch:0-193/231, Loss: 3.3549325466156006 \n",
      "Epoch:0-194/231, Loss: 3.3528127670288086 \n",
      "Epoch:0-195/231, Loss: 3.3495640754699707 \n",
      "Epoch:0-196/231, Loss: 3.2597103118896484 \n",
      "Epoch:0-197/231, Loss: 3.1060097217559814 \n",
      "Epoch:0-198/231, Loss: 2.994302272796631 \n",
      "Epoch:0-199/231, Loss: 2.7213261127471924 \n",
      "Epoch:0-200/231, Loss: 2.929218053817749 \n",
      "Epoch:0-201/231, Loss: 2.838921308517456 \n",
      "Epoch:0-202/231, Loss: 2.4865500926971436 \n",
      "Epoch:0-203/231, Loss: 2.4282233715057373 \n",
      "Epoch:0-204/231, Loss: 2.174602508544922 \n",
      "Epoch:0-205/231, Loss: 2.2415075302124023 \n",
      "Epoch:0-206/231, Loss: 2.0484304428100586 \n",
      "Epoch:0-207/231, Loss: 2.136103868484497 \n",
      "Epoch:0-208/231, Loss: 1.819719910621643 \n",
      "Epoch:0-209/231, Loss: 1.7823903560638428 \n",
      "Epoch:0-210/231, Loss: 1.59646475315094 \n",
      "Epoch:0-211/231, Loss: 1.516427755355835 \n",
      "Epoch:0-212/231, Loss: 1.4655722379684448 \n",
      "Epoch:0-213/231, Loss: 2.6540119647979736 \n",
      "Epoch:0-214/231, Loss: 2.8419933319091797 \n",
      "Epoch:0-215/231, Loss: 3.07062029838562 \n",
      "Epoch:0-216/231, Loss: 2.758667469024658 \n",
      "Epoch:0-217/231, Loss: 2.7857956886291504 \n",
      "Epoch:0-218/231, Loss: 2.7037134170532227 \n",
      "Epoch:0-219/231, Loss: 2.776426315307617 \n",
      "Epoch:0-220/231, Loss: 2.664217233657837 \n",
      "Epoch:0-221/231, Loss: 2.361274480819702 \n",
      "Epoch:0-222/231, Loss: 2.291463613510132 \n",
      "Epoch:0-223/231, Loss: 2.377821922302246 \n",
      "Epoch:0-224/231, Loss: 1.9518730640411377 \n",
      "Epoch:0-225/231, Loss: 1.9161819219589233 \n",
      "Epoch:0-226/231, Loss: 1.830063819885254 \n",
      "Epoch:0-227/231, Loss: 1.6739065647125244 \n",
      "Epoch:0-228/231, Loss: 1.6243046522140503 \n",
      "Epoch:0-229/231, Loss: 1.587241291999817 \n",
      "Epoch:0-230/231, Loss: 1.380682349205017 \n",
      "--------------------------------------------------------------\n",
      "Epoch:0 completed, Total training's Loss: 565.8771777749062, Spend: 1.5664215167363484m\n",
      "Epoch:0, Acc:8.33 on Valid_set, Spend: 0.095 minutes for evaluation\n",
      "--------------------------------------------------------------\n",
      "Epoch:1-0/231, Loss: 2.7667622566223145 \n",
      "Epoch:1-1/231, Loss: 2.787457227706909 \n",
      "Epoch:1-2/231, Loss: 2.8157286643981934 \n",
      "Epoch:1-3/231, Loss: 2.745166778564453 \n",
      "Epoch:1-4/231, Loss: 2.634909152984619 \n",
      "Epoch:1-5/231, Loss: 2.4637060165405273 \n",
      "Epoch:1-6/231, Loss: 2.511657238006592 \n",
      "Epoch:1-7/231, Loss: 2.394693374633789 \n",
      "Epoch:1-8/231, Loss: 2.150066375732422 \n",
      "Epoch:1-9/231, Loss: 2.149394989013672 \n",
      "Epoch:1-10/231, Loss: 2.0684046745300293 \n",
      "Epoch:1-11/231, Loss: 1.9977214336395264 \n",
      "Epoch:1-12/231, Loss: 1.8659471273422241 \n",
      "Epoch:1-13/231, Loss: 1.8550255298614502 \n",
      "Epoch:1-14/231, Loss: 1.5284146070480347 \n",
      "Epoch:1-15/231, Loss: 1.5237306356430054 \n",
      "Epoch:1-16/231, Loss: 1.501660704612732 \n",
      "Epoch:1-17/231, Loss: 1.3059608936309814 \n",
      "Epoch:1-18/231, Loss: 1.1943738460540771 \n",
      "Epoch:1-19/231, Loss: 3.1473488807678223 \n",
      "Epoch:1-20/231, Loss: 3.8598744869232178 \n",
      "Epoch:1-21/231, Loss: 3.6636061668395996 \n",
      "Epoch:1-22/231, Loss: 3.673135757446289 \n",
      "Epoch:1-23/231, Loss: 3.223489284515381 \n",
      "Epoch:1-24/231, Loss: 3.2899041175842285 \n",
      "Epoch:1-25/231, Loss: 3.2338647842407227 \n",
      "Epoch:1-26/231, Loss: 3.1601240634918213 \n",
      "Epoch:1-27/231, Loss: 2.94260573387146 \n",
      "Epoch:1-28/231, Loss: 2.8866400718688965 \n",
      "Epoch:1-29/231, Loss: 2.5581023693084717 \n",
      "Epoch:1-30/231, Loss: 2.456353187561035 \n",
      "Epoch:1-31/231, Loss: 2.3408188819885254 \n",
      "Epoch:1-32/231, Loss: 2.2501513957977295 \n",
      "Epoch:1-33/231, Loss: 2.154341697692871 \n",
      "Epoch:1-34/231, Loss: 1.9938273429870605 \n",
      "Epoch:1-35/231, Loss: 1.8258838653564453 \n",
      "Epoch:1-36/231, Loss: 1.891156792640686 \n",
      "Epoch:1-37/231, Loss: 1.7423522472381592 \n",
      "Epoch:1-38/231, Loss: 2.6438815593719482 \n",
      "Epoch:1-39/231, Loss: 3.5980982780456543 \n",
      "Epoch:1-40/231, Loss: 3.683432102203369 \n",
      "Epoch:1-41/231, Loss: 3.6183512210845947 \n",
      "Epoch:1-42/231, Loss: 3.5923948287963867 \n",
      "Epoch:1-43/231, Loss: 3.3589210510253906 \n",
      "Epoch:1-44/231, Loss: 3.35568904876709 \n",
      "Epoch:1-45/231, Loss: 3.0964035987854004 \n",
      "Epoch:1-46/231, Loss: 2.9921083450317383 \n",
      "Epoch:1-47/231, Loss: 2.7991318702697754 \n",
      "Epoch:1-48/231, Loss: 2.678325653076172 \n",
      "Epoch:1-49/231, Loss: 2.6871180534362793 \n",
      "Epoch:1-50/231, Loss: 2.4882349967956543 \n",
      "Epoch:1-51/231, Loss: 2.4959664344787598 \n",
      "Epoch:1-52/231, Loss: 2.31673002243042 \n",
      "Epoch:1-53/231, Loss: 2.205420970916748 \n",
      "Epoch:1-54/231, Loss: 2.1778225898742676 \n",
      "Epoch:1-55/231, Loss: 1.9190244674682617 \n",
      "Epoch:1-56/231, Loss: 2.0256567001342773 \n",
      "Epoch:1-57/231, Loss: 2.1692662239074707 \n",
      "Epoch:1-58/231, Loss: 2.901710033416748 \n",
      "Epoch:1-59/231, Loss: 2.8667588233947754 \n",
      "Epoch:1-60/231, Loss: 3.0739693641662598 \n",
      "Epoch:1-61/231, Loss: 2.784583330154419 \n",
      "Epoch:1-62/231, Loss: 2.9549686908721924 \n",
      "Epoch:1-63/231, Loss: 2.9714558124542236 \n",
      "Epoch:1-64/231, Loss: 2.891065835952759 \n",
      "Epoch:1-65/231, Loss: 2.646411895751953 \n",
      "Epoch:1-66/231, Loss: 2.503283977508545 \n",
      "Epoch:1-67/231, Loss: 2.333381414413452 \n",
      "Epoch:1-68/231, Loss: 2.251293659210205 \n",
      "Epoch:1-69/231, Loss: 2.2681641578674316 \n",
      "Epoch:1-70/231, Loss: 1.9345886707305908 \n",
      "Epoch:1-71/231, Loss: 2.028811454772949 \n",
      "Epoch:1-72/231, Loss: 1.8627738952636719 \n",
      "Epoch:1-73/231, Loss: 1.7575159072875977 \n",
      "Epoch:1-74/231, Loss: 1.570767879486084 \n",
      "Epoch:1-75/231, Loss: 1.794433832168579 \n",
      "Epoch:1-76/231, Loss: 1.4932763576507568 \n",
      "Epoch:1-77/231, Loss: 3.0236892700195312 \n",
      "Epoch:1-78/231, Loss: 2.82902193069458 \n",
      "Epoch:1-79/231, Loss: 2.9277591705322266 \n",
      "Epoch:1-80/231, Loss: 2.7990353107452393 \n",
      "Epoch:1-81/231, Loss: 2.7826714515686035 \n",
      "Epoch:1-82/231, Loss: 2.5950160026550293 \n",
      "Epoch:1-83/231, Loss: 2.4835257530212402 \n",
      "Epoch:1-84/231, Loss: 2.3550305366516113 \n",
      "Epoch:1-85/231, Loss: 2.263819694519043 \n",
      "Epoch:1-86/231, Loss: 2.285122871398926 \n",
      "Epoch:1-87/231, Loss: 2.155346155166626 \n",
      "Epoch:1-88/231, Loss: 1.9787123203277588 \n",
      "Epoch:1-89/231, Loss: 1.8913607597351074 \n",
      "Epoch:1-90/231, Loss: 1.7896082401275635 \n",
      "Epoch:1-91/231, Loss: 1.7306479215621948 \n",
      "Epoch:1-92/231, Loss: 1.6477532386779785 \n",
      "Epoch:1-93/231, Loss: 1.5743919610977173 \n",
      "Epoch:1-94/231, Loss: 1.5546674728393555 \n",
      "Epoch:1-95/231, Loss: 1.4022912979125977 \n",
      "Epoch:1-96/231, Loss: 3.1459462642669678 \n",
      "Epoch:1-97/231, Loss: 3.109393835067749 \n",
      "Epoch:1-98/231, Loss: 3.0834999084472656 \n",
      "Epoch:1-99/231, Loss: 3.1854162216186523 \n",
      "Epoch:1-100/231, Loss: 3.071026086807251 \n",
      "Epoch:1-101/231, Loss: 2.9365506172180176 \n",
      "Epoch:1-102/231, Loss: 2.793717384338379 \n",
      "Epoch:1-103/231, Loss: 2.5206801891326904 \n",
      "Epoch:1-104/231, Loss: 2.7124972343444824 \n",
      "Epoch:1-105/231, Loss: 2.371926784515381 \n",
      "Epoch:1-106/231, Loss: 2.3200290203094482 \n",
      "Epoch:1-107/231, Loss: 2.1344213485717773 \n",
      "Epoch:1-108/231, Loss: 2.125490188598633 \n",
      "Epoch:1-109/231, Loss: 2.092045307159424 \n",
      "Epoch:1-110/231, Loss: 1.9092601537704468 \n",
      "Epoch:1-111/231, Loss: 1.9190205335617065 \n",
      "Epoch:1-112/231, Loss: 1.8335418701171875 \n",
      "Epoch:1-113/231, Loss: 1.6948127746582031 \n",
      "Epoch:1-114/231, Loss: 1.6114228963851929 \n",
      "Epoch:1-115/231, Loss: 2.5062499046325684 \n",
      "Epoch:1-116/231, Loss: 3.5955700874328613 \n",
      "Epoch:1-117/231, Loss: 3.758000612258911 \n",
      "Epoch:1-118/231, Loss: 3.49094557762146 \n",
      "Epoch:1-119/231, Loss: 3.5105583667755127 \n",
      "Epoch:1-120/231, Loss: 3.2195518016815186 \n",
      "Epoch:1-121/231, Loss: 3.3515207767486572 \n",
      "Epoch:1-122/231, Loss: 3.2257332801818848 \n",
      "Epoch:1-123/231, Loss: 3.136575698852539 \n",
      "Epoch:1-124/231, Loss: 2.844726085662842 \n",
      "Epoch:1-125/231, Loss: 2.6566872596740723 \n",
      "Epoch:1-126/231, Loss: 2.7411386966705322 \n",
      "Epoch:1-127/231, Loss: 2.508700370788574 \n",
      "Epoch:1-128/231, Loss: 2.5781912803649902 \n",
      "Epoch:1-129/231, Loss: 2.4948699474334717 \n",
      "Epoch:1-130/231, Loss: 2.241373062133789 \n",
      "Epoch:1-131/231, Loss: 2.2869820594787598 \n",
      "Epoch:1-132/231, Loss: 2.0340802669525146 \n",
      "Epoch:1-133/231, Loss: 2.0466911792755127 \n",
      "Epoch:1-134/231, Loss: 2.027409315109253 \n",
      "Epoch:1-135/231, Loss: 2.552558183670044 \n",
      "Epoch:1-136/231, Loss: 2.554070472717285 \n",
      "Epoch:1-137/231, Loss: 2.5316450595855713 \n",
      "Epoch:1-138/231, Loss: 2.4665684700012207 \n",
      "Epoch:1-139/231, Loss: 2.437108039855957 \n",
      "Epoch:1-140/231, Loss: 2.179222583770752 \n",
      "Epoch:1-141/231, Loss: 2.0408716201782227 \n",
      "Epoch:1-142/231, Loss: 2.034432888031006 \n",
      "Epoch:1-143/231, Loss: 2.015000104904175 \n",
      "Epoch:1-144/231, Loss: 1.7476986646652222 \n",
      "Epoch:1-145/231, Loss: 1.6375408172607422 \n",
      "Epoch:1-146/231, Loss: 1.5339800119400024 \n",
      "Epoch:1-147/231, Loss: 1.557835340499878 \n",
      "Epoch:1-148/231, Loss: 1.4707326889038086 \n",
      "Epoch:1-149/231, Loss: 1.305957317352295 \n",
      "Epoch:1-150/231, Loss: 1.3301295042037964 \n",
      "Epoch:1-151/231, Loss: 1.2799479961395264 \n",
      "Epoch:1-152/231, Loss: 1.0643768310546875 \n",
      "Epoch:1-153/231, Loss: 1.5507782697677612 \n",
      "Epoch:1-154/231, Loss: 2.865504741668701 \n",
      "Epoch:1-155/231, Loss: 2.792332649230957 \n",
      "Epoch:1-156/231, Loss: 2.858919143676758 \n",
      "Epoch:1-157/231, Loss: 2.82472562789917 \n",
      "Epoch:1-158/231, Loss: 2.6620144844055176 \n",
      "Epoch:1-159/231, Loss: 2.410604476928711 \n",
      "Epoch:1-160/231, Loss: 2.4205446243286133 \n",
      "Epoch:1-161/231, Loss: 2.1642708778381348 \n",
      "Epoch:1-162/231, Loss: 2.138244867324829 \n",
      "Epoch:1-163/231, Loss: 1.9522231817245483 \n",
      "Epoch:1-164/231, Loss: 1.9146254062652588 \n",
      "Epoch:1-165/231, Loss: 1.8273823261260986 \n",
      "Epoch:1-166/231, Loss: 1.6457895040512085 \n",
      "Epoch:1-167/231, Loss: 1.5191984176635742 \n",
      "Epoch:1-168/231, Loss: 1.4292807579040527 \n",
      "Epoch:1-169/231, Loss: 1.4046082496643066 \n",
      "Epoch:1-170/231, Loss: 1.1675084829330444 \n",
      "Epoch:1-171/231, Loss: 1.206323504447937 \n",
      "Epoch:1-172/231, Loss: 1.0120500326156616 \n",
      "Epoch:1-173/231, Loss: 3.5396909713745117 \n",
      "Epoch:1-174/231, Loss: 3.4543700218200684 \n",
      "Epoch:1-175/231, Loss: 3.5723676681518555 \n",
      "Epoch:1-176/231, Loss: 3.5386173725128174 \n",
      "Epoch:1-177/231, Loss: 3.455852508544922 \n",
      "Epoch:1-178/231, Loss: 3.2383031845092773 \n",
      "Epoch:1-179/231, Loss: 3.1456079483032227 \n",
      "Epoch:1-180/231, Loss: 3.191953659057617 \n",
      "Epoch:1-181/231, Loss: 3.0821938514709473 \n",
      "Epoch:1-182/231, Loss: 2.9310619831085205 \n",
      "Epoch:1-183/231, Loss: 2.7371292114257812 \n",
      "Epoch:1-184/231, Loss: 2.5932271480560303 \n",
      "Epoch:1-185/231, Loss: 2.373307704925537 \n",
      "Epoch:1-186/231, Loss: 2.238713026046753 \n",
      "Epoch:1-187/231, Loss: 2.2918834686279297 \n",
      "Epoch:1-188/231, Loss: 2.085165023803711 \n",
      "Epoch:1-189/231, Loss: 2.0450973510742188 \n",
      "Epoch:1-190/231, Loss: 1.730112075805664 \n",
      "Epoch:1-191/231, Loss: 1.7652984857559204 \n",
      "Epoch:1-192/231, Loss: 2.3736486434936523 \n",
      "Epoch:1-193/231, Loss: 2.7343521118164062 \n",
      "Epoch:1-194/231, Loss: 2.8098912239074707 \n",
      "Epoch:1-195/231, Loss: 2.4833383560180664 \n",
      "Epoch:1-196/231, Loss: 2.5839788913726807 \n",
      "Epoch:1-197/231, Loss: 2.351945400238037 \n",
      "Epoch:1-198/231, Loss: 2.2753143310546875 \n",
      "Epoch:1-199/231, Loss: 2.1653459072113037 \n",
      "Epoch:1-200/231, Loss: 2.0950889587402344 \n",
      "Epoch:1-201/231, Loss: 2.272263765335083 \n",
      "Epoch:1-202/231, Loss: 1.9448769092559814 \n",
      "Epoch:1-203/231, Loss: 1.7907898426055908 \n",
      "Epoch:1-204/231, Loss: 1.6714162826538086 \n",
      "Epoch:1-205/231, Loss: 1.7870417833328247 \n",
      "Epoch:1-206/231, Loss: 1.453993320465088 \n",
      "Epoch:1-207/231, Loss: 1.489636778831482 \n",
      "Epoch:1-208/231, Loss: 1.5227258205413818 \n",
      "Epoch:1-209/231, Loss: 1.430933952331543 \n",
      "Epoch:1-210/231, Loss: 1.3704414367675781 \n",
      "Epoch:1-211/231, Loss: 1.309899926185608 \n",
      "Epoch:1-212/231, Loss: 1.289122462272644 \n",
      "Epoch:1-213/231, Loss: 2.332348108291626 \n",
      "Epoch:1-214/231, Loss: 2.6850697994232178 \n",
      "Epoch:1-215/231, Loss: 2.720323324203491 \n",
      "Epoch:1-216/231, Loss: 2.674428939819336 \n",
      "Epoch:1-217/231, Loss: 2.816669464111328 \n",
      "Epoch:1-218/231, Loss: 2.5021274089813232 \n",
      "Epoch:1-219/231, Loss: 2.419595718383789 \n",
      "Epoch:1-220/231, Loss: 2.3359227180480957 \n",
      "Epoch:1-221/231, Loss: 2.2199058532714844 \n",
      "Epoch:1-222/231, Loss: 2.2277474403381348 \n",
      "Epoch:1-223/231, Loss: 2.127272129058838 \n",
      "Epoch:1-224/231, Loss: 1.8785549402236938 \n",
      "Epoch:1-225/231, Loss: 1.8503961563110352 \n",
      "Epoch:1-226/231, Loss: 1.694439172744751 \n",
      "Epoch:1-227/231, Loss: 1.6263208389282227 \n",
      "Epoch:1-228/231, Loss: 1.4693171977996826 \n",
      "Epoch:1-229/231, Loss: 1.618532657623291 \n",
      "Epoch:1-230/231, Loss: 1.3707600831985474 \n",
      "--------------------------------------------------------------\n",
      "Epoch:1 completed, Total training's Loss: 544.5336182117462, Spend: 1.5661518136660257m\n",
      "Epoch:1, Acc:8.33 on Valid_set, Spend: 0.096 minutes for evaluation\n",
      "--------------------------------------------------------------\n",
      "Epoch:2-0/231, Loss: 2.608884811401367 \n",
      "Epoch:2-1/231, Loss: 2.5762510299682617 \n",
      "Epoch:2-2/231, Loss: 2.6096739768981934 \n",
      "Epoch:2-3/231, Loss: 2.442770004272461 \n",
      "Epoch:2-4/231, Loss: 2.530120372772217 \n",
      "Epoch:2-5/231, Loss: 2.21779203414917 \n",
      "Epoch:2-6/231, Loss: 2.321406602859497 \n",
      "Epoch:2-7/231, Loss: 2.2334682941436768 \n",
      "Epoch:2-8/231, Loss: 2.082829475402832 \n",
      "Epoch:2-9/231, Loss: 1.8525992631912231 \n",
      "Epoch:2-10/231, Loss: 1.7987926006317139 \n",
      "Epoch:2-11/231, Loss: 1.7106165885925293 \n",
      "Epoch:2-12/231, Loss: 1.7582687139511108 \n",
      "Epoch:2-13/231, Loss: 1.506717562675476 \n",
      "Epoch:2-14/231, Loss: 1.4922468662261963 \n",
      "Epoch:2-15/231, Loss: 1.2486076354980469 \n",
      "Epoch:2-16/231, Loss: 1.397658348083496 \n",
      "Epoch:2-17/231, Loss: 1.2781000137329102 \n",
      "Epoch:2-18/231, Loss: 1.27107834815979 \n",
      "Epoch:2-19/231, Loss: 2.299424171447754 \n",
      "Epoch:2-20/231, Loss: 2.7961201667785645 \n",
      "Epoch:2-21/231, Loss: 2.9203405380249023 \n",
      "Epoch:2-22/231, Loss: 2.793792247772217 \n",
      "Epoch:2-23/231, Loss: 2.669161558151245 \n",
      "Epoch:2-24/231, Loss: 2.609349012374878 \n",
      "Epoch:2-25/231, Loss: 2.32694149017334 \n",
      "Epoch:2-26/231, Loss: 2.2972612380981445 \n",
      "Epoch:2-27/231, Loss: 2.247028112411499 \n",
      "Epoch:2-28/231, Loss: 2.035294771194458 \n",
      "Epoch:2-29/231, Loss: 2.018078327178955 \n",
      "Epoch:2-30/231, Loss: 1.9776297807693481 \n",
      "Epoch:2-31/231, Loss: 1.7741620540618896 \n",
      "Epoch:2-32/231, Loss: 1.792046308517456 \n",
      "Epoch:2-33/231, Loss: 1.732954978942871 \n",
      "Epoch:2-34/231, Loss: 1.5739734172821045 \n",
      "Epoch:2-35/231, Loss: 1.568204641342163 \n",
      "Epoch:2-36/231, Loss: 1.4481066465377808 \n",
      "Epoch:2-37/231, Loss: 1.3865431547164917 \n",
      "Epoch:2-38/231, Loss: 2.1294989585876465 \n",
      "Epoch:2-39/231, Loss: 2.6565170288085938 \n",
      "Epoch:2-40/231, Loss: 2.7660269737243652 \n",
      "Epoch:2-41/231, Loss: 2.683490753173828 \n",
      "Epoch:2-42/231, Loss: 2.661494731903076 \n",
      "Epoch:2-43/231, Loss: 2.522143602371216 \n",
      "Epoch:2-44/231, Loss: 2.311483860015869 \n",
      "Epoch:2-45/231, Loss: 2.278247594833374 \n",
      "Epoch:2-46/231, Loss: 2.263857841491699 \n",
      "Epoch:2-47/231, Loss: 1.881704330444336 \n",
      "Epoch:2-48/231, Loss: 1.7694244384765625 \n",
      "Epoch:2-49/231, Loss: 1.93104887008667 \n",
      "Epoch:2-50/231, Loss: 1.7456685304641724 \n",
      "Epoch:2-51/231, Loss: 1.6451959609985352 \n",
      "Epoch:2-52/231, Loss: 1.629043698310852 \n",
      "Epoch:2-53/231, Loss: 1.6237834692001343 \n",
      "Epoch:2-54/231, Loss: 1.4097239971160889 \n",
      "Epoch:2-55/231, Loss: 1.3854608535766602 \n",
      "Epoch:2-56/231, Loss: 1.4382096529006958 \n",
      "Epoch:2-57/231, Loss: 1.6696462631225586 \n",
      "Epoch:2-58/231, Loss: 2.588064193725586 \n",
      "Epoch:2-59/231, Loss: 2.497633457183838 \n",
      "Epoch:2-60/231, Loss: 2.29948353767395 \n",
      "Epoch:2-61/231, Loss: 2.2866220474243164 \n",
      "Epoch:2-62/231, Loss: 2.3192896842956543 \n",
      "Epoch:2-63/231, Loss: 2.2355093955993652 \n",
      "Epoch:2-64/231, Loss: 2.2807750701904297 \n",
      "Epoch:2-65/231, Loss: 2.0545554161071777 \n",
      "Epoch:2-66/231, Loss: 1.9930386543273926 \n",
      "Epoch:2-67/231, Loss: 2.0461018085479736 \n",
      "Epoch:2-68/231, Loss: 1.698256492614746 \n",
      "Epoch:2-69/231, Loss: 1.650028944015503 \n",
      "Epoch:2-70/231, Loss: 1.5586490631103516 \n",
      "Epoch:2-71/231, Loss: 1.6035466194152832 \n",
      "Epoch:2-72/231, Loss: 1.5361592769622803 \n",
      "Epoch:2-73/231, Loss: 1.5402785539627075 \n",
      "Epoch:2-74/231, Loss: 1.5058858394622803 \n",
      "Epoch:2-75/231, Loss: 1.4489514827728271 \n",
      "Epoch:2-76/231, Loss: 1.2157585620880127 \n",
      "Epoch:2-77/231, Loss: 2.815282106399536 \n",
      "Epoch:2-78/231, Loss: 2.7141942977905273 \n",
      "Epoch:2-79/231, Loss: 2.6749186515808105 \n",
      "Epoch:2-80/231, Loss: 2.5053579807281494 \n",
      "Epoch:2-81/231, Loss: 2.4672679901123047 \n",
      "Epoch:2-82/231, Loss: 2.24851655960083 \n",
      "Epoch:2-83/231, Loss: 2.135951519012451 \n",
      "Epoch:2-84/231, Loss: 1.85231614112854 \n",
      "Epoch:2-85/231, Loss: 1.8424662351608276 \n",
      "Epoch:2-86/231, Loss: 1.8218276500701904 \n",
      "Epoch:2-87/231, Loss: 1.5774928331375122 \n",
      "Epoch:2-88/231, Loss: 1.6964234113693237 \n",
      "Epoch:2-89/231, Loss: 1.4285751581192017 \n",
      "Epoch:2-90/231, Loss: 1.594264030456543 \n",
      "Epoch:2-91/231, Loss: 1.4498629570007324 \n",
      "Epoch:2-92/231, Loss: 1.4472122192382812 \n",
      "Epoch:2-93/231, Loss: 1.430781602859497 \n",
      "Epoch:2-94/231, Loss: 1.3945680856704712 \n",
      "Epoch:2-95/231, Loss: 1.4157123565673828 \n",
      "Epoch:2-96/231, Loss: 2.566967725753784 \n",
      "Epoch:2-97/231, Loss: 2.613729476928711 \n",
      "Epoch:2-98/231, Loss: 2.7291154861450195 \n",
      "Epoch:2-99/231, Loss: 2.635556221008301 \n",
      "Epoch:2-100/231, Loss: 2.6374013423919678 \n",
      "Epoch:2-101/231, Loss: 2.3171098232269287 \n",
      "Epoch:2-102/231, Loss: 2.3041939735412598 \n",
      "Epoch:2-103/231, Loss: 2.187312126159668 \n",
      "Epoch:2-104/231, Loss: 2.197037696838379 \n",
      "Epoch:2-105/231, Loss: 1.9161577224731445 \n",
      "Epoch:2-106/231, Loss: 1.7950210571289062 \n",
      "Epoch:2-107/231, Loss: 1.7174090147018433 \n",
      "Epoch:2-108/231, Loss: 1.731069564819336 \n",
      "Epoch:2-109/231, Loss: 1.8562815189361572 \n",
      "Epoch:2-110/231, Loss: 1.5306894779205322 \n",
      "Epoch:2-111/231, Loss: 1.5469099283218384 \n",
      "Epoch:2-112/231, Loss: 1.5019066333770752 \n",
      "Epoch:2-113/231, Loss: 1.415200114250183 \n",
      "Epoch:2-114/231, Loss: 1.51802659034729 \n",
      "Epoch:2-115/231, Loss: 2.081460475921631 \n",
      "Epoch:2-116/231, Loss: 2.7532765865325928 \n",
      "Epoch:2-117/231, Loss: 2.649601459503174 \n",
      "Epoch:2-118/231, Loss: 2.836850166320801 \n",
      "Epoch:2-119/231, Loss: 2.596043109893799 \n",
      "Epoch:2-120/231, Loss: 2.5255603790283203 \n",
      "Epoch:2-121/231, Loss: 2.4127349853515625 \n",
      "Epoch:2-122/231, Loss: 2.3849072456359863 \n",
      "Epoch:2-123/231, Loss: 2.3060827255249023 \n",
      "Epoch:2-124/231, Loss: 2.3261425495147705 \n",
      "Epoch:2-125/231, Loss: 2.183993339538574 \n",
      "Epoch:2-126/231, Loss: 2.013397455215454 \n",
      "Epoch:2-127/231, Loss: 2.0377309322357178 \n",
      "Epoch:2-128/231, Loss: 1.9755827188491821 \n",
      "Epoch:2-129/231, Loss: 1.8429374694824219 \n",
      "Epoch:2-130/231, Loss: 1.7653915882110596 \n",
      "Epoch:2-131/231, Loss: 1.701286792755127 \n",
      "Epoch:2-132/231, Loss: 1.4910274744033813 \n",
      "Epoch:2-133/231, Loss: 1.6659870147705078 \n",
      "Epoch:2-134/231, Loss: 1.7443318367004395 \n",
      "Epoch:2-135/231, Loss: 2.560105800628662 \n",
      "Epoch:2-136/231, Loss: 2.51593279838562 \n",
      "Epoch:2-137/231, Loss: 2.4399945735931396 \n",
      "Epoch:2-138/231, Loss: 2.4283080101013184 \n",
      "Epoch:2-139/231, Loss: 2.2979652881622314 \n",
      "Epoch:2-140/231, Loss: 2.1911797523498535 \n",
      "Epoch:2-141/231, Loss: 2.046985626220703 \n",
      "Epoch:2-142/231, Loss: 1.9635114669799805 \n",
      "Epoch:2-143/231, Loss: 1.8676364421844482 \n",
      "Epoch:2-144/231, Loss: 1.6872320175170898 \n",
      "Epoch:2-145/231, Loss: 1.7691633701324463 \n",
      "Epoch:2-146/231, Loss: 1.67220139503479 \n",
      "Epoch:2-147/231, Loss: 1.564323902130127 \n",
      "Epoch:2-148/231, Loss: 1.5789824724197388 \n",
      "Epoch:2-149/231, Loss: 1.3324744701385498 \n",
      "Epoch:2-150/231, Loss: 1.3274060487747192 \n",
      "Epoch:2-151/231, Loss: 1.3633732795715332 \n",
      "Epoch:2-152/231, Loss: 1.2787631750106812 \n",
      "Epoch:2-153/231, Loss: 1.6713237762451172 \n",
      "Epoch:2-154/231, Loss: 2.78079891204834 \n",
      "Epoch:2-155/231, Loss: 2.895055055618286 \n",
      "Epoch:2-156/231, Loss: 2.8036880493164062 \n",
      "Epoch:2-157/231, Loss: 2.778789520263672 \n",
      "Epoch:2-158/231, Loss: 2.7221145629882812 \n",
      "Epoch:2-159/231, Loss: 2.4235572814941406 \n",
      "Epoch:2-160/231, Loss: 2.3969783782958984 \n",
      "Epoch:2-161/231, Loss: 2.0928215980529785 \n",
      "Epoch:2-162/231, Loss: 2.1099298000335693 \n",
      "Epoch:2-163/231, Loss: 2.0289316177368164 \n",
      "Epoch:2-164/231, Loss: 1.904526948928833 \n",
      "Epoch:2-165/231, Loss: 1.8241945505142212 \n",
      "Epoch:2-166/231, Loss: 1.8646552562713623 \n",
      "Epoch:2-167/231, Loss: 1.5958149433135986 \n",
      "Epoch:2-168/231, Loss: 1.5345475673675537 \n",
      "Epoch:2-169/231, Loss: 1.5009511709213257 \n",
      "Epoch:2-170/231, Loss: 1.4155277013778687 \n",
      "Epoch:2-171/231, Loss: 1.2426551580429077 \n",
      "Epoch:2-172/231, Loss: 1.2028822898864746 \n",
      "Epoch:2-173/231, Loss: 2.4301042556762695 \n",
      "Epoch:2-174/231, Loss: 2.610384464263916 \n",
      "Epoch:2-175/231, Loss: 2.411797046661377 \n",
      "Epoch:2-176/231, Loss: 2.5676212310791016 \n",
      "Epoch:2-177/231, Loss: 2.3413631916046143 \n",
      "Epoch:2-178/231, Loss: 2.495908260345459 \n",
      "Epoch:2-179/231, Loss: 2.2337913513183594 \n",
      "Epoch:2-180/231, Loss: 2.150076389312744 \n",
      "Epoch:2-181/231, Loss: 2.0581295490264893 \n",
      "Epoch:2-182/231, Loss: 1.9359718561172485 \n",
      "Epoch:2-183/231, Loss: 1.9027199745178223 \n",
      "Epoch:2-184/231, Loss: 1.714177131652832 \n",
      "Epoch:2-185/231, Loss: 1.820212721824646 \n",
      "Epoch:2-186/231, Loss: 1.5810744762420654 \n",
      "Epoch:2-187/231, Loss: 1.5358421802520752 \n",
      "Epoch:2-188/231, Loss: 1.4547605514526367 \n",
      "Epoch:2-189/231, Loss: 1.2956464290618896 \n",
      "Epoch:2-190/231, Loss: 1.3532294034957886 \n",
      "Epoch:2-191/231, Loss: 1.311458945274353 \n",
      "Epoch:2-192/231, Loss: 2.241140127182007 \n",
      "Epoch:2-193/231, Loss: 2.7556562423706055 \n",
      "Epoch:2-194/231, Loss: 2.888996124267578 \n",
      "Epoch:2-195/231, Loss: 2.5950927734375 \n",
      "Epoch:2-196/231, Loss: 2.566488265991211 \n",
      "Epoch:2-197/231, Loss: 2.5349783897399902 \n",
      "Epoch:2-198/231, Loss: 2.318119525909424 \n",
      "Epoch:2-199/231, Loss: 2.3102121353149414 \n",
      "Epoch:2-200/231, Loss: 2.106325626373291 \n",
      "Epoch:2-201/231, Loss: 2.032646417617798 \n",
      "Epoch:2-202/231, Loss: 1.8393447399139404 \n",
      "Epoch:2-203/231, Loss: 1.8163151741027832 \n",
      "Epoch:2-204/231, Loss: 1.705523133277893 \n",
      "Epoch:2-205/231, Loss: 1.5059256553649902 \n",
      "Epoch:2-206/231, Loss: 1.5541799068450928 \n",
      "Epoch:2-207/231, Loss: 1.4432129859924316 \n",
      "Epoch:2-208/231, Loss: 1.5438082218170166 \n",
      "Epoch:2-209/231, Loss: 1.2326197624206543 \n",
      "Epoch:2-210/231, Loss: 1.3140616416931152 \n",
      "Epoch:2-211/231, Loss: 1.2069499492645264 \n",
      "Epoch:2-212/231, Loss: 1.1426621675491333 \n",
      "Epoch:2-213/231, Loss: 2.234600305557251 \n",
      "Epoch:2-214/231, Loss: 2.5355405807495117 \n",
      "Epoch:2-215/231, Loss: 2.51173996925354 \n",
      "Epoch:2-216/231, Loss: 2.2583465576171875 \n",
      "Epoch:2-217/231, Loss: 2.534005641937256 \n",
      "Epoch:2-218/231, Loss: 2.3618242740631104 \n",
      "Epoch:2-219/231, Loss: 2.209073781967163 \n",
      "Epoch:2-220/231, Loss: 2.0483553409576416 \n",
      "Epoch:2-221/231, Loss: 1.9861868619918823 \n",
      "Epoch:2-222/231, Loss: 1.8810524940490723 \n",
      "Epoch:2-223/231, Loss: 1.7260490655899048 \n",
      "Epoch:2-224/231, Loss: 1.8487861156463623 \n",
      "Epoch:2-225/231, Loss: 1.6225221157073975 \n",
      "Epoch:2-226/231, Loss: 1.5722206830978394 \n",
      "Epoch:2-227/231, Loss: 1.6219055652618408 \n",
      "Epoch:2-228/231, Loss: 1.368631362915039 \n",
      "Epoch:2-229/231, Loss: 1.38853120803833 \n",
      "Epoch:2-230/231, Loss: 1.3911243677139282 \n",
      "--------------------------------------------------------------\n",
      "Epoch:2 completed, Total training's Loss: 460.12327694892883, Spend: 1.5657010595003764m\n",
      "Epoch:2, Acc:8.33 on Valid_set, Spend: 0.095 minutes for evaluation\n",
      "--------------------------------------------------------------\n",
      "Epoch:3-0/231, Loss: 2.407700777053833 \n",
      "Epoch:3-1/231, Loss: 2.609433650970459 \n",
      "Epoch:3-2/231, Loss: 2.318368911743164 \n",
      "Epoch:3-3/231, Loss: 2.1793057918548584 \n",
      "Epoch:3-4/231, Loss: 2.047999858856201 \n",
      "Epoch:3-5/231, Loss: 1.9814435243606567 \n",
      "Epoch:3-6/231, Loss: 1.9852004051208496 \n",
      "Epoch:3-7/231, Loss: 1.8196864128112793 \n",
      "Epoch:3-8/231, Loss: 1.8472545146942139 \n",
      "Epoch:3-9/231, Loss: 1.6368768215179443 \n",
      "Epoch:3-10/231, Loss: 1.7284669876098633 \n",
      "Epoch:3-11/231, Loss: 1.5792627334594727 \n",
      "Epoch:3-12/231, Loss: 1.4662131071090698 \n",
      "Epoch:3-13/231, Loss: 1.5001413822174072 \n",
      "Epoch:3-14/231, Loss: 1.3260278701782227 \n",
      "Epoch:3-15/231, Loss: 1.351686716079712 \n",
      "Epoch:3-16/231, Loss: 1.2253868579864502 \n",
      "Epoch:3-17/231, Loss: 1.1158292293548584 \n",
      "Epoch:3-18/231, Loss: 1.1835412979125977 \n",
      "Epoch:3-19/231, Loss: 2.2637195587158203 \n",
      "Epoch:3-20/231, Loss: 2.8060052394866943 \n",
      "Epoch:3-21/231, Loss: 2.6759097576141357 \n",
      "Epoch:3-22/231, Loss: 2.6141915321350098 \n",
      "Epoch:3-23/231, Loss: 2.3631176948547363 \n",
      "Epoch:3-24/231, Loss: 2.3407247066497803 \n",
      "Epoch:3-25/231, Loss: 2.1849985122680664 \n",
      "Epoch:3-26/231, Loss: 2.028085708618164 \n",
      "Epoch:3-27/231, Loss: 1.9595646858215332 \n",
      "Epoch:3-28/231, Loss: 1.79118013381958 \n",
      "Epoch:3-29/231, Loss: 1.8968732357025146 \n",
      "Epoch:3-30/231, Loss: 1.8265324831008911 \n",
      "Epoch:3-31/231, Loss: 1.5299639701843262 \n",
      "Epoch:3-32/231, Loss: 1.322731852531433 \n",
      "Epoch:3-33/231, Loss: 1.4291839599609375 \n",
      "Epoch:3-34/231, Loss: 1.5397268533706665 \n",
      "Epoch:3-35/231, Loss: 1.2454736232757568 \n",
      "Epoch:3-36/231, Loss: 1.2768092155456543 \n",
      "Epoch:3-37/231, Loss: 1.199593186378479 \n",
      "Epoch:3-38/231, Loss: 1.942063570022583 \n",
      "Epoch:3-39/231, Loss: 2.57283616065979 \n",
      "Epoch:3-40/231, Loss: 2.571845531463623 \n",
      "Epoch:3-41/231, Loss: 2.3167033195495605 \n",
      "Epoch:3-42/231, Loss: 2.466519832611084 \n",
      "Epoch:3-43/231, Loss: 2.171814441680908 \n",
      "Epoch:3-44/231, Loss: 1.9355759620666504 \n",
      "Epoch:3-45/231, Loss: 1.7869625091552734 \n",
      "Epoch:3-46/231, Loss: 1.6153721809387207 \n",
      "Epoch:3-47/231, Loss: 1.5548419952392578 \n",
      "Epoch:3-48/231, Loss: 1.4229907989501953 \n",
      "Epoch:3-49/231, Loss: 1.60971200466156 \n",
      "Epoch:3-50/231, Loss: 1.5179213285446167 \n",
      "Epoch:3-51/231, Loss: 1.428067922592163 \n",
      "Epoch:3-52/231, Loss: 1.360416054725647 \n",
      "Epoch:3-53/231, Loss: 1.2095586061477661 \n",
      "Epoch:3-54/231, Loss: 1.3995143175125122 \n",
      "Epoch:3-55/231, Loss: 1.2550798654556274 \n",
      "Epoch:3-56/231, Loss: 1.2023309469223022 \n",
      "Epoch:3-57/231, Loss: 1.5006228685379028 \n",
      "Epoch:3-58/231, Loss: 2.3793187141418457 \n",
      "Epoch:3-59/231, Loss: 2.2420504093170166 \n",
      "Epoch:3-60/231, Loss: 2.2695133686065674 \n",
      "Epoch:3-61/231, Loss: 2.361436367034912 \n",
      "Epoch:3-62/231, Loss: 2.104069471359253 \n",
      "Epoch:3-63/231, Loss: 2.242103338241577 \n",
      "Epoch:3-64/231, Loss: 1.8255784511566162 \n",
      "Epoch:3-65/231, Loss: 1.8370999097824097 \n",
      "Epoch:3-66/231, Loss: 1.6899800300598145 \n",
      "Epoch:3-67/231, Loss: 1.7097033262252808 \n",
      "Epoch:3-68/231, Loss: 1.5985907316207886 \n",
      "Epoch:3-69/231, Loss: 1.6141771078109741 \n",
      "Epoch:3-70/231, Loss: 1.4901342391967773 \n",
      "Epoch:3-71/231, Loss: 1.434434413909912 \n",
      "Epoch:3-72/231, Loss: 1.3967907428741455 \n",
      "Epoch:3-73/231, Loss: 1.4545857906341553 \n",
      "Epoch:3-74/231, Loss: 1.365425705909729 \n",
      "Epoch:3-75/231, Loss: 1.220630168914795 \n",
      "Epoch:3-76/231, Loss: 1.075727105140686 \n",
      "Epoch:3-77/231, Loss: 2.615203380584717 \n",
      "Epoch:3-78/231, Loss: 2.810030937194824 \n",
      "Epoch:3-79/231, Loss: 2.5088634490966797 \n",
      "Epoch:3-80/231, Loss: 2.3686344623565674 \n",
      "Epoch:3-81/231, Loss: 2.429373025894165 \n",
      "Epoch:3-82/231, Loss: 2.1899263858795166 \n",
      "Epoch:3-83/231, Loss: 1.906548023223877 \n",
      "Epoch:3-84/231, Loss: 1.7590495347976685 \n",
      "Epoch:3-85/231, Loss: 1.6009455919265747 \n",
      "Epoch:3-86/231, Loss: 1.5762524604797363 \n",
      "Epoch:3-87/231, Loss: 1.6827222108840942 \n",
      "Epoch:3-88/231, Loss: 1.5412501096725464 \n",
      "Epoch:3-89/231, Loss: 1.5473562479019165 \n",
      "Epoch:3-90/231, Loss: 1.4106109142303467 \n",
      "Epoch:3-91/231, Loss: 1.295007586479187 \n",
      "Epoch:3-92/231, Loss: 1.6084538698196411 \n",
      "Epoch:3-93/231, Loss: 1.436692237854004 \n",
      "Epoch:3-94/231, Loss: 1.2812674045562744 \n",
      "Epoch:3-95/231, Loss: 1.3133082389831543 \n",
      "Epoch:3-96/231, Loss: 2.1976430416107178 \n",
      "Epoch:3-97/231, Loss: 2.3800978660583496 \n",
      "Epoch:3-98/231, Loss: 2.37351655960083 \n",
      "Epoch:3-99/231, Loss: 2.2887284755706787 \n",
      "Epoch:3-100/231, Loss: 2.314723491668701 \n",
      "Epoch:3-101/231, Loss: 2.061552047729492 \n",
      "Epoch:3-102/231, Loss: 1.8051725625991821 \n",
      "Epoch:3-103/231, Loss: 1.8120139837265015 \n",
      "Epoch:3-104/231, Loss: 1.982069492340088 \n",
      "Epoch:3-105/231, Loss: 1.4585587978363037 \n",
      "Epoch:3-106/231, Loss: 1.6028941869735718 \n",
      "Epoch:3-107/231, Loss: 1.3500319719314575 \n",
      "Epoch:3-108/231, Loss: 1.3902493715286255 \n",
      "Epoch:3-109/231, Loss: 1.3190584182739258 \n",
      "Epoch:3-110/231, Loss: 1.3093702793121338 \n",
      "Epoch:3-111/231, Loss: 1.2893576622009277 \n",
      "Epoch:3-112/231, Loss: 1.267460584640503 \n",
      "Epoch:3-113/231, Loss: 1.3259413242340088 \n",
      "Epoch:3-114/231, Loss: 1.3796724081039429 \n",
      "Epoch:3-115/231, Loss: 1.6646838188171387 \n",
      "Epoch:3-116/231, Loss: 2.5333776473999023 \n",
      "Epoch:3-117/231, Loss: 2.527480125427246 \n",
      "Epoch:3-118/231, Loss: 2.345116138458252 \n",
      "Epoch:3-119/231, Loss: 2.19144868850708 \n",
      "Epoch:3-120/231, Loss: 2.0011000633239746 \n",
      "Epoch:3-121/231, Loss: 2.077678680419922 \n",
      "Epoch:3-122/231, Loss: 2.03764009475708 \n",
      "Epoch:3-123/231, Loss: 2.084852933883667 \n",
      "Epoch:3-124/231, Loss: 1.872282862663269 \n",
      "Epoch:3-125/231, Loss: 1.7874938249588013 \n",
      "Epoch:3-126/231, Loss: 1.8878028392791748 \n",
      "Epoch:3-127/231, Loss: 1.5127123594284058 \n",
      "Epoch:3-128/231, Loss: 1.541090965270996 \n",
      "Epoch:3-129/231, Loss: 1.477070927619934 \n",
      "Epoch:3-130/231, Loss: 1.4099897146224976 \n",
      "Epoch:3-131/231, Loss: 1.415932536125183 \n",
      "Epoch:3-132/231, Loss: 1.342081069946289 \n",
      "Epoch:3-133/231, Loss: 1.3882057666778564 \n",
      "Epoch:3-134/231, Loss: 1.4033018350601196 \n",
      "Epoch:3-135/231, Loss: 2.317800521850586 \n",
      "Epoch:3-136/231, Loss: 2.38874888420105 \n",
      "Epoch:3-137/231, Loss: 2.2446811199188232 \n",
      "Epoch:3-138/231, Loss: 2.1703763008117676 \n",
      "Epoch:3-139/231, Loss: 1.9792284965515137 \n",
      "Epoch:3-140/231, Loss: 1.8349374532699585 \n",
      "Epoch:3-141/231, Loss: 1.942987322807312 \n",
      "Epoch:3-142/231, Loss: 1.9913041591644287 \n",
      "Epoch:3-143/231, Loss: 1.6701042652130127 \n",
      "Epoch:3-144/231, Loss: 1.4724798202514648 \n",
      "Epoch:3-145/231, Loss: 1.4735288619995117 \n",
      "Epoch:3-146/231, Loss: 1.4850088357925415 \n",
      "Epoch:3-147/231, Loss: 1.403754472732544 \n",
      "Epoch:3-148/231, Loss: 1.3913028240203857 \n",
      "Epoch:3-149/231, Loss: 1.2309508323669434 \n",
      "Epoch:3-150/231, Loss: 1.259016990661621 \n",
      "Epoch:3-151/231, Loss: 1.1516718864440918 \n",
      "Epoch:3-152/231, Loss: 1.0781952142715454 \n",
      "Epoch:3-153/231, Loss: 1.4627176523208618 \n",
      "Epoch:3-154/231, Loss: 2.682908773422241 \n",
      "Epoch:3-155/231, Loss: 2.711264133453369 \n",
      "Epoch:3-156/231, Loss: 2.522326946258545 \n",
      "Epoch:3-157/231, Loss: 2.707796573638916 \n",
      "Epoch:3-158/231, Loss: 2.8021602630615234 \n",
      "Epoch:3-159/231, Loss: 2.35788893699646 \n",
      "Epoch:3-160/231, Loss: 2.3828041553497314 \n",
      "Epoch:3-161/231, Loss: 2.251312255859375 \n",
      "Epoch:3-162/231, Loss: 2.0257163047790527 \n",
      "Epoch:3-163/231, Loss: 1.8184607028961182 \n",
      "Epoch:3-164/231, Loss: 1.7884814739227295 \n",
      "Epoch:3-165/231, Loss: 1.6813395023345947 \n",
      "Epoch:3-166/231, Loss: 1.6121079921722412 \n",
      "Epoch:3-167/231, Loss: 1.625592589378357 \n",
      "Epoch:3-168/231, Loss: 1.5498629808425903 \n",
      "Epoch:3-169/231, Loss: 1.2948681116104126 \n",
      "Epoch:3-170/231, Loss: 1.329304575920105 \n",
      "Epoch:3-171/231, Loss: 1.2240047454833984 \n",
      "Epoch:3-172/231, Loss: 1.209412693977356 \n",
      "Epoch:3-173/231, Loss: 2.446506977081299 \n",
      "Epoch:3-174/231, Loss: 2.627861499786377 \n",
      "Epoch:3-175/231, Loss: 2.6670312881469727 \n",
      "Epoch:3-176/231, Loss: 2.4669747352600098 \n",
      "Epoch:3-177/231, Loss: 2.291696310043335 \n",
      "Epoch:3-178/231, Loss: 2.347666025161743 \n",
      "Epoch:3-179/231, Loss: 2.1805453300476074 \n",
      "Epoch:3-180/231, Loss: 1.917301893234253 \n",
      "Epoch:3-181/231, Loss: 1.8654406070709229 \n",
      "Epoch:3-182/231, Loss: 1.8005988597869873 \n",
      "Epoch:3-183/231, Loss: 1.7188115119934082 \n",
      "Epoch:3-184/231, Loss: 1.5865156650543213 \n",
      "Epoch:3-185/231, Loss: 1.5204830169677734 \n",
      "Epoch:3-186/231, Loss: 1.5921612977981567 \n",
      "Epoch:3-187/231, Loss: 1.2865631580352783 \n",
      "Epoch:3-188/231, Loss: 1.5359225273132324 \n",
      "Epoch:3-189/231, Loss: 1.4225640296936035 \n",
      "Epoch:3-190/231, Loss: 1.364877462387085 \n",
      "Epoch:3-191/231, Loss: 1.232717752456665 \n",
      "Epoch:3-192/231, Loss: 2.1060872077941895 \n",
      "Epoch:3-193/231, Loss: 2.406360626220703 \n",
      "Epoch:3-194/231, Loss: 2.3660361766815186 \n",
      "Epoch:3-195/231, Loss: 2.16516375541687 \n",
      "Epoch:3-196/231, Loss: 2.1088695526123047 \n",
      "Epoch:3-197/231, Loss: 1.9824753999710083 \n",
      "Epoch:3-198/231, Loss: 1.8111966848373413 \n",
      "Epoch:3-199/231, Loss: 1.6337299346923828 \n",
      "Epoch:3-200/231, Loss: 1.7215276956558228 \n",
      "Epoch:3-201/231, Loss: 1.6514004468917847 \n",
      "Epoch:3-202/231, Loss: 1.8922722339630127 \n",
      "Epoch:3-203/231, Loss: 1.40682053565979 \n",
      "Epoch:3-204/231, Loss: 1.2850522994995117 \n",
      "Epoch:3-205/231, Loss: 1.4006516933441162 \n",
      "Epoch:3-206/231, Loss: 1.2655787467956543 \n",
      "Epoch:3-207/231, Loss: 1.2990106344223022 \n",
      "Epoch:3-208/231, Loss: 1.2764520645141602 \n",
      "Epoch:3-209/231, Loss: 1.1674277782440186 \n",
      "Epoch:3-210/231, Loss: 1.1288115978240967 \n",
      "Epoch:3-211/231, Loss: 1.1425985097885132 \n",
      "Epoch:3-212/231, Loss: 1.1087068319320679 \n",
      "Epoch:3-213/231, Loss: 2.202803134918213 \n",
      "Epoch:3-214/231, Loss: 2.3891570568084717 \n",
      "Epoch:3-215/231, Loss: 2.43123722076416 \n",
      "Epoch:3-216/231, Loss: 2.1899030208587646 \n",
      "Epoch:3-217/231, Loss: 2.2275567054748535 \n",
      "Epoch:3-218/231, Loss: 2.0722484588623047 \n",
      "Epoch:3-219/231, Loss: 2.0747857093811035 \n",
      "Epoch:3-220/231, Loss: 1.7689523696899414 \n",
      "Epoch:3-221/231, Loss: 1.516498327255249 \n",
      "Epoch:3-222/231, Loss: 1.769702672958374 \n",
      "Epoch:3-223/231, Loss: 1.5038633346557617 \n",
      "Epoch:3-224/231, Loss: 1.4224191904067993 \n",
      "Epoch:3-225/231, Loss: 1.5594089031219482 \n",
      "Epoch:3-226/231, Loss: 1.3255075216293335 \n",
      "Epoch:3-227/231, Loss: 1.3887383937835693 \n",
      "Epoch:3-228/231, Loss: 1.2814512252807617 \n",
      "Epoch:3-229/231, Loss: 1.2545279264450073 \n",
      "Epoch:3-230/231, Loss: 1.1884245872497559 \n",
      "--------------------------------------------------------------\n",
      "Epoch:3 completed, Total training's Loss: 413.33932960033417, Spend: 1.5671902418136596m\n",
      "Epoch:3, Acc:8.33 on Valid_set, Spend: 0.096 minutes for evaluation\n",
      "--------------------------------------------------------------\n",
      "Epoch:4-0/231, Loss: 2.1795783042907715 \n",
      "Epoch:4-1/231, Loss: 2.294381618499756 \n",
      "Epoch:4-2/231, Loss: 2.1832330226898193 \n",
      "Epoch:4-3/231, Loss: 2.1861393451690674 \n",
      "Epoch:4-4/231, Loss: 2.104389190673828 \n",
      "Epoch:4-5/231, Loss: 1.9757051467895508 \n",
      "Epoch:4-6/231, Loss: 1.9373674392700195 \n",
      "Epoch:4-7/231, Loss: 1.800087809562683 \n",
      "Epoch:4-8/231, Loss: 1.694821834564209 \n",
      "Epoch:4-9/231, Loss: 1.576896071434021 \n",
      "Epoch:4-10/231, Loss: 1.5080617666244507 \n",
      "Epoch:4-11/231, Loss: 1.4576959609985352 \n",
      "Epoch:4-12/231, Loss: 1.3408643007278442 \n",
      "Epoch:4-13/231, Loss: 1.4461252689361572 \n",
      "Epoch:4-14/231, Loss: 1.3031551837921143 \n",
      "Epoch:4-15/231, Loss: 1.339032769203186 \n",
      "Epoch:4-16/231, Loss: 1.2212245464324951 \n",
      "Epoch:4-17/231, Loss: 1.122917890548706 \n",
      "Epoch:4-18/231, Loss: 1.222366452217102 \n",
      "Epoch:4-19/231, Loss: 1.889909267425537 \n",
      "Epoch:4-20/231, Loss: 2.2516956329345703 \n",
      "Epoch:4-21/231, Loss: 2.3403518199920654 \n",
      "Epoch:4-22/231, Loss: 2.1621522903442383 \n",
      "Epoch:4-23/231, Loss: 1.8619261980056763 \n",
      "Epoch:4-24/231, Loss: 1.8025652170181274 \n",
      "Epoch:4-25/231, Loss: 1.7462717294692993 \n",
      "Epoch:4-26/231, Loss: 1.8298170566558838 \n",
      "Epoch:4-27/231, Loss: 1.6647136211395264 \n",
      "Epoch:4-28/231, Loss: 1.5235626697540283 \n",
      "Epoch:4-29/231, Loss: 1.4281595945358276 \n",
      "Epoch:4-30/231, Loss: 1.4858239889144897 \n",
      "Epoch:4-31/231, Loss: 1.4304225444793701 \n",
      "Epoch:4-32/231, Loss: 1.3670532703399658 \n",
      "Epoch:4-33/231, Loss: 1.327794075012207 \n",
      "Epoch:4-34/231, Loss: 1.214273452758789 \n",
      "Epoch:4-35/231, Loss: 1.1449439525604248 \n",
      "Epoch:4-36/231, Loss: 0.9832301139831543 \n",
      "Epoch:4-37/231, Loss: 1.0366134643554688 \n",
      "Epoch:4-38/231, Loss: 1.7093418836593628 \n",
      "Epoch:4-39/231, Loss: 2.9040679931640625 \n",
      "Epoch:4-40/231, Loss: 2.570950508117676 \n",
      "Epoch:4-41/231, Loss: 2.5276737213134766 \n",
      "Epoch:4-42/231, Loss: 2.5109610557556152 \n",
      "Epoch:4-43/231, Loss: 2.3141026496887207 \n",
      "Epoch:4-44/231, Loss: 2.0120596885681152 \n",
      "Epoch:4-45/231, Loss: 1.9349673986434937 \n",
      "Epoch:4-46/231, Loss: 1.551079273223877 \n",
      "Epoch:4-47/231, Loss: 1.6871562004089355 \n",
      "Epoch:4-48/231, Loss: 1.568990707397461 \n",
      "Epoch:4-49/231, Loss: 1.5495891571044922 \n",
      "Epoch:4-50/231, Loss: 1.4675053358078003 \n",
      "Epoch:4-51/231, Loss: 1.274179458618164 \n",
      "Epoch:4-52/231, Loss: 1.4808977842330933 \n",
      "Epoch:4-53/231, Loss: 1.2479139566421509 \n",
      "Epoch:4-54/231, Loss: 1.2150764465332031 \n",
      "Epoch:4-55/231, Loss: 1.1007428169250488 \n",
      "Epoch:4-56/231, Loss: 1.0267882347106934 \n",
      "Epoch:4-57/231, Loss: 1.5076215267181396 \n",
      "Epoch:4-58/231, Loss: 2.2855427265167236 \n",
      "Epoch:4-59/231, Loss: 2.4553139209747314 \n",
      "Epoch:4-60/231, Loss: 2.271638870239258 \n",
      "Epoch:4-61/231, Loss: 2.104591131210327 \n",
      "Epoch:4-62/231, Loss: 1.9951794147491455 \n",
      "Epoch:4-63/231, Loss: 1.9964500665664673 \n",
      "Epoch:4-64/231, Loss: 1.7818284034729004 \n",
      "Epoch:4-65/231, Loss: 1.682273030281067 \n",
      "Epoch:4-66/231, Loss: 1.692940354347229 \n",
      "Epoch:4-67/231, Loss: 1.5005104541778564 \n",
      "Epoch:4-68/231, Loss: 1.365848183631897 \n",
      "Epoch:4-69/231, Loss: 1.3277137279510498 \n",
      "Epoch:4-70/231, Loss: 1.3184384107589722 \n",
      "Epoch:4-71/231, Loss: 1.4063618183135986 \n",
      "Epoch:4-72/231, Loss: 1.3510971069335938 \n",
      "Epoch:4-73/231, Loss: 1.2392661571502686 \n",
      "Epoch:4-74/231, Loss: 1.1825587749481201 \n",
      "Epoch:4-75/231, Loss: 1.2002112865447998 \n",
      "Epoch:4-76/231, Loss: 1.0339879989624023 \n",
      "Epoch:4-77/231, Loss: 2.5212996006011963 \n",
      "Epoch:4-78/231, Loss: 2.462129592895508 \n",
      "Epoch:4-79/231, Loss: 2.3020718097686768 \n",
      "Epoch:4-80/231, Loss: 2.2332329750061035 \n",
      "Epoch:4-81/231, Loss: 1.9915192127227783 \n",
      "Epoch:4-82/231, Loss: 1.9842664003372192 \n",
      "Epoch:4-83/231, Loss: 1.7771575450897217 \n",
      "Epoch:4-84/231, Loss: 1.6328595876693726 \n",
      "Epoch:4-85/231, Loss: 1.662574052810669 \n",
      "Epoch:4-86/231, Loss: 1.343031406402588 \n",
      "Epoch:4-87/231, Loss: 1.449339747428894 \n",
      "Epoch:4-88/231, Loss: 1.2886937856674194 \n",
      "Epoch:4-89/231, Loss: 1.3841400146484375 \n",
      "Epoch:4-90/231, Loss: 1.2414969205856323 \n",
      "Epoch:4-91/231, Loss: 1.212613821029663 \n",
      "Epoch:4-92/231, Loss: 1.4862189292907715 \n",
      "Epoch:4-93/231, Loss: 1.4913538694381714 \n",
      "Epoch:4-94/231, Loss: 1.3064862489700317 \n",
      "Epoch:4-95/231, Loss: 1.2798690795898438 \n",
      "Epoch:4-96/231, Loss: 2.2533819675445557 \n",
      "Epoch:4-97/231, Loss: 2.249891996383667 \n",
      "Epoch:4-98/231, Loss: 2.0086257457733154 \n",
      "Epoch:4-99/231, Loss: 1.8730432987213135 \n",
      "Epoch:4-100/231, Loss: 2.1245651245117188 \n",
      "Epoch:4-101/231, Loss: 1.753900170326233 \n",
      "Epoch:4-102/231, Loss: 1.7423077821731567 \n",
      "Epoch:4-103/231, Loss: 1.5903457403182983 \n",
      "Epoch:4-104/231, Loss: 1.636917233467102 \n",
      "Epoch:4-105/231, Loss: 1.4592771530151367 \n",
      "Epoch:4-106/231, Loss: 1.262154221534729 \n",
      "Epoch:4-107/231, Loss: 1.2157732248306274 \n",
      "Epoch:4-108/231, Loss: 1.2742022275924683 \n",
      "Epoch:4-109/231, Loss: 1.2098926305770874 \n",
      "Epoch:4-110/231, Loss: 1.1408514976501465 \n",
      "Epoch:4-111/231, Loss: 1.2351906299591064 \n",
      "Epoch:4-112/231, Loss: 1.0159611701965332 \n",
      "Epoch:4-113/231, Loss: 0.9906142950057983 \n",
      "Epoch:4-114/231, Loss: 1.0647839307785034 \n",
      "Epoch:4-115/231, Loss: 1.8615353107452393 \n",
      "Epoch:4-116/231, Loss: 2.508528232574463 \n",
      "Epoch:4-117/231, Loss: 2.5441291332244873 \n",
      "Epoch:4-118/231, Loss: 2.587719440460205 \n",
      "Epoch:4-119/231, Loss: 2.3260951042175293 \n",
      "Epoch:4-120/231, Loss: 2.0547051429748535 \n",
      "Epoch:4-121/231, Loss: 1.894390344619751 \n",
      "Epoch:4-122/231, Loss: 1.7260987758636475 \n",
      "Epoch:4-123/231, Loss: 1.6553611755371094 \n",
      "Epoch:4-124/231, Loss: 1.7704133987426758 \n",
      "Epoch:4-125/231, Loss: 1.671080231666565 \n",
      "Epoch:4-126/231, Loss: 1.5538558959960938 \n",
      "Epoch:4-127/231, Loss: 1.6232049465179443 \n",
      "Epoch:4-128/231, Loss: 1.2704527378082275 \n",
      "Epoch:4-129/231, Loss: 1.1327356100082397 \n",
      "Epoch:4-130/231, Loss: 1.1652140617370605 \n",
      "Epoch:4-131/231, Loss: 1.1066325902938843 \n",
      "Epoch:4-132/231, Loss: 0.95130455493927 \n",
      "Epoch:4-133/231, Loss: 0.9401572346687317 \n",
      "Epoch:4-134/231, Loss: 1.2302296161651611 \n",
      "Epoch:4-135/231, Loss: 2.0948567390441895 \n",
      "Epoch:4-136/231, Loss: 1.9848374128341675 \n",
      "Epoch:4-137/231, Loss: 1.9848976135253906 \n",
      "Epoch:4-138/231, Loss: 1.9675586223602295 \n",
      "Epoch:4-139/231, Loss: 1.5541081428527832 \n",
      "Epoch:4-140/231, Loss: 1.4427661895751953 \n",
      "Epoch:4-141/231, Loss: 1.359535574913025 \n",
      "Epoch:4-142/231, Loss: 1.1630709171295166 \n",
      "Epoch:4-143/231, Loss: 1.0425894260406494 \n",
      "Epoch:4-144/231, Loss: 0.8093506097793579 \n",
      "Epoch:4-145/231, Loss: 0.8001956939697266 \n",
      "Epoch:4-146/231, Loss: 0.5064378380775452 \n",
      "Epoch:4-147/231, Loss: 0.4705907106399536 \n",
      "Epoch:4-148/231, Loss: 0.3700202703475952 \n",
      "Epoch:4-149/231, Loss: 0.34522518515586853 \n",
      "Epoch:4-150/231, Loss: 0.3171257972717285 \n",
      "Epoch:4-151/231, Loss: 0.26799094676971436 \n",
      "Epoch:4-152/231, Loss: 0.22625403106212616 \n",
      "Epoch:4-153/231, Loss: 0.8141440153121948 \n",
      "Epoch:4-154/231, Loss: 2.713454246520996 \n",
      "Epoch:4-155/231, Loss: 2.573056697845459 \n",
      "Epoch:4-156/231, Loss: 2.571709156036377 \n",
      "Epoch:4-157/231, Loss: 2.5150768756866455 \n",
      "Epoch:4-158/231, Loss: 2.2424049377441406 \n",
      "Epoch:4-159/231, Loss: 2.065735340118408 \n",
      "Epoch:4-160/231, Loss: 1.8693767786026 \n",
      "Epoch:4-161/231, Loss: 1.7585902214050293 \n",
      "Epoch:4-162/231, Loss: 1.5787891149520874 \n",
      "Epoch:4-163/231, Loss: 1.435577392578125 \n",
      "Epoch:4-164/231, Loss: 1.419548511505127 \n",
      "Epoch:4-165/231, Loss: 1.4212017059326172 \n",
      "Epoch:4-166/231, Loss: 1.3061413764953613 \n",
      "Epoch:4-167/231, Loss: 1.3081848621368408 \n",
      "Epoch:4-168/231, Loss: 1.5033001899719238 \n",
      "Epoch:4-169/231, Loss: 1.246671438217163 \n",
      "Epoch:4-170/231, Loss: 1.199122428894043 \n",
      "Epoch:4-171/231, Loss: 1.1014200448989868 \n",
      "Epoch:4-172/231, Loss: 1.1078770160675049 \n",
      "Epoch:4-173/231, Loss: 2.450530529022217 \n",
      "Epoch:4-174/231, Loss: 2.0779855251312256 \n",
      "Epoch:4-175/231, Loss: 2.044044256210327 \n",
      "Epoch:4-176/231, Loss: 2.1563880443573 \n",
      "Epoch:4-177/231, Loss: 2.1469779014587402 \n",
      "Epoch:4-178/231, Loss: 2.1301181316375732 \n",
      "Epoch:4-179/231, Loss: 1.8101727962493896 \n",
      "Epoch:4-180/231, Loss: 1.7701818943023682 \n",
      "Epoch:4-181/231, Loss: 1.6062592267990112 \n",
      "Epoch:4-182/231, Loss: 1.641662836074829 \n",
      "Epoch:4-183/231, Loss: 1.6426568031311035 \n",
      "Epoch:4-184/231, Loss: 1.5124582052230835 \n",
      "Epoch:4-185/231, Loss: 1.4600889682769775 \n",
      "Epoch:4-186/231, Loss: 1.2424815893173218 \n",
      "Epoch:4-187/231, Loss: 1.199030876159668 \n",
      "Epoch:4-188/231, Loss: 1.2333838939666748 \n",
      "Epoch:4-189/231, Loss: 1.2815790176391602 \n",
      "Epoch:4-190/231, Loss: 1.1409354209899902 \n",
      "Epoch:4-191/231, Loss: 1.0582633018493652 \n",
      "Epoch:4-192/231, Loss: 1.998915433883667 \n",
      "Epoch:4-193/231, Loss: 2.160107135772705 \n",
      "Epoch:4-194/231, Loss: 1.9240272045135498 \n",
      "Epoch:4-195/231, Loss: 1.7634273767471313 \n",
      "Epoch:4-196/231, Loss: 1.4249731302261353 \n",
      "Epoch:4-197/231, Loss: 1.3089637756347656 \n",
      "Epoch:4-198/231, Loss: 1.4467353820800781 \n",
      "Epoch:4-199/231, Loss: 1.2276638746261597 \n",
      "Epoch:4-200/231, Loss: 1.3345025777816772 \n",
      "Epoch:4-201/231, Loss: 1.2515857219696045 \n",
      "Epoch:4-202/231, Loss: 0.8510687351226807 \n",
      "Epoch:4-203/231, Loss: 0.7439770698547363 \n",
      "Epoch:4-204/231, Loss: 0.6855661869049072 \n",
      "Epoch:4-205/231, Loss: 0.5093780755996704 \n",
      "Epoch:4-206/231, Loss: 0.4300480782985687 \n",
      "Epoch:4-207/231, Loss: 0.5320549011230469 \n",
      "Epoch:4-208/231, Loss: 0.47182953357696533 \n",
      "Epoch:4-209/231, Loss: 0.36472630500793457 \n",
      "Epoch:4-210/231, Loss: 0.35259586572647095 \n",
      "Epoch:4-211/231, Loss: 0.3650484085083008 \n",
      "Epoch:4-212/231, Loss: 0.31739717721939087 \n",
      "Epoch:4-213/231, Loss: 2.643784761428833 \n",
      "Epoch:4-214/231, Loss: 3.493028402328491 \n",
      "Epoch:4-215/231, Loss: 3.500871181488037 \n",
      "Epoch:4-216/231, Loss: 2.8973097801208496 \n",
      "Epoch:4-217/231, Loss: 2.816638946533203 \n",
      "Epoch:4-218/231, Loss: 2.362610101699829 \n",
      "Epoch:4-219/231, Loss: 1.9828293323516846 \n",
      "Epoch:4-220/231, Loss: 1.8752858638763428 \n",
      "Epoch:4-221/231, Loss: 1.5093815326690674 \n",
      "Epoch:4-222/231, Loss: 1.598142385482788 \n",
      "Epoch:4-223/231, Loss: 1.4062412977218628 \n",
      "Epoch:4-224/231, Loss: 1.3614360094070435 \n",
      "Epoch:4-225/231, Loss: 1.231152892112732 \n",
      "Epoch:4-226/231, Loss: 1.1150099039077759 \n",
      "Epoch:4-227/231, Loss: 1.021220326423645 \n",
      "Epoch:4-228/231, Loss: 0.9185590147972107 \n",
      "Epoch:4-229/231, Loss: 0.8498823642730713 \n",
      "Epoch:4-230/231, Loss: 0.9229939579963684 \n",
      "--------------------------------------------------------------\n",
      "Epoch:4 completed, Total training's Loss: 362.499582991004, Spend: 1.567758611838023m\n",
      "Epoch:4, Acc:8.33 on Valid_set, Spend: 0.096 minutes for evaluation\n",
      "--------------------------------------------------------------\n",
      "Epoch:5-0/231, Loss: 2.2910966873168945 \n",
      "Epoch:5-1/231, Loss: 2.2619638442993164 \n",
      "Epoch:5-2/231, Loss: 2.0858311653137207 \n",
      "Epoch:5-3/231, Loss: 2.0812649726867676 \n",
      "Epoch:5-4/231, Loss: 2.0718958377838135 \n",
      "Epoch:5-5/231, Loss: 1.9257073402404785 \n",
      "Epoch:5-6/231, Loss: 1.659034013748169 \n",
      "Epoch:5-7/231, Loss: 1.7773772478103638 \n",
      "Epoch:5-8/231, Loss: 1.4863409996032715 \n",
      "Epoch:5-9/231, Loss: 1.5092055797576904 \n",
      "Epoch:5-10/231, Loss: 1.4401596784591675 \n",
      "Epoch:5-11/231, Loss: 1.3376872539520264 \n",
      "Epoch:5-12/231, Loss: 1.230372428894043 \n",
      "Epoch:5-13/231, Loss: 1.1169273853302002 \n",
      "Epoch:5-14/231, Loss: 1.1724495887756348 \n",
      "Epoch:5-15/231, Loss: 1.109094262123108 \n",
      "Epoch:5-16/231, Loss: 0.8790258765220642 \n",
      "Epoch:5-17/231, Loss: 0.7934934496879578 \n",
      "Epoch:5-18/231, Loss: 0.8978697061538696 \n",
      "Epoch:5-19/231, Loss: 1.9004590511322021 \n",
      "Epoch:5-20/231, Loss: 2.1581215858459473 \n",
      "Epoch:5-21/231, Loss: 2.085641384124756 \n",
      "Epoch:5-22/231, Loss: 2.1314587593078613 \n",
      "Epoch:5-23/231, Loss: 1.7251853942871094 \n",
      "Epoch:5-24/231, Loss: 1.8011195659637451 \n",
      "Epoch:5-25/231, Loss: 1.5039952993392944 \n",
      "Epoch:5-26/231, Loss: 1.4101738929748535 \n",
      "Epoch:5-27/231, Loss: 1.3124182224273682 \n",
      "Epoch:5-28/231, Loss: 1.3514649868011475 \n",
      "Epoch:5-29/231, Loss: 1.18107008934021 \n",
      "Epoch:5-30/231, Loss: 1.1826879978179932 \n",
      "Epoch:5-31/231, Loss: 1.2298071384429932 \n",
      "Epoch:5-32/231, Loss: 1.0360625982284546 \n",
      "Epoch:5-33/231, Loss: 1.0174646377563477 \n",
      "Epoch:5-34/231, Loss: 0.995047926902771 \n",
      "Epoch:5-35/231, Loss: 0.9765181541442871 \n",
      "Epoch:5-36/231, Loss: 0.7944312691688538 \n",
      "Epoch:5-37/231, Loss: 0.8094282746315002 \n",
      "Epoch:5-38/231, Loss: 1.9360398054122925 \n",
      "Epoch:5-39/231, Loss: 3.299859046936035 \n",
      "Epoch:5-40/231, Loss: 3.2735958099365234 \n",
      "Epoch:5-41/231, Loss: 3.1015336513519287 \n",
      "Epoch:5-42/231, Loss: 3.12203049659729 \n",
      "Epoch:5-43/231, Loss: 2.9202911853790283 \n",
      "Epoch:5-44/231, Loss: 2.5724453926086426 \n",
      "Epoch:5-45/231, Loss: 2.11040997505188 \n",
      "Epoch:5-46/231, Loss: 2.102762460708618 \n",
      "Epoch:5-47/231, Loss: 1.7598495483398438 \n",
      "Epoch:5-48/231, Loss: 1.7027159929275513 \n",
      "Epoch:5-49/231, Loss: 1.6901707649230957 \n",
      "Epoch:5-50/231, Loss: 1.6634725332260132 \n",
      "Epoch:5-51/231, Loss: 1.2893438339233398 \n",
      "Epoch:5-52/231, Loss: 1.3454742431640625 \n",
      "Epoch:5-53/231, Loss: 1.1640236377716064 \n",
      "Epoch:5-54/231, Loss: 1.1583527326583862 \n",
      "Epoch:5-55/231, Loss: 1.1288318634033203 \n",
      "Epoch:5-56/231, Loss: 1.0961952209472656 \n",
      "Epoch:5-57/231, Loss: 1.3003687858581543 \n",
      "Epoch:5-58/231, Loss: 2.145573139190674 \n",
      "Epoch:5-59/231, Loss: 2.230496883392334 \n",
      "Epoch:5-60/231, Loss: 2.2147583961486816 \n",
      "Epoch:5-61/231, Loss: 1.8576399087905884 \n",
      "Epoch:5-62/231, Loss: 1.7518348693847656 \n",
      "Epoch:5-63/231, Loss: 1.8744490146636963 \n",
      "Epoch:5-64/231, Loss: 1.6267280578613281 \n",
      "Epoch:5-65/231, Loss: 1.4280768632888794 \n",
      "Epoch:5-66/231, Loss: 1.2816381454467773 \n",
      "Epoch:5-67/231, Loss: 1.3328983783721924 \n",
      "Epoch:5-68/231, Loss: 1.2608380317687988 \n",
      "Epoch:5-69/231, Loss: 1.0949463844299316 \n",
      "Epoch:5-70/231, Loss: 1.0014678239822388 \n",
      "Epoch:5-71/231, Loss: 1.0376787185668945 \n",
      "Epoch:5-72/231, Loss: 0.9989303350448608 \n",
      "Epoch:5-73/231, Loss: 0.8925665616989136 \n",
      "Epoch:5-74/231, Loss: 0.8155086040496826 \n",
      "Epoch:5-75/231, Loss: 0.7690699100494385 \n",
      "Epoch:5-76/231, Loss: 0.8114131689071655 \n",
      "Epoch:5-77/231, Loss: 2.5262956619262695 \n",
      "Epoch:5-78/231, Loss: 2.4787580966949463 \n",
      "Epoch:5-79/231, Loss: 2.459031343460083 \n",
      "Epoch:5-80/231, Loss: 2.3468334674835205 \n",
      "Epoch:5-81/231, Loss: 2.2174534797668457 \n",
      "Epoch:5-82/231, Loss: 2.204862117767334 \n",
      "Epoch:5-83/231, Loss: 2.042400360107422 \n",
      "Epoch:5-84/231, Loss: 1.6662538051605225 \n",
      "Epoch:5-85/231, Loss: 1.5218604803085327 \n",
      "Epoch:5-86/231, Loss: 1.3004157543182373 \n",
      "Epoch:5-87/231, Loss: 1.347438097000122 \n",
      "Epoch:5-88/231, Loss: 1.3059078454971313 \n",
      "Epoch:5-89/231, Loss: 1.1439621448516846 \n",
      "Epoch:5-90/231, Loss: 1.2300429344177246 \n",
      "Epoch:5-91/231, Loss: 1.213832974433899 \n",
      "Epoch:5-92/231, Loss: 1.2546426057815552 \n",
      "Epoch:5-93/231, Loss: 1.3784997463226318 \n",
      "Epoch:5-94/231, Loss: 1.2161424160003662 \n",
      "Epoch:5-95/231, Loss: 1.240664005279541 \n",
      "Epoch:5-96/231, Loss: 2.1241884231567383 \n",
      "Epoch:5-97/231, Loss: 2.1003572940826416 \n",
      "Epoch:5-98/231, Loss: 1.9318138360977173 \n",
      "Epoch:5-99/231, Loss: 1.9747613668441772 \n",
      "Epoch:5-100/231, Loss: 1.7446649074554443 \n",
      "Epoch:5-101/231, Loss: 1.5610542297363281 \n",
      "Epoch:5-102/231, Loss: 1.517600178718567 \n",
      "Epoch:5-103/231, Loss: 1.5609607696533203 \n",
      "Epoch:5-104/231, Loss: 1.2771809101104736 \n",
      "Epoch:5-105/231, Loss: 1.3151538372039795 \n",
      "Epoch:5-106/231, Loss: 1.200642466545105 \n",
      "Epoch:5-107/231, Loss: 1.1327095031738281 \n",
      "Epoch:5-108/231, Loss: 1.0324608087539673 \n",
      "Epoch:5-109/231, Loss: 0.9326485395431519 \n",
      "Epoch:5-110/231, Loss: 0.8926803469657898 \n",
      "Epoch:5-111/231, Loss: 0.8885098695755005 \n",
      "Epoch:5-112/231, Loss: 0.9447731971740723 \n",
      "Epoch:5-113/231, Loss: 0.9755425453186035 \n",
      "Epoch:5-114/231, Loss: 0.9464696049690247 \n",
      "Epoch:5-115/231, Loss: 1.7504076957702637 \n",
      "Epoch:5-116/231, Loss: 2.4242331981658936 \n",
      "Epoch:5-117/231, Loss: 2.512683391571045 \n",
      "Epoch:5-118/231, Loss: 2.327181100845337 \n",
      "Epoch:5-119/231, Loss: 2.360745429992676 \n",
      "Epoch:5-120/231, Loss: 2.2810990810394287 \n",
      "Epoch:5-121/231, Loss: 2.188812732696533 \n",
      "Epoch:5-122/231, Loss: 1.9182920455932617 \n",
      "Epoch:5-123/231, Loss: 1.7053686380386353 \n",
      "Epoch:5-124/231, Loss: 1.5675996541976929 \n",
      "Epoch:5-125/231, Loss: 1.5486369132995605 \n",
      "Epoch:5-126/231, Loss: 1.5097123384475708 \n",
      "Epoch:5-127/231, Loss: 1.3140300512313843 \n",
      "Epoch:5-128/231, Loss: 1.530057430267334 \n",
      "Epoch:5-129/231, Loss: 1.27312171459198 \n",
      "Epoch:5-130/231, Loss: 1.2639601230621338 \n",
      "Epoch:5-131/231, Loss: 1.2471246719360352 \n",
      "Epoch:5-132/231, Loss: 1.1446524858474731 \n",
      "Epoch:5-133/231, Loss: 1.0189476013183594 \n",
      "Epoch:5-134/231, Loss: 1.4121952056884766 \n",
      "Epoch:5-135/231, Loss: 2.5428385734558105 \n",
      "Epoch:5-136/231, Loss: 2.5436477661132812 \n",
      "Epoch:5-137/231, Loss: 2.295325994491577 \n",
      "Epoch:5-138/231, Loss: 2.1261672973632812 \n",
      "Epoch:5-139/231, Loss: 2.2121520042419434 \n",
      "Epoch:5-140/231, Loss: 2.3371591567993164 \n",
      "Epoch:5-141/231, Loss: 2.24135160446167 \n",
      "Epoch:5-142/231, Loss: 2.0244412422180176 \n",
      "Epoch:5-143/231, Loss: 1.9039465188980103 \n",
      "Epoch:5-144/231, Loss: 1.852231740951538 \n",
      "Epoch:5-145/231, Loss: 1.6708688735961914 \n",
      "Epoch:5-146/231, Loss: 1.7284936904907227 \n",
      "Epoch:5-147/231, Loss: 1.4916588068008423 \n",
      "Epoch:5-148/231, Loss: 1.3041067123413086 \n",
      "Epoch:5-149/231, Loss: 1.299269676208496 \n",
      "Epoch:5-150/231, Loss: 1.3706339597702026 \n",
      "Epoch:5-151/231, Loss: 1.3604962825775146 \n",
      "Epoch:5-152/231, Loss: 1.2273027896881104 \n",
      "Epoch:5-153/231, Loss: 1.3683844804763794 \n",
      "Epoch:5-154/231, Loss: 2.104816198348999 \n",
      "Epoch:5-155/231, Loss: 2.0527443885803223 \n",
      "Epoch:5-156/231, Loss: 1.8813652992248535 \n",
      "Epoch:5-157/231, Loss: 1.9708237648010254 \n",
      "Epoch:5-158/231, Loss: 1.8197293281555176 \n",
      "Epoch:5-159/231, Loss: 1.7088799476623535 \n",
      "Epoch:5-160/231, Loss: 1.6371753215789795 \n",
      "Epoch:5-161/231, Loss: 1.686773657798767 \n",
      "Epoch:5-162/231, Loss: 1.4962949752807617 \n",
      "Epoch:5-163/231, Loss: 1.4175504446029663 \n",
      "Epoch:5-164/231, Loss: 1.2226080894470215 \n",
      "Epoch:5-165/231, Loss: 1.2951509952545166 \n",
      "Epoch:5-166/231, Loss: 1.1398817300796509 \n",
      "Epoch:5-167/231, Loss: 1.2017616033554077 \n",
      "Epoch:5-168/231, Loss: 1.0898422002792358 \n",
      "Epoch:5-169/231, Loss: 1.2096807956695557 \n",
      "Epoch:5-170/231, Loss: 1.0386989116668701 \n",
      "Epoch:5-171/231, Loss: 0.9653741717338562 \n",
      "Epoch:5-172/231, Loss: 0.9582929015159607 \n",
      "Epoch:5-173/231, Loss: 2.345320224761963 \n",
      "Epoch:5-174/231, Loss: 2.1756556034088135 \n",
      "Epoch:5-175/231, Loss: 2.1224920749664307 \n",
      "Epoch:5-176/231, Loss: 1.9333842992782593 \n",
      "Epoch:5-177/231, Loss: 1.726518154144287 \n",
      "Epoch:5-178/231, Loss: 1.343010663986206 \n",
      "Epoch:5-179/231, Loss: 1.3084121942520142 \n",
      "Epoch:5-180/231, Loss: 1.3054554462432861 \n",
      "Epoch:5-181/231, Loss: 1.3388320207595825 \n",
      "Epoch:5-182/231, Loss: 1.2610392570495605 \n",
      "Epoch:5-183/231, Loss: 1.263688087463379 \n",
      "Epoch:5-184/231, Loss: 1.1615798473358154 \n",
      "Epoch:5-185/231, Loss: 1.1927735805511475 \n",
      "Epoch:5-186/231, Loss: 0.9499590396881104 \n",
      "Epoch:5-187/231, Loss: 0.9453104138374329 \n",
      "Epoch:5-188/231, Loss: 0.8468663692474365 \n",
      "Epoch:5-189/231, Loss: 0.8845884799957275 \n",
      "Epoch:5-190/231, Loss: 0.8027452230453491 \n",
      "Epoch:5-191/231, Loss: 0.8461426496505737 \n",
      "Epoch:5-192/231, Loss: 2.2235779762268066 \n",
      "Epoch:5-193/231, Loss: 2.717376708984375 \n",
      "Epoch:5-194/231, Loss: 2.5559241771698 \n",
      "Epoch:5-195/231, Loss: 2.482879638671875 \n",
      "Epoch:5-196/231, Loss: 2.289792060852051 \n",
      "Epoch:5-197/231, Loss: 2.332000494003296 \n",
      "Epoch:5-198/231, Loss: 2.396963596343994 \n",
      "Epoch:5-199/231, Loss: 2.034036159515381 \n",
      "Epoch:5-200/231, Loss: 1.8278226852416992 \n",
      "Epoch:5-201/231, Loss: 1.860264539718628 \n",
      "Epoch:5-202/231, Loss: 1.6528663635253906 \n",
      "Epoch:5-203/231, Loss: 1.5767420530319214 \n",
      "Epoch:5-204/231, Loss: 1.6781032085418701 \n",
      "Epoch:5-205/231, Loss: 1.4196290969848633 \n",
      "Epoch:5-206/231, Loss: 1.4491969347000122 \n",
      "Epoch:5-207/231, Loss: 1.4441657066345215 \n",
      "Epoch:5-208/231, Loss: 1.365957260131836 \n",
      "Epoch:5-209/231, Loss: 1.3579350709915161 \n",
      "Epoch:5-210/231, Loss: 1.3817470073699951 \n",
      "Epoch:5-211/231, Loss: 1.180283784866333 \n",
      "Epoch:5-212/231, Loss: 1.0911750793457031 \n",
      "Epoch:5-213/231, Loss: 2.187382698059082 \n",
      "Epoch:5-214/231, Loss: 2.6079840660095215 \n",
      "Epoch:5-215/231, Loss: 2.4896533489227295 \n",
      "Epoch:5-216/231, Loss: 2.4403066635131836 \n",
      "Epoch:5-217/231, Loss: 2.1383228302001953 \n",
      "Epoch:5-218/231, Loss: 2.1273770332336426 \n",
      "Epoch:5-219/231, Loss: 2.1497268676757812 \n",
      "Epoch:5-220/231, Loss: 2.192768096923828 \n",
      "Epoch:5-221/231, Loss: 1.9387176036834717 \n",
      "Epoch:5-222/231, Loss: 1.693443775177002 \n",
      "Epoch:5-223/231, Loss: 1.6620032787322998 \n",
      "Epoch:5-224/231, Loss: 1.4297983646392822 \n",
      "Epoch:5-225/231, Loss: 1.3690606355667114 \n",
      "Epoch:5-226/231, Loss: 1.4012268781661987 \n",
      "Epoch:5-227/231, Loss: 1.3242650032043457 \n",
      "Epoch:5-228/231, Loss: 1.3226723670959473 \n",
      "Epoch:5-229/231, Loss: 1.4230835437774658 \n",
      "Epoch:5-230/231, Loss: 1.3052366971969604 \n",
      "--------------------------------------------------------------\n",
      "Epoch:5 completed, Total training's Loss: 374.4151300787926, Spend: 1.5669460018475851m\n",
      "Epoch:5, Acc:8.33 on Valid_set, Spend: 0.096 minutes for evaluation\n",
      "--------------------------------------------------------------\n",
      "Epoch:6-0/231, Loss: 1.9580202102661133 \n",
      "Epoch:6-1/231, Loss: 2.205023765563965 \n",
      "Epoch:6-2/231, Loss: 2.1675233840942383 \n",
      "Epoch:6-3/231, Loss: 1.9668611288070679 \n",
      "Epoch:6-4/231, Loss: 1.9725983142852783 \n",
      "Epoch:6-5/231, Loss: 1.7441051006317139 \n",
      "Epoch:6-6/231, Loss: 1.588024616241455 \n",
      "Epoch:6-7/231, Loss: 1.5453646183013916 \n",
      "Epoch:6-8/231, Loss: 1.3885908126831055 \n",
      "Epoch:6-9/231, Loss: 1.3074307441711426 \n",
      "Epoch:6-10/231, Loss: 1.2302494049072266 \n",
      "Epoch:6-11/231, Loss: 1.3353393077850342 \n",
      "Epoch:6-12/231, Loss: 1.2145745754241943 \n",
      "Epoch:6-13/231, Loss: 1.144585371017456 \n",
      "Epoch:6-14/231, Loss: 1.1760915517807007 \n",
      "Epoch:6-15/231, Loss: 1.1698288917541504 \n",
      "Epoch:6-16/231, Loss: 1.1670689582824707 \n",
      "Epoch:6-17/231, Loss: 1.1301894187927246 \n",
      "Epoch:6-18/231, Loss: 0.971468448638916 \n",
      "Epoch:6-19/231, Loss: 1.7348746061325073 \n",
      "Epoch:6-20/231, Loss: 2.134115695953369 \n",
      "Epoch:6-21/231, Loss: 1.9498094320297241 \n",
      "Epoch:6-22/231, Loss: 1.787075400352478 \n",
      "Epoch:6-23/231, Loss: 1.6954679489135742 \n",
      "Epoch:6-24/231, Loss: 1.4360274076461792 \n",
      "Epoch:6-25/231, Loss: 1.2562475204467773 \n",
      "Epoch:6-26/231, Loss: 1.3568943738937378 \n",
      "Epoch:6-27/231, Loss: 1.1766273975372314 \n",
      "Epoch:6-28/231, Loss: 1.124263048171997 \n",
      "Epoch:6-29/231, Loss: 0.9969558715820312 \n",
      "Epoch:6-30/231, Loss: 1.022812843322754 \n",
      "Epoch:6-31/231, Loss: 1.0704375505447388 \n",
      "Epoch:6-32/231, Loss: 0.9892672300338745 \n",
      "Epoch:6-33/231, Loss: 0.8767844438552856 \n",
      "Epoch:6-34/231, Loss: 0.7146415114402771 \n",
      "Epoch:6-35/231, Loss: 0.678747832775116 \n",
      "Epoch:6-36/231, Loss: 0.7488945722579956 \n",
      "Epoch:6-37/231, Loss: 0.644800066947937 \n",
      "Epoch:6-38/231, Loss: 1.4406328201293945 \n",
      "Epoch:6-39/231, Loss: 1.8772269487380981 \n",
      "Epoch:6-40/231, Loss: 1.6856393814086914 \n",
      "Epoch:6-41/231, Loss: 1.8164012432098389 \n",
      "Epoch:6-42/231, Loss: 1.8172237873077393 \n",
      "Epoch:6-43/231, Loss: 1.4687819480895996 \n",
      "Epoch:6-44/231, Loss: 1.2388875484466553 \n",
      "Epoch:6-45/231, Loss: 1.4261152744293213 \n",
      "Epoch:6-46/231, Loss: 1.0810892581939697 \n",
      "Epoch:6-47/231, Loss: 1.270228624343872 \n",
      "Epoch:6-48/231, Loss: 1.1807830333709717 \n",
      "Epoch:6-49/231, Loss: 1.0086899995803833 \n",
      "Epoch:6-50/231, Loss: 1.0059008598327637 \n",
      "Epoch:6-51/231, Loss: 1.0929818153381348 \n",
      "Epoch:6-52/231, Loss: 1.01668119430542 \n",
      "Epoch:6-53/231, Loss: 1.0017242431640625 \n",
      "Epoch:6-54/231, Loss: 0.8403094410896301 \n",
      "Epoch:6-55/231, Loss: 0.8590962886810303 \n",
      "Epoch:6-56/231, Loss: 0.8542430400848389 \n",
      "Epoch:6-57/231, Loss: 1.4448779821395874 \n",
      "Epoch:6-58/231, Loss: 2.4428892135620117 \n",
      "Epoch:6-59/231, Loss: 2.541661262512207 \n",
      "Epoch:6-60/231, Loss: 2.4370977878570557 \n",
      "Epoch:6-61/231, Loss: 2.317469835281372 \n",
      "Epoch:6-62/231, Loss: 2.2169270515441895 \n",
      "Epoch:6-63/231, Loss: 2.2033071517944336 \n",
      "Epoch:6-64/231, Loss: 1.8513388633728027 \n",
      "Epoch:6-65/231, Loss: 1.5098952054977417 \n",
      "Epoch:6-66/231, Loss: 1.6150472164154053 \n",
      "Epoch:6-67/231, Loss: 1.4708518981933594 \n",
      "Epoch:6-68/231, Loss: 1.3734698295593262 \n",
      "Epoch:6-69/231, Loss: 1.330993413925171 \n",
      "Epoch:6-70/231, Loss: 1.313352346420288 \n",
      "Epoch:6-71/231, Loss: 1.1550803184509277 \n",
      "Epoch:6-72/231, Loss: 1.2344039678573608 \n",
      "Epoch:6-73/231, Loss: 1.1570907831192017 \n",
      "Epoch:6-74/231, Loss: 0.9881922602653503 \n",
      "Epoch:6-75/231, Loss: 1.055509090423584 \n",
      "Epoch:6-76/231, Loss: 0.954429030418396 \n",
      "Epoch:6-77/231, Loss: 2.4131293296813965 \n",
      "Epoch:6-78/231, Loss: 2.480729103088379 \n",
      "Epoch:6-79/231, Loss: 2.2930450439453125 \n",
      "Epoch:6-80/231, Loss: 2.0562071800231934 \n",
      "Epoch:6-81/231, Loss: 2.0826587677001953 \n",
      "Epoch:6-82/231, Loss: 1.8598052263259888 \n",
      "Epoch:6-83/231, Loss: 1.4281847476959229 \n",
      "Epoch:6-84/231, Loss: 1.44636869430542 \n",
      "Epoch:6-85/231, Loss: 1.3911515474319458 \n",
      "Epoch:6-86/231, Loss: 1.3822753429412842 \n",
      "Epoch:6-87/231, Loss: 1.5049721002578735 \n",
      "Epoch:6-88/231, Loss: 1.243949055671692 \n",
      "Epoch:6-89/231, Loss: 1.3618175983428955 \n",
      "Epoch:6-90/231, Loss: 1.2653565406799316 \n",
      "Epoch:6-91/231, Loss: 1.071229100227356 \n",
      "Epoch:6-92/231, Loss: 1.2691608667373657 \n",
      "Epoch:6-93/231, Loss: 1.2549543380737305 \n",
      "Epoch:6-94/231, Loss: 1.0873348712921143 \n",
      "Epoch:6-95/231, Loss: 1.3151136636734009 \n",
      "Epoch:6-96/231, Loss: 2.1581711769104004 \n",
      "Epoch:6-97/231, Loss: 2.359467029571533 \n",
      "Epoch:6-98/231, Loss: 2.3677475452423096 \n",
      "Epoch:6-99/231, Loss: 2.0875017642974854 \n",
      "Epoch:6-100/231, Loss: 1.95583176612854 \n",
      "Epoch:6-101/231, Loss: 1.7972676753997803 \n",
      "Epoch:6-102/231, Loss: 1.769117832183838 \n",
      "Epoch:6-103/231, Loss: 1.4865387678146362 \n",
      "Epoch:6-104/231, Loss: 1.4411532878875732 \n",
      "Epoch:6-105/231, Loss: 1.2696809768676758 \n",
      "Epoch:6-106/231, Loss: 1.2830617427825928 \n",
      "Epoch:6-107/231, Loss: 1.1396682262420654 \n",
      "Epoch:6-108/231, Loss: 1.2318096160888672 \n",
      "Epoch:6-109/231, Loss: 1.123977541923523 \n",
      "Epoch:6-110/231, Loss: 0.995208203792572 \n",
      "Epoch:6-111/231, Loss: 1.0473688840866089 \n",
      "Epoch:6-112/231, Loss: 1.037070870399475 \n",
      "Epoch:6-113/231, Loss: 1.084397792816162 \n",
      "Epoch:6-114/231, Loss: 1.0632965564727783 \n",
      "Epoch:6-115/231, Loss: 1.5280990600585938 \n",
      "Epoch:6-116/231, Loss: 2.214745044708252 \n",
      "Epoch:6-117/231, Loss: 2.298739433288574 \n",
      "Epoch:6-118/231, Loss: 2.413609266281128 \n",
      "Epoch:6-119/231, Loss: 2.155459403991699 \n",
      "Epoch:6-120/231, Loss: 2.1038413047790527 \n",
      "Epoch:6-121/231, Loss: 1.9420546293258667 \n",
      "Epoch:6-122/231, Loss: 1.7123043537139893 \n",
      "Epoch:6-123/231, Loss: 1.7753431797027588 \n",
      "Epoch:6-124/231, Loss: 1.5204901695251465 \n",
      "Epoch:6-125/231, Loss: 1.6496614217758179 \n",
      "Epoch:6-126/231, Loss: 1.5320217609405518 \n",
      "Epoch:6-127/231, Loss: 1.5670042037963867 \n",
      "Epoch:6-128/231, Loss: 1.490083932876587 \n",
      "Epoch:6-129/231, Loss: 1.3937183618545532 \n",
      "Epoch:6-130/231, Loss: 1.267561912536621 \n",
      "Epoch:6-131/231, Loss: 1.2814743518829346 \n",
      "Epoch:6-132/231, Loss: 1.2051215171813965 \n",
      "Epoch:6-133/231, Loss: 1.1576087474822998 \n",
      "Epoch:6-134/231, Loss: 1.3274199962615967 \n",
      "Epoch:6-135/231, Loss: 1.95635986328125 \n",
      "Epoch:6-136/231, Loss: 2.1446871757507324 \n",
      "Epoch:6-137/231, Loss: 1.9131193161010742 \n",
      "Epoch:6-138/231, Loss: 1.8561081886291504 \n",
      "Epoch:6-139/231, Loss: 1.8328144550323486 \n",
      "Epoch:6-140/231, Loss: 1.7676243782043457 \n",
      "Epoch:6-141/231, Loss: 1.5489342212677002 \n",
      "Epoch:6-142/231, Loss: 1.6071176528930664 \n",
      "Epoch:6-143/231, Loss: 1.4805543422698975 \n",
      "Epoch:6-144/231, Loss: 1.2660466432571411 \n",
      "Epoch:6-145/231, Loss: 1.5538265705108643 \n",
      "Epoch:6-146/231, Loss: 1.297235369682312 \n",
      "Epoch:6-147/231, Loss: 1.2385227680206299 \n",
      "Epoch:6-148/231, Loss: 1.2229844331741333 \n",
      "Epoch:6-149/231, Loss: 1.1100924015045166 \n",
      "Epoch:6-150/231, Loss: 1.124037265777588 \n",
      "Epoch:6-151/231, Loss: 0.943718671798706 \n",
      "Epoch:6-152/231, Loss: 0.8935038447380066 \n",
      "Epoch:6-153/231, Loss: 1.215761661529541 \n",
      "Epoch:6-154/231, Loss: 1.990215539932251 \n",
      "Epoch:6-155/231, Loss: 2.176217555999756 \n",
      "Epoch:6-156/231, Loss: 2.004868507385254 \n",
      "Epoch:6-157/231, Loss: 1.8507641553878784 \n",
      "Epoch:6-158/231, Loss: 1.6941782236099243 \n",
      "Epoch:6-159/231, Loss: 1.591949462890625 \n",
      "Epoch:6-160/231, Loss: 1.4640721082687378 \n",
      "Epoch:6-161/231, Loss: 1.1911473274230957 \n",
      "Epoch:6-162/231, Loss: 1.2307825088500977 \n",
      "Epoch:6-163/231, Loss: 1.2014288902282715 \n",
      "Epoch:6-164/231, Loss: 1.1316611766815186 \n",
      "Epoch:6-165/231, Loss: 1.2885019779205322 \n",
      "Epoch:6-166/231, Loss: 1.1574068069458008 \n",
      "Epoch:6-167/231, Loss: 1.013279676437378 \n",
      "Epoch:6-168/231, Loss: 0.9918174743652344 \n",
      "Epoch:6-169/231, Loss: 0.8874212503433228 \n",
      "Epoch:6-170/231, Loss: 0.8080384731292725 \n",
      "Epoch:6-171/231, Loss: 0.8032180070877075 \n",
      "Epoch:6-172/231, Loss: 0.6368668079376221 \n",
      "Epoch:6-173/231, Loss: 2.6093716621398926 \n",
      "Epoch:6-174/231, Loss: 2.5051684379577637 \n",
      "Epoch:6-175/231, Loss: 2.5392723083496094 \n",
      "Epoch:6-176/231, Loss: 2.3936960697174072 \n",
      "Epoch:6-177/231, Loss: 2.33034086227417 \n",
      "Epoch:6-178/231, Loss: 1.9427103996276855 \n",
      "Epoch:6-179/231, Loss: 1.5229765176773071 \n",
      "Epoch:6-180/231, Loss: 1.6400461196899414 \n",
      "Epoch:6-181/231, Loss: 1.3830814361572266 \n",
      "Epoch:6-182/231, Loss: 1.348862648010254 \n",
      "Epoch:6-183/231, Loss: 1.2456129789352417 \n",
      "Epoch:6-184/231, Loss: 1.1497468948364258 \n",
      "Epoch:6-185/231, Loss: 0.9981486797332764 \n",
      "Epoch:6-186/231, Loss: 1.0530078411102295 \n",
      "Epoch:6-187/231, Loss: 0.8529029488563538 \n",
      "Epoch:6-188/231, Loss: 0.8429256677627563 \n",
      "Epoch:6-189/231, Loss: 0.7635502815246582 \n",
      "Epoch:6-190/231, Loss: 0.6886158585548401 \n",
      "Epoch:6-191/231, Loss: 0.6575490236282349 \n",
      "Epoch:6-192/231, Loss: 1.7024056911468506 \n",
      "Epoch:6-193/231, Loss: 1.907572627067566 \n",
      "Epoch:6-194/231, Loss: 1.8164548873901367 \n",
      "Epoch:6-195/231, Loss: 1.751056432723999 \n",
      "Epoch:6-196/231, Loss: 1.3944364786148071 \n",
      "Epoch:6-197/231, Loss: 1.355046272277832 \n",
      "Epoch:6-198/231, Loss: 1.5097638368606567 \n",
      "Epoch:6-199/231, Loss: 1.394027829170227 \n",
      "Epoch:6-200/231, Loss: 1.515411615371704 \n",
      "Epoch:6-201/231, Loss: 1.518768548965454 \n",
      "Epoch:6-202/231, Loss: 1.190964698791504 \n",
      "Epoch:6-203/231, Loss: 1.0996497869491577 \n",
      "Epoch:6-204/231, Loss: 1.1344881057739258 \n",
      "Epoch:6-205/231, Loss: 1.0116585493087769 \n",
      "Epoch:6-206/231, Loss: 1.0458728075027466 \n",
      "Epoch:6-207/231, Loss: 1.214530110359192 \n",
      "Epoch:6-208/231, Loss: 0.9504779577255249 \n",
      "Epoch:6-209/231, Loss: 1.0092830657958984 \n",
      "Epoch:6-210/231, Loss: 0.9691461324691772 \n",
      "Epoch:6-211/231, Loss: 0.9575388431549072 \n",
      "Epoch:6-212/231, Loss: 0.8959201574325562 \n",
      "Epoch:6-213/231, Loss: 2.2664403915405273 \n",
      "Epoch:6-214/231, Loss: 2.418076276779175 \n",
      "Epoch:6-215/231, Loss: 2.1938064098358154 \n",
      "Epoch:6-216/231, Loss: 1.9148608446121216 \n",
      "Epoch:6-217/231, Loss: 1.7890384197235107 \n",
      "Epoch:6-218/231, Loss: 1.543973445892334 \n",
      "Epoch:6-219/231, Loss: 1.7284525632858276 \n",
      "Epoch:6-220/231, Loss: 1.6734645366668701 \n",
      "Epoch:6-221/231, Loss: 1.6169028282165527 \n",
      "Epoch:6-222/231, Loss: 1.493342638015747 \n",
      "Epoch:6-223/231, Loss: 1.4519344568252563 \n",
      "Epoch:6-224/231, Loss: 1.463871717453003 \n",
      "Epoch:6-225/231, Loss: 1.3617420196533203 \n",
      "Epoch:6-226/231, Loss: 1.3844327926635742 \n",
      "Epoch:6-227/231, Loss: 1.2830400466918945 \n",
      "Epoch:6-228/231, Loss: 1.3233273029327393 \n",
      "Epoch:6-229/231, Loss: 1.0556988716125488 \n",
      "Epoch:6-230/231, Loss: 1.1937047243118286 \n",
      "--------------------------------------------------------------\n",
      "Epoch:6 completed, Total training's Loss: 338.87140357494354, Spend: 1.5654251376787822m\n",
      "Epoch:6, Acc:8.33 on Valid_set, Spend: 0.095 minutes for evaluation\n",
      "--------------------------------------------------------------\n",
      "Epoch:7-0/231, Loss: 2.0765607357025146 \n",
      "Epoch:7-1/231, Loss: 2.1594035625457764 \n",
      "Epoch:7-2/231, Loss: 2.1633729934692383 \n",
      "Epoch:7-3/231, Loss: 2.0808162689208984 \n",
      "Epoch:7-4/231, Loss: 2.0945394039154053 \n",
      "Epoch:7-5/231, Loss: 1.915993332862854 \n",
      "Epoch:7-6/231, Loss: 1.8998298645019531 \n",
      "Epoch:7-7/231, Loss: 1.8333669900894165 \n",
      "Epoch:7-8/231, Loss: 1.5619373321533203 \n",
      "Epoch:7-9/231, Loss: 1.439222812652588 \n",
      "Epoch:7-10/231, Loss: 1.4447333812713623 \n",
      "Epoch:7-11/231, Loss: 1.1983873844146729 \n",
      "Epoch:7-12/231, Loss: 1.0774413347244263 \n",
      "Epoch:7-13/231, Loss: 1.0978338718414307 \n",
      "Epoch:7-14/231, Loss: 1.0053222179412842 \n",
      "Epoch:7-15/231, Loss: 1.1015429496765137 \n",
      "Epoch:7-16/231, Loss: 1.0746220350265503 \n",
      "Epoch:7-17/231, Loss: 0.8479299545288086 \n",
      "Epoch:7-18/231, Loss: 0.9344867467880249 \n",
      "Epoch:7-19/231, Loss: 1.734526515007019 \n",
      "Epoch:7-20/231, Loss: 2.0968174934387207 \n",
      "Epoch:7-21/231, Loss: 1.5454368591308594 \n",
      "Epoch:7-22/231, Loss: 1.5417908430099487 \n",
      "Epoch:7-23/231, Loss: 1.3055286407470703 \n",
      "Epoch:7-24/231, Loss: 0.9962738156318665 \n",
      "Epoch:7-25/231, Loss: 0.8612923622131348 \n",
      "Epoch:7-26/231, Loss: 0.7458550930023193 \n",
      "Epoch:7-27/231, Loss: 0.774763286113739 \n",
      "Epoch:7-28/231, Loss: 0.6598836779594421 \n",
      "Epoch:7-29/231, Loss: 0.6782253980636597 \n",
      "Epoch:7-30/231, Loss: 0.708777666091919 \n",
      "Epoch:7-31/231, Loss: 0.6256570816040039 \n",
      "Epoch:7-32/231, Loss: 0.5074379444122314 \n",
      "Epoch:7-33/231, Loss: 0.5635888576507568 \n",
      "Epoch:7-34/231, Loss: 0.4869396388530731 \n",
      "Epoch:7-35/231, Loss: 0.43762949109077454 \n",
      "Epoch:7-36/231, Loss: 0.4420335292816162 \n",
      "Epoch:7-37/231, Loss: 0.3311205506324768 \n",
      "Epoch:7-38/231, Loss: 0.984101414680481 \n",
      "Epoch:7-39/231, Loss: 2.0230369567871094 \n",
      "Epoch:7-40/231, Loss: 2.0451600551605225 \n",
      "Epoch:7-41/231, Loss: 1.9132763147354126 \n",
      "Epoch:7-42/231, Loss: 2.068187713623047 \n",
      "Epoch:7-43/231, Loss: 1.7598485946655273 \n",
      "Epoch:7-44/231, Loss: 1.4290595054626465 \n",
      "Epoch:7-45/231, Loss: 1.4136550426483154 \n",
      "Epoch:7-46/231, Loss: 1.2011388540267944 \n",
      "Epoch:7-47/231, Loss: 1.2222732305526733 \n",
      "Epoch:7-48/231, Loss: 1.1565039157867432 \n",
      "Epoch:7-49/231, Loss: 1.236473560333252 \n",
      "Epoch:7-50/231, Loss: 0.9007307291030884 \n",
      "Epoch:7-51/231, Loss: 0.9942027926445007 \n",
      "Epoch:7-52/231, Loss: 0.9803998470306396 \n",
      "Epoch:7-53/231, Loss: 0.9398385286331177 \n",
      "Epoch:7-54/231, Loss: 0.8570573329925537 \n",
      "Epoch:7-55/231, Loss: 0.8765591382980347 \n",
      "Epoch:7-56/231, Loss: 0.8721010088920593 \n",
      "Epoch:7-57/231, Loss: 1.5353350639343262 \n",
      "Epoch:7-58/231, Loss: 1.714738130569458 \n",
      "Epoch:7-59/231, Loss: 1.9949175119400024 \n",
      "Epoch:7-60/231, Loss: 2.01420259475708 \n",
      "Epoch:7-61/231, Loss: 1.7281901836395264 \n",
      "Epoch:7-62/231, Loss: 1.7476557493209839 \n",
      "Epoch:7-63/231, Loss: 1.63363516330719 \n",
      "Epoch:7-64/231, Loss: 1.3906588554382324 \n",
      "Epoch:7-65/231, Loss: 1.366438865661621 \n",
      "Epoch:7-66/231, Loss: 1.2544149160385132 \n",
      "Epoch:7-67/231, Loss: 1.250256061553955 \n",
      "Epoch:7-68/231, Loss: 1.1943438053131104 \n",
      "Epoch:7-69/231, Loss: 1.2824598550796509 \n",
      "Epoch:7-70/231, Loss: 1.1220622062683105 \n",
      "Epoch:7-71/231, Loss: 1.126441240310669 \n",
      "Epoch:7-72/231, Loss: 1.0404361486434937 \n",
      "Epoch:7-73/231, Loss: 1.0548275709152222 \n",
      "Epoch:7-74/231, Loss: 1.0669962167739868 \n",
      "Epoch:7-75/231, Loss: 0.8766061067581177 \n",
      "Epoch:7-76/231, Loss: 0.8475335836410522 \n",
      "Epoch:7-77/231, Loss: 2.3186793327331543 \n",
      "Epoch:7-78/231, Loss: 2.358487129211426 \n",
      "Epoch:7-79/231, Loss: 2.0903573036193848 \n",
      "Epoch:7-80/231, Loss: 2.2862014770507812 \n",
      "Epoch:7-81/231, Loss: 1.7990926504135132 \n",
      "Epoch:7-82/231, Loss: 1.6042792797088623 \n",
      "Epoch:7-83/231, Loss: 1.508824348449707 \n",
      "Epoch:7-84/231, Loss: 1.4066545963287354 \n",
      "Epoch:7-85/231, Loss: 1.1890395879745483 \n",
      "Epoch:7-86/231, Loss: 1.2164465188980103 \n",
      "Epoch:7-87/231, Loss: 1.2639044523239136 \n",
      "Epoch:7-88/231, Loss: 1.1345062255859375 \n",
      "Epoch:7-89/231, Loss: 1.1933190822601318 \n",
      "Epoch:7-90/231, Loss: 1.2046432495117188 \n",
      "Epoch:7-91/231, Loss: 1.0769538879394531 \n",
      "Epoch:7-92/231, Loss: 1.2895383834838867 \n",
      "Epoch:7-93/231, Loss: 1.198235273361206 \n",
      "Epoch:7-94/231, Loss: 1.0365591049194336 \n",
      "Epoch:7-95/231, Loss: 1.3061593770980835 \n",
      "Epoch:7-96/231, Loss: 2.166851758956909 \n",
      "Epoch:7-97/231, Loss: 2.354823350906372 \n",
      "Epoch:7-98/231, Loss: 2.2013754844665527 \n",
      "Epoch:7-99/231, Loss: 2.0801610946655273 \n",
      "Epoch:7-100/231, Loss: 2.0950350761413574 \n",
      "Epoch:7-101/231, Loss: 1.8023402690887451 \n",
      "Epoch:7-102/231, Loss: 1.619610071182251 \n",
      "Epoch:7-103/231, Loss: 1.5372138023376465 \n",
      "Epoch:7-104/231, Loss: 1.4608122110366821 \n",
      "Epoch:7-105/231, Loss: 1.254582166671753 \n",
      "Epoch:7-106/231, Loss: 1.2081223726272583 \n",
      "Epoch:7-107/231, Loss: 0.9713993072509766 \n",
      "Epoch:7-108/231, Loss: 0.9091600179672241 \n",
      "Epoch:7-109/231, Loss: 1.1451945304870605 \n",
      "Epoch:7-110/231, Loss: 1.0986874103546143 \n",
      "Epoch:7-111/231, Loss: 1.0136821269989014 \n",
      "Epoch:7-112/231, Loss: 0.9373647570610046 \n",
      "Epoch:7-113/231, Loss: 1.058781623840332 \n",
      "Epoch:7-114/231, Loss: 1.2565635442733765 \n",
      "Epoch:7-115/231, Loss: 1.3985192775726318 \n",
      "Epoch:7-116/231, Loss: 1.861851692199707 \n",
      "Epoch:7-117/231, Loss: 2.003174066543579 \n",
      "Epoch:7-118/231, Loss: 1.8671410083770752 \n",
      "Epoch:7-119/231, Loss: 1.7777230739593506 \n",
      "Epoch:7-120/231, Loss: 1.7717680931091309 \n",
      "Epoch:7-121/231, Loss: 1.7674485445022583 \n",
      "Epoch:7-122/231, Loss: 1.3788946866989136 \n",
      "Epoch:7-123/231, Loss: 1.5171098709106445 \n",
      "Epoch:7-124/231, Loss: 1.283822774887085 \n",
      "Epoch:7-125/231, Loss: 1.3622407913208008 \n",
      "Epoch:7-126/231, Loss: 1.414409875869751 \n",
      "Epoch:7-127/231, Loss: 1.4959522485733032 \n",
      "Epoch:7-128/231, Loss: 1.328239917755127 \n",
      "Epoch:7-129/231, Loss: 1.1425402164459229 \n",
      "Epoch:7-130/231, Loss: 1.0499486923217773 \n",
      "Epoch:7-131/231, Loss: 1.098144769668579 \n",
      "Epoch:7-132/231, Loss: 1.1059343814849854 \n",
      "Epoch:7-133/231, Loss: 1.1526859998703003 \n",
      "Epoch:7-134/231, Loss: 1.2356563806533813 \n",
      "Epoch:7-135/231, Loss: 1.9782644510269165 \n",
      "Epoch:7-136/231, Loss: 2.0918967723846436 \n",
      "Epoch:7-137/231, Loss: 1.9247667789459229 \n",
      "Epoch:7-138/231, Loss: 1.8244861364364624 \n",
      "Epoch:7-139/231, Loss: 1.9249982833862305 \n",
      "Epoch:7-140/231, Loss: 1.8282155990600586 \n",
      "Epoch:7-141/231, Loss: 1.6784471273422241 \n",
      "Epoch:7-142/231, Loss: 1.5245003700256348 \n",
      "Epoch:7-143/231, Loss: 1.3327444791793823 \n",
      "Epoch:7-144/231, Loss: 1.2506537437438965 \n",
      "Epoch:7-145/231, Loss: 1.2880827188491821 \n",
      "Epoch:7-146/231, Loss: 1.1115840673446655 \n",
      "Epoch:7-147/231, Loss: 1.0722366571426392 \n",
      "Epoch:7-148/231, Loss: 1.001281976699829 \n",
      "Epoch:7-149/231, Loss: 1.1242961883544922 \n",
      "Epoch:7-150/231, Loss: 0.990193784236908 \n",
      "Epoch:7-151/231, Loss: 0.9781671762466431 \n",
      "Epoch:7-152/231, Loss: 0.9774037599563599 \n",
      "Epoch:7-153/231, Loss: 1.2357302904129028 \n",
      "Epoch:7-154/231, Loss: 1.9461957216262817 \n",
      "Epoch:7-155/231, Loss: 2.0985608100891113 \n",
      "Epoch:7-156/231, Loss: 1.8515591621398926 \n",
      "Epoch:7-157/231, Loss: 1.7289924621582031 \n",
      "Epoch:7-158/231, Loss: 1.5593324899673462 \n",
      "Epoch:7-159/231, Loss: 1.3402122259140015 \n",
      "Epoch:7-160/231, Loss: 1.4841734170913696 \n",
      "Epoch:7-161/231, Loss: 1.363391399383545 \n",
      "Epoch:7-162/231, Loss: 1.20641028881073 \n",
      "Epoch:7-163/231, Loss: 1.2760943174362183 \n",
      "Epoch:7-164/231, Loss: 1.1981451511383057 \n",
      "Epoch:7-165/231, Loss: 1.1423777341842651 \n",
      "Epoch:7-166/231, Loss: 0.9995620846748352 \n",
      "Epoch:7-167/231, Loss: 1.1449127197265625 \n",
      "Epoch:7-168/231, Loss: 1.0397279262542725 \n",
      "Epoch:7-169/231, Loss: 0.9571864604949951 \n",
      "Epoch:7-170/231, Loss: 0.932323157787323 \n",
      "Epoch:7-171/231, Loss: 0.980993390083313 \n",
      "Epoch:7-172/231, Loss: 0.8756030201911926 \n",
      "Epoch:7-173/231, Loss: 2.5465598106384277 \n",
      "Epoch:7-174/231, Loss: 2.113485097885132 \n",
      "Epoch:7-175/231, Loss: 2.3183817863464355 \n",
      "Epoch:7-176/231, Loss: 2.0025253295898438 \n",
      "Epoch:7-177/231, Loss: 1.790856122970581 \n",
      "Epoch:7-178/231, Loss: 1.4585591554641724 \n",
      "Epoch:7-179/231, Loss: 1.2829896211624146 \n",
      "Epoch:7-180/231, Loss: 1.0602850914001465 \n",
      "Epoch:7-181/231, Loss: 0.8720662593841553 \n",
      "Epoch:7-182/231, Loss: 0.6950082778930664 \n",
      "Epoch:7-183/231, Loss: 0.6732258200645447 \n",
      "Epoch:7-184/231, Loss: 0.6036721467971802 \n",
      "Epoch:7-185/231, Loss: 0.5956082344055176 \n",
      "Epoch:7-186/231, Loss: 0.5744550824165344 \n",
      "Epoch:7-187/231, Loss: 0.5974540114402771 \n",
      "Epoch:7-188/231, Loss: 0.4954426884651184 \n",
      "Epoch:7-189/231, Loss: 0.4784468114376068 \n",
      "Epoch:7-190/231, Loss: 0.47006750106811523 \n",
      "Epoch:7-191/231, Loss: 0.3168237805366516 \n",
      "Epoch:7-192/231, Loss: 1.4906278848648071 \n",
      "Epoch:7-193/231, Loss: 1.8135871887207031 \n",
      "Epoch:7-194/231, Loss: 1.8336601257324219 \n",
      "Epoch:7-195/231, Loss: 1.86061429977417 \n",
      "Epoch:7-196/231, Loss: 1.358056664466858 \n",
      "Epoch:7-197/231, Loss: 1.2557941675186157 \n",
      "Epoch:7-198/231, Loss: 1.2480684518814087 \n",
      "Epoch:7-199/231, Loss: 1.3484580516815186 \n",
      "Epoch:7-200/231, Loss: 1.9500473737716675 \n",
      "Epoch:7-201/231, Loss: 1.46397066116333 \n",
      "Epoch:7-202/231, Loss: 0.9689473509788513 \n",
      "Epoch:7-203/231, Loss: 1.037658929824829 \n",
      "Epoch:7-204/231, Loss: 0.9622204303741455 \n",
      "Epoch:7-205/231, Loss: 0.8685641288757324 \n",
      "Epoch:7-206/231, Loss: 0.9755352735519409 \n",
      "Epoch:7-207/231, Loss: 0.9464905261993408 \n",
      "Epoch:7-208/231, Loss: 0.8657784461975098 \n",
      "Epoch:7-209/231, Loss: 0.8899260759353638 \n",
      "Epoch:7-210/231, Loss: 0.7283142805099487 \n",
      "Epoch:7-211/231, Loss: 0.7429598569869995 \n",
      "Epoch:7-212/231, Loss: 0.6704360246658325 \n",
      "Epoch:7-213/231, Loss: 2.1426756381988525 \n",
      "Epoch:7-214/231, Loss: 2.4808950424194336 \n",
      "Epoch:7-215/231, Loss: 2.4169821739196777 \n",
      "Epoch:7-216/231, Loss: 2.185330390930176 \n",
      "Epoch:7-217/231, Loss: 1.8082129955291748 \n",
      "Epoch:7-218/231, Loss: 1.8085486888885498 \n",
      "Epoch:7-219/231, Loss: 1.5771619081497192 \n",
      "Epoch:7-220/231, Loss: 1.516890048980713 \n",
      "Epoch:7-221/231, Loss: 1.4344274997711182 \n",
      "Epoch:7-222/231, Loss: 1.6221411228179932 \n",
      "Epoch:7-223/231, Loss: 1.4536240100860596 \n",
      "Epoch:7-224/231, Loss: 1.507231593132019 \n",
      "Epoch:7-225/231, Loss: 1.3885066509246826 \n",
      "Epoch:7-226/231, Loss: 1.212188720703125 \n",
      "Epoch:7-227/231, Loss: 1.1230623722076416 \n",
      "Epoch:7-228/231, Loss: 1.157529592514038 \n",
      "Epoch:7-229/231, Loss: 1.072924017906189 \n",
      "Epoch:7-230/231, Loss: 1.1120991706848145 \n",
      "--------------------------------------------------------------\n",
      "Epoch:7 completed, Total training's Loss: 311.3432896435261, Spend: 1.5672012170155842m\n",
      "Epoch:7, Acc:8.89 on Valid_set, Spend: 0.095 minutes for evaluation\n",
      "--------------------------------------------------------------\n",
      "Epoch:8-0/231, Loss: 2.012690544128418 \n",
      "Epoch:8-1/231, Loss: 2.00197172164917 \n",
      "Epoch:8-2/231, Loss: 1.888089656829834 \n",
      "Epoch:8-3/231, Loss: 1.8823386430740356 \n",
      "Epoch:8-4/231, Loss: 1.8106412887573242 \n",
      "Epoch:8-5/231, Loss: 1.7569636106491089 \n",
      "Epoch:8-6/231, Loss: 1.6472371816635132 \n",
      "Epoch:8-7/231, Loss: 1.5325119495391846 \n",
      "Epoch:8-8/231, Loss: 1.4241138696670532 \n",
      "Epoch:8-9/231, Loss: 1.2144955396652222 \n",
      "Epoch:8-10/231, Loss: 1.1521058082580566 \n",
      "Epoch:8-11/231, Loss: 1.0092687606811523 \n",
      "Epoch:8-12/231, Loss: 1.094974398612976 \n",
      "Epoch:8-13/231, Loss: 0.9243788719177246 \n",
      "Epoch:8-14/231, Loss: 1.035821795463562 \n",
      "Epoch:8-15/231, Loss: 0.924115777015686 \n",
      "Epoch:8-16/231, Loss: 0.8727344274520874 \n",
      "Epoch:8-17/231, Loss: 0.8044244050979614 \n",
      "Epoch:8-18/231, Loss: 0.8665884137153625 \n",
      "Epoch:8-19/231, Loss: 1.9449639320373535 \n",
      "Epoch:8-20/231, Loss: 2.269949436187744 \n",
      "Epoch:8-21/231, Loss: 1.2928673028945923 \n",
      "Epoch:8-22/231, Loss: 0.9746689796447754 \n",
      "Epoch:8-23/231, Loss: 0.6214696168899536 \n",
      "Epoch:8-24/231, Loss: 0.4953197240829468 \n",
      "Epoch:8-25/231, Loss: 0.3711092174053192 \n",
      "Epoch:8-26/231, Loss: 0.3345129191875458 \n",
      "Epoch:8-27/231, Loss: 0.33760809898376465 \n",
      "Epoch:8-28/231, Loss: 0.3788550794124603 \n",
      "Epoch:8-29/231, Loss: 0.31544971466064453 \n",
      "Epoch:8-30/231, Loss: 0.27673783898353577 \n",
      "Epoch:8-31/231, Loss: 0.26992928981781006 \n",
      "Epoch:8-32/231, Loss: 0.2160228192806244 \n",
      "Epoch:8-33/231, Loss: 0.2045843005180359 \n",
      "Epoch:8-34/231, Loss: 0.19122880697250366 \n",
      "Epoch:8-35/231, Loss: 0.17447713017463684 \n",
      "Epoch:8-36/231, Loss: 0.19349078834056854 \n",
      "Epoch:8-37/231, Loss: 0.18683196604251862 \n",
      "Epoch:8-38/231, Loss: 0.9869391918182373 \n",
      "Epoch:8-39/231, Loss: 1.675210952758789 \n",
      "Epoch:8-40/231, Loss: 1.5707544088363647 \n",
      "Epoch:8-41/231, Loss: 1.2644693851470947 \n",
      "Epoch:8-42/231, Loss: 1.7067458629608154 \n",
      "Epoch:8-43/231, Loss: 1.0778203010559082 \n",
      "Epoch:8-44/231, Loss: 1.0543814897537231 \n",
      "Epoch:8-45/231, Loss: 1.023140549659729 \n",
      "Epoch:8-46/231, Loss: 0.9114676713943481 \n",
      "Epoch:8-47/231, Loss: 0.9373646974563599 \n",
      "Epoch:8-48/231, Loss: 0.8784013986587524 \n",
      "Epoch:8-49/231, Loss: 0.8962308764457703 \n",
      "Epoch:8-50/231, Loss: 0.8488620519638062 \n",
      "Epoch:8-51/231, Loss: 0.9495984315872192 \n",
      "Epoch:8-52/231, Loss: 0.8162837028503418 \n",
      "Epoch:8-53/231, Loss: 0.7069486379623413 \n",
      "Epoch:8-54/231, Loss: 0.7436513900756836 \n",
      "Epoch:8-55/231, Loss: 0.763059675693512 \n",
      "Epoch:8-56/231, Loss: 0.7142032384872437 \n",
      "Epoch:8-57/231, Loss: 1.894019365310669 \n",
      "Epoch:8-58/231, Loss: 1.6591715812683105 \n",
      "Epoch:8-59/231, Loss: 2.330883502960205 \n",
      "Epoch:8-60/231, Loss: 2.161689519882202 \n",
      "Epoch:8-61/231, Loss: 1.7908802032470703 \n",
      "Epoch:8-62/231, Loss: 1.8221426010131836 \n",
      "Epoch:8-63/231, Loss: 1.8965535163879395 \n",
      "Epoch:8-64/231, Loss: 1.460575819015503 \n",
      "Epoch:8-65/231, Loss: 1.3040220737457275 \n",
      "Epoch:8-66/231, Loss: 1.28535795211792 \n",
      "Epoch:8-67/231, Loss: 1.187852382659912 \n",
      "Epoch:8-68/231, Loss: 1.0051523447036743 \n",
      "Epoch:8-69/231, Loss: 0.994088351726532 \n",
      "Epoch:8-70/231, Loss: 0.9358512759208679 \n",
      "Epoch:8-71/231, Loss: 0.8937009572982788 \n",
      "Epoch:8-72/231, Loss: 0.8882611989974976 \n",
      "Epoch:8-73/231, Loss: 0.761186957359314 \n",
      "Epoch:8-74/231, Loss: 0.7089803814888 \n",
      "Epoch:8-75/231, Loss: 0.8307636380195618 \n",
      "Epoch:8-76/231, Loss: 0.6317644119262695 \n",
      "Epoch:8-77/231, Loss: 2.343809127807617 \n",
      "Epoch:8-78/231, Loss: 2.0118391513824463 \n",
      "Epoch:8-79/231, Loss: 1.881974220275879 \n",
      "Epoch:8-80/231, Loss: 1.6158435344696045 \n",
      "Epoch:8-81/231, Loss: 1.405137300491333 \n",
      "Epoch:8-82/231, Loss: 1.3033757209777832 \n",
      "Epoch:8-83/231, Loss: 1.1737442016601562 \n",
      "Epoch:8-84/231, Loss: 1.1903324127197266 \n",
      "Epoch:8-85/231, Loss: 1.0949227809906006 \n",
      "Epoch:8-86/231, Loss: 0.917826771736145 \n",
      "Epoch:8-87/231, Loss: 0.8731042146682739 \n",
      "Epoch:8-88/231, Loss: 0.7744752764701843 \n",
      "Epoch:8-89/231, Loss: 0.7235747575759888 \n",
      "Epoch:8-90/231, Loss: 0.8372225761413574 \n",
      "Epoch:8-91/231, Loss: 0.6665079593658447 \n",
      "Epoch:8-92/231, Loss: 0.9895443916320801 \n",
      "Epoch:8-93/231, Loss: 0.9910398721694946 \n",
      "Epoch:8-94/231, Loss: 0.8249754309654236 \n",
      "Epoch:8-95/231, Loss: 0.8653743863105774 \n",
      "Epoch:8-96/231, Loss: 2.467759132385254 \n",
      "Epoch:8-97/231, Loss: 2.664646625518799 \n",
      "Epoch:8-98/231, Loss: 2.3311002254486084 \n",
      "Epoch:8-99/231, Loss: 2.2909035682678223 \n",
      "Epoch:8-100/231, Loss: 1.927462100982666 \n",
      "Epoch:8-101/231, Loss: 1.834446668624878 \n",
      "Epoch:8-102/231, Loss: 1.6448800563812256 \n",
      "Epoch:8-103/231, Loss: 1.5533705949783325 \n",
      "Epoch:8-104/231, Loss: 1.238830327987671 \n",
      "Epoch:8-105/231, Loss: 1.2623902559280396 \n",
      "Epoch:8-106/231, Loss: 1.0500247478485107 \n",
      "Epoch:8-107/231, Loss: 0.9562655091285706 \n",
      "Epoch:8-108/231, Loss: 0.9461526870727539 \n",
      "Epoch:8-109/231, Loss: 1.0099422931671143 \n",
      "Epoch:8-110/231, Loss: 0.8441286087036133 \n",
      "Epoch:8-111/231, Loss: 0.9039446115493774 \n",
      "Epoch:8-112/231, Loss: 0.7281007766723633 \n",
      "Epoch:8-113/231, Loss: 0.962006688117981 \n",
      "Epoch:8-114/231, Loss: 1.155529260635376 \n",
      "Epoch:8-115/231, Loss: 1.4385628700256348 \n",
      "Epoch:8-116/231, Loss: 2.258356809616089 \n",
      "Epoch:8-117/231, Loss: 2.250483274459839 \n",
      "Epoch:8-118/231, Loss: 2.167067050933838 \n",
      "Epoch:8-119/231, Loss: 1.991923213005066 \n",
      "Epoch:8-120/231, Loss: 2.0397095680236816 \n",
      "Epoch:8-121/231, Loss: 1.7524688243865967 \n",
      "Epoch:8-122/231, Loss: 1.3840858936309814 \n",
      "Epoch:8-123/231, Loss: 1.6479579210281372 \n",
      "Epoch:8-124/231, Loss: 1.3720145225524902 \n",
      "Epoch:8-125/231, Loss: 1.2099601030349731 \n",
      "Epoch:8-126/231, Loss: 1.3712483644485474 \n",
      "Epoch:8-127/231, Loss: 1.1907052993774414 \n",
      "Epoch:8-128/231, Loss: 1.1476783752441406 \n",
      "Epoch:8-129/231, Loss: 1.1507326364517212 \n",
      "Epoch:8-130/231, Loss: 1.1461442708969116 \n",
      "Epoch:8-131/231, Loss: 1.1532044410705566 \n",
      "Epoch:8-132/231, Loss: 0.9725322127342224 \n",
      "Epoch:8-133/231, Loss: 1.1398029327392578 \n",
      "Epoch:8-134/231, Loss: 1.1391632556915283 \n",
      "Epoch:8-135/231, Loss: 1.808456301689148 \n",
      "Epoch:8-136/231, Loss: 1.8311982154846191 \n",
      "Epoch:8-137/231, Loss: 1.5864460468292236 \n",
      "Epoch:8-138/231, Loss: 1.6544129848480225 \n",
      "Epoch:8-139/231, Loss: 1.664161205291748 \n",
      "Epoch:8-140/231, Loss: 1.5560485124588013 \n",
      "Epoch:8-141/231, Loss: 1.4897162914276123 \n",
      "Epoch:8-142/231, Loss: 1.3253532648086548 \n",
      "Epoch:8-143/231, Loss: 1.1377382278442383 \n",
      "Epoch:8-144/231, Loss: 1.0901345014572144 \n",
      "Epoch:8-145/231, Loss: 1.2223646640777588 \n",
      "Epoch:8-146/231, Loss: 0.9401705265045166 \n",
      "Epoch:8-147/231, Loss: 0.8653599619865417 \n",
      "Epoch:8-148/231, Loss: 0.9450165629386902 \n",
      "Epoch:8-149/231, Loss: 0.8710947632789612 \n",
      "Epoch:8-150/231, Loss: 0.896743893623352 \n",
      "Epoch:8-151/231, Loss: 0.7967257499694824 \n",
      "Epoch:8-152/231, Loss: 0.7329191565513611 \n",
      "Epoch:8-153/231, Loss: 0.9721183776855469 \n",
      "Epoch:8-154/231, Loss: 1.820617437362671 \n",
      "Epoch:8-155/231, Loss: 1.959973692893982 \n",
      "Epoch:8-156/231, Loss: 1.9014561176300049 \n",
      "Epoch:8-157/231, Loss: 1.7931796312332153 \n",
      "Epoch:8-158/231, Loss: 1.5161751508712769 \n",
      "Epoch:8-159/231, Loss: 1.5558454990386963 \n",
      "Epoch:8-160/231, Loss: 1.3625974655151367 \n",
      "Epoch:8-161/231, Loss: 1.2671735286712646 \n",
      "Epoch:8-162/231, Loss: 1.3089879751205444 \n",
      "Epoch:8-163/231, Loss: 1.1899800300598145 \n",
      "Epoch:8-164/231, Loss: 1.1261541843414307 \n",
      "Epoch:8-165/231, Loss: 1.32451331615448 \n",
      "Epoch:8-166/231, Loss: 0.9981060028076172 \n",
      "Epoch:8-167/231, Loss: 1.0372222661972046 \n",
      "Epoch:8-168/231, Loss: 0.8670026063919067 \n",
      "Epoch:8-169/231, Loss: 0.7997357249259949 \n",
      "Epoch:8-170/231, Loss: 0.7884718179702759 \n",
      "Epoch:8-171/231, Loss: 0.7456705570220947 \n",
      "Epoch:8-172/231, Loss: 0.7355589270591736 \n",
      "Epoch:8-173/231, Loss: 3.0499026775360107 \n",
      "Epoch:8-174/231, Loss: 3.1579527854919434 \n",
      "Epoch:8-175/231, Loss: 2.853598117828369 \n",
      "Epoch:8-176/231, Loss: 2.8888416290283203 \n",
      "Epoch:8-177/231, Loss: 2.8379855155944824 \n",
      "Epoch:8-178/231, Loss: 2.4423623085021973 \n",
      "Epoch:8-179/231, Loss: 2.354471206665039 \n",
      "Epoch:8-180/231, Loss: 2.292508602142334 \n",
      "Epoch:8-181/231, Loss: 2.040567636489868 \n",
      "Epoch:8-182/231, Loss: 1.8016303777694702 \n",
      "Epoch:8-183/231, Loss: 1.6151610612869263 \n",
      "Epoch:8-184/231, Loss: 1.4156558513641357 \n",
      "Epoch:8-185/231, Loss: 1.223557472229004 \n",
      "Epoch:8-186/231, Loss: 1.2329692840576172 \n",
      "Epoch:8-187/231, Loss: 1.0566176176071167 \n",
      "Epoch:8-188/231, Loss: 0.8449314832687378 \n",
      "Epoch:8-189/231, Loss: 0.7202413082122803 \n",
      "Epoch:8-190/231, Loss: 0.6085038781166077 \n",
      "Epoch:8-191/231, Loss: 0.4965244233608246 \n",
      "Epoch:8-192/231, Loss: 1.3018486499786377 \n",
      "Epoch:8-193/231, Loss: 1.4246331453323364 \n",
      "Epoch:8-194/231, Loss: 1.4194879531860352 \n",
      "Epoch:8-195/231, Loss: 1.4261339902877808 \n",
      "Epoch:8-196/231, Loss: 1.2456278800964355 \n",
      "Epoch:8-197/231, Loss: 0.9779284000396729 \n",
      "Epoch:8-198/231, Loss: 1.0926692485809326 \n",
      "Epoch:8-199/231, Loss: 0.9853903651237488 \n",
      "Epoch:8-200/231, Loss: 1.3336877822875977 \n",
      "Epoch:8-201/231, Loss: 1.12838613986969 \n",
      "Epoch:8-202/231, Loss: 0.9025795459747314 \n",
      "Epoch:8-203/231, Loss: 0.8488624095916748 \n",
      "Epoch:8-204/231, Loss: 0.7997185587882996 \n",
      "Epoch:8-205/231, Loss: 0.7796266674995422 \n",
      "Epoch:8-206/231, Loss: 0.7482258677482605 \n",
      "Epoch:8-207/231, Loss: 0.8466675877571106 \n",
      "Epoch:8-208/231, Loss: 0.7529712915420532 \n",
      "Epoch:8-209/231, Loss: 0.7588793635368347 \n",
      "Epoch:8-210/231, Loss: 0.7570959329605103 \n",
      "Epoch:8-211/231, Loss: 0.6453871130943298 \n",
      "Epoch:8-212/231, Loss: 0.6284791231155396 \n",
      "Epoch:8-213/231, Loss: 2.300593852996826 \n",
      "Epoch:8-214/231, Loss: 2.7045958042144775 \n",
      "Epoch:8-215/231, Loss: 2.4763669967651367 \n",
      "Epoch:8-216/231, Loss: 2.402139186859131 \n",
      "Epoch:8-217/231, Loss: 2.222954750061035 \n",
      "Epoch:8-218/231, Loss: 2.105513572692871 \n",
      "Epoch:8-219/231, Loss: 1.7428128719329834 \n",
      "Epoch:8-220/231, Loss: 1.8480815887451172 \n",
      "Epoch:8-221/231, Loss: 1.5309474468231201 \n",
      "Epoch:8-222/231, Loss: 1.349577784538269 \n",
      "Epoch:8-223/231, Loss: 1.6061570644378662 \n",
      "Epoch:8-224/231, Loss: 1.3531802892684937 \n",
      "Epoch:8-225/231, Loss: 1.4067847728729248 \n",
      "Epoch:8-226/231, Loss: 1.2897546291351318 \n",
      "Epoch:8-227/231, Loss: 1.1314311027526855 \n",
      "Epoch:8-228/231, Loss: 1.1786439418792725 \n",
      "Epoch:8-229/231, Loss: 1.066904902458191 \n",
      "Epoch:8-230/231, Loss: 0.9742725491523743 \n",
      "--------------------------------------------------------------\n",
      "Epoch:8 completed, Total training's Loss: 296.9678707420826, Spend: 1.5675658464431763m\n",
      "Epoch:8, Acc:8.33 on Valid_set, Spend: 0.095 minutes for evaluation\n",
      "--------------------------------------------------------------\n",
      "Epoch:9-0/231, Loss: 1.714470386505127 \n",
      "Epoch:9-1/231, Loss: 1.840851068496704 \n",
      "Epoch:9-2/231, Loss: 1.9654748439788818 \n",
      "Epoch:9-3/231, Loss: 2.0305564403533936 \n",
      "Epoch:9-4/231, Loss: 1.4495478868484497 \n",
      "Epoch:9-5/231, Loss: 1.6107301712036133 \n",
      "Epoch:9-6/231, Loss: 1.4554626941680908 \n",
      "Epoch:9-7/231, Loss: 1.5730955600738525 \n",
      "Epoch:9-8/231, Loss: 1.2019176483154297 \n",
      "Epoch:9-9/231, Loss: 1.0552825927734375 \n",
      "Epoch:9-10/231, Loss: 1.0075212717056274 \n",
      "Epoch:9-11/231, Loss: 0.819506049156189 \n",
      "Epoch:9-12/231, Loss: 0.8950034379959106 \n",
      "Epoch:9-13/231, Loss: 0.8098670840263367 \n",
      "Epoch:9-14/231, Loss: 0.7530734539031982 \n",
      "Epoch:9-15/231, Loss: 0.6870543956756592 \n",
      "Epoch:9-16/231, Loss: 0.6779766082763672 \n",
      "Epoch:9-17/231, Loss: 0.6509191989898682 \n",
      "Epoch:9-18/231, Loss: 0.669554591178894 \n",
      "Epoch:9-19/231, Loss: 1.9969546794891357 \n",
      "Epoch:9-20/231, Loss: 2.6265623569488525 \n",
      "Epoch:9-21/231, Loss: 0.9231864213943481 \n",
      "Epoch:9-22/231, Loss: 0.3582625687122345 \n",
      "Epoch:9-23/231, Loss: 0.3322952687740326 \n",
      "Epoch:9-24/231, Loss: 0.35563230514526367 \n",
      "Epoch:9-25/231, Loss: 0.31677085161209106 \n",
      "Epoch:9-26/231, Loss: 0.36279046535491943 \n",
      "Epoch:9-27/231, Loss: 0.2649123966693878 \n",
      "Epoch:9-28/231, Loss: 0.23666353523731232 \n",
      "Epoch:9-29/231, Loss: 0.21160082519054413 \n",
      "Epoch:9-30/231, Loss: 0.21508988738059998 \n",
      "Epoch:9-31/231, Loss: 0.2044055312871933 \n",
      "Epoch:9-32/231, Loss: 0.20004649460315704 \n",
      "Epoch:9-33/231, Loss: 0.18141475319862366 \n",
      "Epoch:9-34/231, Loss: 0.16603264212608337 \n",
      "Epoch:9-35/231, Loss: 0.15790477395057678 \n",
      "Epoch:9-36/231, Loss: 0.14258292317390442 \n",
      "Epoch:9-37/231, Loss: 0.1364499032497406 \n",
      "Epoch:9-38/231, Loss: 0.8825312852859497 \n",
      "Epoch:9-39/231, Loss: 1.7881219387054443 \n",
      "Epoch:9-40/231, Loss: 1.5140305757522583 \n",
      "Epoch:9-41/231, Loss: 1.1826164722442627 \n",
      "Epoch:9-42/231, Loss: 1.3176823854446411 \n",
      "Epoch:9-43/231, Loss: 0.8947179317474365 \n",
      "Epoch:9-44/231, Loss: 1.0204766988754272 \n",
      "Epoch:9-45/231, Loss: 0.9482095241546631 \n",
      "Epoch:9-46/231, Loss: 0.9713490009307861 \n",
      "Epoch:9-47/231, Loss: 0.9357519149780273 \n",
      "Epoch:9-48/231, Loss: 0.9641278982162476 \n",
      "Epoch:9-49/231, Loss: 0.7472364902496338 \n",
      "Epoch:9-50/231, Loss: 0.9781689643859863 \n",
      "Epoch:9-51/231, Loss: 0.8130382895469666 \n",
      "Epoch:9-52/231, Loss: 0.7227948904037476 \n",
      "Epoch:9-53/231, Loss: 0.5829737186431885 \n",
      "Epoch:9-54/231, Loss: 0.6077015995979309 \n",
      "Epoch:9-55/231, Loss: 0.587683916091919 \n",
      "Epoch:9-56/231, Loss: 0.6072472333908081 \n",
      "Epoch:9-57/231, Loss: 1.7580779790878296 \n",
      "Epoch:9-58/231, Loss: 1.486395001411438 \n",
      "Epoch:9-59/231, Loss: 2.1939730644226074 \n",
      "Epoch:9-60/231, Loss: 2.1855435371398926 \n",
      "Epoch:9-61/231, Loss: 1.6379249095916748 \n",
      "Epoch:9-62/231, Loss: 1.5857161283493042 \n",
      "Epoch:9-63/231, Loss: 1.6449251174926758 \n",
      "Epoch:9-64/231, Loss: 1.28074312210083 \n",
      "Epoch:9-65/231, Loss: 1.162026047706604 \n",
      "Epoch:9-66/231, Loss: 1.144451379776001 \n",
      "Epoch:9-67/231, Loss: 1.1589027643203735 \n",
      "Epoch:9-68/231, Loss: 1.0325994491577148 \n",
      "Epoch:9-69/231, Loss: 0.9797090291976929 \n",
      "Epoch:9-70/231, Loss: 1.033754825592041 \n",
      "Epoch:9-71/231, Loss: 0.9948352575302124 \n",
      "Epoch:9-72/231, Loss: 0.8236545324325562 \n",
      "Epoch:9-73/231, Loss: 0.8294671773910522 \n",
      "Epoch:9-74/231, Loss: 0.7854671478271484 \n",
      "Epoch:9-75/231, Loss: 0.8045852184295654 \n",
      "Epoch:9-76/231, Loss: 0.6800093650817871 \n",
      "Epoch:9-77/231, Loss: 2.6181581020355225 \n",
      "Epoch:9-78/231, Loss: 2.2431790828704834 \n",
      "Epoch:9-79/231, Loss: 2.0248920917510986 \n",
      "Epoch:9-80/231, Loss: 1.804344892501831 \n",
      "Epoch:9-81/231, Loss: 1.791702389717102 \n",
      "Epoch:9-82/231, Loss: 1.6675689220428467 \n",
      "Epoch:9-83/231, Loss: 1.1982262134552002 \n",
      "Epoch:9-84/231, Loss: 1.175750494003296 \n",
      "Epoch:9-85/231, Loss: 1.0796988010406494 \n",
      "Epoch:9-86/231, Loss: 1.0673213005065918 \n",
      "Epoch:9-87/231, Loss: 1.1265695095062256 \n",
      "Epoch:9-88/231, Loss: 1.1082179546356201 \n",
      "Epoch:9-89/231, Loss: 0.9762008190155029 \n",
      "Epoch:9-90/231, Loss: 0.9031521081924438 \n",
      "Epoch:9-91/231, Loss: 0.8134198188781738 \n",
      "Epoch:9-92/231, Loss: 0.9991729259490967 \n",
      "Epoch:9-93/231, Loss: 1.1606158018112183 \n",
      "Epoch:9-94/231, Loss: 1.0099661350250244 \n",
      "Epoch:9-95/231, Loss: 1.0142990350723267 \n",
      "Epoch:9-96/231, Loss: 2.4948811531066895 \n",
      "Epoch:9-97/231, Loss: 2.3618271350860596 \n",
      "Epoch:9-98/231, Loss: 2.314418315887451 \n",
      "Epoch:9-99/231, Loss: 2.3846182823181152 \n",
      "Epoch:9-100/231, Loss: 2.1332218647003174 \n",
      "Epoch:9-101/231, Loss: 1.774624228477478 \n",
      "Epoch:9-102/231, Loss: 1.6063125133514404 \n",
      "Epoch:9-103/231, Loss: 1.4114934206008911 \n",
      "Epoch:9-104/231, Loss: 1.3052922487258911 \n",
      "Epoch:9-105/231, Loss: 1.2561800479888916 \n",
      "Epoch:9-106/231, Loss: 1.224589467048645 \n",
      "Epoch:9-107/231, Loss: 0.9630552530288696 \n",
      "Epoch:9-108/231, Loss: 1.09259033203125 \n",
      "Epoch:9-109/231, Loss: 1.117684245109558 \n",
      "Epoch:9-110/231, Loss: 0.9898016452789307 \n",
      "Epoch:9-111/231, Loss: 1.007730484008789 \n",
      "Epoch:9-112/231, Loss: 1.0032740831375122 \n",
      "Epoch:9-113/231, Loss: 1.109955072402954 \n",
      "Epoch:9-114/231, Loss: 1.1531329154968262 \n",
      "Epoch:9-115/231, Loss: 1.2799084186553955 \n",
      "Epoch:9-116/231, Loss: 1.7463634014129639 \n",
      "Epoch:9-117/231, Loss: 1.650526523590088 \n",
      "Epoch:9-118/231, Loss: 1.783857822418213 \n",
      "Epoch:9-119/231, Loss: 1.5801913738250732 \n",
      "Epoch:9-120/231, Loss: 1.6519793272018433 \n",
      "Epoch:9-121/231, Loss: 1.4781911373138428 \n",
      "Epoch:9-122/231, Loss: 1.3243167400360107 \n",
      "Epoch:9-123/231, Loss: 1.533512830734253 \n",
      "Epoch:9-124/231, Loss: 1.199049472808838 \n",
      "Epoch:9-125/231, Loss: 1.120046615600586 \n",
      "Epoch:9-126/231, Loss: 1.3674548864364624 \n",
      "Epoch:9-127/231, Loss: 1.2473111152648926 \n",
      "Epoch:9-128/231, Loss: 1.0827910900115967 \n",
      "Epoch:9-129/231, Loss: 1.0871608257293701 \n",
      "Epoch:9-130/231, Loss: 1.0280966758728027 \n",
      "Epoch:9-131/231, Loss: 1.1252245903015137 \n",
      "Epoch:9-132/231, Loss: 1.0936423540115356 \n",
      "Epoch:9-133/231, Loss: 0.9845260381698608 \n",
      "Epoch:9-134/231, Loss: 1.185533046722412 \n",
      "Epoch:9-135/231, Loss: 1.9466100931167603 \n",
      "Epoch:9-136/231, Loss: 2.140420913696289 \n",
      "Epoch:9-137/231, Loss: 1.8558475971221924 \n",
      "Epoch:9-138/231, Loss: 1.7939598560333252 \n",
      "Epoch:9-139/231, Loss: 1.5494663715362549 \n",
      "Epoch:9-140/231, Loss: 1.4855868816375732 \n",
      "Epoch:9-141/231, Loss: 1.4098786115646362 \n",
      "Epoch:9-142/231, Loss: 1.4235230684280396 \n",
      "Epoch:9-143/231, Loss: 1.0838018655776978 \n",
      "Epoch:9-144/231, Loss: 0.9669206142425537 \n",
      "Epoch:9-145/231, Loss: 0.9857057929039001 \n",
      "Epoch:9-146/231, Loss: 0.9700939655303955 \n",
      "Epoch:9-147/231, Loss: 1.0032132863998413 \n",
      "Epoch:9-148/231, Loss: 0.755366325378418 \n",
      "Epoch:9-149/231, Loss: 0.802372932434082 \n",
      "Epoch:9-150/231, Loss: 0.889942467212677 \n",
      "Epoch:9-151/231, Loss: 0.7698798179626465 \n",
      "Epoch:9-152/231, Loss: 0.7790238857269287 \n",
      "Epoch:9-153/231, Loss: 1.2685012817382812 \n",
      "Epoch:9-154/231, Loss: 2.36663818359375 \n",
      "Epoch:9-155/231, Loss: 2.4271862506866455 \n",
      "Epoch:9-156/231, Loss: 2.1309938430786133 \n",
      "Epoch:9-157/231, Loss: 1.8590151071548462 \n",
      "Epoch:9-158/231, Loss: 1.809162974357605 \n",
      "Epoch:9-159/231, Loss: 1.691392183303833 \n",
      "Epoch:9-160/231, Loss: 1.606029748916626 \n",
      "Epoch:9-161/231, Loss: 1.320549488067627 \n",
      "Epoch:9-162/231, Loss: 1.2910815477371216 \n",
      "Epoch:9-163/231, Loss: 1.3826913833618164 \n",
      "Epoch:9-164/231, Loss: 1.143225908279419 \n",
      "Epoch:9-165/231, Loss: 1.1009944677352905 \n",
      "Epoch:9-166/231, Loss: 1.2472484111785889 \n",
      "Epoch:9-167/231, Loss: 1.0106011629104614 \n",
      "Epoch:9-168/231, Loss: 1.2281068563461304 \n",
      "Epoch:9-169/231, Loss: 1.144728183746338 \n",
      "Epoch:9-170/231, Loss: 0.9727912545204163 \n",
      "Epoch:9-171/231, Loss: 0.9149696826934814 \n",
      "Epoch:9-172/231, Loss: 0.908395528793335 \n",
      "Epoch:9-173/231, Loss: 2.261471748352051 \n",
      "Epoch:9-174/231, Loss: 2.0022575855255127 \n",
      "Epoch:9-175/231, Loss: 1.677804708480835 \n",
      "Epoch:9-176/231, Loss: 1.719170093536377 \n",
      "Epoch:9-177/231, Loss: 1.3375825881958008 \n",
      "Epoch:9-178/231, Loss: 1.0839409828186035 \n",
      "Epoch:9-179/231, Loss: 0.7414228916168213 \n",
      "Epoch:9-180/231, Loss: 0.7822606563568115 \n",
      "Epoch:9-181/231, Loss: 0.7421953678131104 \n",
      "Epoch:9-182/231, Loss: 0.7186041474342346 \n",
      "Epoch:9-183/231, Loss: 0.7101139426231384 \n",
      "Epoch:9-184/231, Loss: 0.6918511390686035 \n",
      "Epoch:9-185/231, Loss: 0.6036784052848816 \n",
      "Epoch:9-186/231, Loss: 0.5656785368919373 \n",
      "Epoch:9-187/231, Loss: 0.42953431606292725 \n",
      "Epoch:9-188/231, Loss: 0.4196629226207733 \n",
      "Epoch:9-189/231, Loss: 0.3793030381202698 \n",
      "Epoch:9-190/231, Loss: 0.39041709899902344 \n",
      "Epoch:9-191/231, Loss: 0.3967120349407196 \n",
      "Epoch:9-192/231, Loss: 1.4994964599609375 \n",
      "Epoch:9-193/231, Loss: 1.9440197944641113 \n",
      "Epoch:9-194/231, Loss: 1.7183401584625244 \n",
      "Epoch:9-195/231, Loss: 1.7657289505004883 \n",
      "Epoch:9-196/231, Loss: 1.5257905721664429 \n",
      "Epoch:9-197/231, Loss: 1.3574515581130981 \n",
      "Epoch:9-198/231, Loss: 1.3360576629638672 \n",
      "Epoch:9-199/231, Loss: 1.1465883255004883 \n",
      "Epoch:9-200/231, Loss: 1.430387258529663 \n",
      "Epoch:9-201/231, Loss: 1.2610957622528076 \n",
      "Epoch:9-202/231, Loss: 0.7930216193199158 \n",
      "Epoch:9-203/231, Loss: 0.8706318140029907 \n",
      "Epoch:9-204/231, Loss: 0.8794975876808167 \n",
      "Epoch:9-205/231, Loss: 0.871741533279419 \n",
      "Epoch:9-206/231, Loss: 0.8726018071174622 \n",
      "Epoch:9-207/231, Loss: 1.0028655529022217 \n",
      "Epoch:9-208/231, Loss: 0.824138343334198 \n",
      "Epoch:9-209/231, Loss: 0.6709386110305786 \n",
      "Epoch:9-210/231, Loss: 0.7960933446884155 \n",
      "Epoch:9-211/231, Loss: 0.6902060508728027 \n",
      "Epoch:9-212/231, Loss: 0.6678915023803711 \n",
      "Epoch:9-213/231, Loss: 1.727858543395996 \n",
      "Epoch:9-214/231, Loss: 2.0623879432678223 \n",
      "Epoch:9-215/231, Loss: 2.1997196674346924 \n",
      "Epoch:9-216/231, Loss: 2.0751075744628906 \n",
      "Epoch:9-217/231, Loss: 1.7727183103561401 \n",
      "Epoch:9-218/231, Loss: 1.817021369934082 \n",
      "Epoch:9-219/231, Loss: 1.491363286972046 \n",
      "Epoch:9-220/231, Loss: 1.6337392330169678 \n",
      "Epoch:9-221/231, Loss: 1.3396799564361572 \n",
      "Epoch:9-222/231, Loss: 1.3577417135238647 \n",
      "Epoch:9-223/231, Loss: 1.3444709777832031 \n",
      "Epoch:9-224/231, Loss: 1.398853063583374 \n",
      "Epoch:9-225/231, Loss: 1.2252001762390137 \n",
      "Epoch:9-226/231, Loss: 1.2926785945892334 \n",
      "Epoch:9-227/231, Loss: 1.2381300926208496 \n",
      "Epoch:9-228/231, Loss: 1.1484711170196533 \n",
      "Epoch:9-229/231, Loss: 1.1033263206481934 \n",
      "Epoch:9-230/231, Loss: 0.9849505424499512 \n",
      "--------------------------------------------------------------\n",
      "Epoch:9 completed, Total training's Loss: 276.79105189442635, Spend: 1.582742722829183m\n",
      "Epoch:9, Acc:8.33 on Valid_set, Spend: 0.098 minutes for evaluation\n",
      "--------------------------------------------------------------\n",
      "Epoch:9, Acc:8.33 on Test_set, Spend: 0.094 minutes for evaluation\n",
      "--------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.08333333333333333"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_train_steps = int(len(train) / batch_size / gradient_accumulation_steps * total_train_epochs)\n",
    "global_step_th = int(len(train) / batch_size / gradient_accumulation_steps * start_epoch)\n",
    "# for epoch in trange(start_epoch, total_train_epochs, desc=\"Epoch\"):\n",
    "for epoch in range(start_epoch, total_train_epochs):\n",
    "    tr_loss = 0\n",
    "    train_start = time.time()\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    # for step, batch in enumerate(tqdm(train_dataloader, desc=\"Iteration\")):\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        \n",
    "        image = batch['image'].to(device)\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        bbox = batch['bbox'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        token_type_ids = batch['token_type_ids'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model(image= image, input_ids=input_ids, bbox=bbox, attention_mask=attention_mask, token_type_ids=token_type_ids,\n",
    "                        labels=labels)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        if gradient_accumulation_steps > 1:\n",
    "            loss = loss / gradient_accumulation_steps\n",
    "\n",
    "        loss.backward()\n",
    "        tr_loss += loss.item()\n",
    "\n",
    "        if (step + 1) % gradient_accumulation_steps == 0:\n",
    "            # modify learning rate with special warm up BERT uses\n",
    "            lr_this_step = learning_rate0\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = lr_this_step\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            global_step_th += 1\n",
    "\n",
    "        print(\"Epoch:{}-{}/{}, Loss: {} \".format(epoch, step, len(train_dataloader), loss.item()))\n",
    "\n",
    "    print('--------------------------------------------------------------')\n",
    "    print(\"Epoch:{} completed, Total training's Loss: {}, Spend: {}m\".format(epoch, tr_loss, (time.time() - train_start) / 60.0))\n",
    "    valid_acc = evaluate(model, test_dataloader, batch_size, epoch, 'Valid_set')\n",
    "    # Save a checkpoint\n",
    "    if valid_acc > valid_acc_prev:\n",
    "        # model_to_save = model.module if hasattr(model, 'module') else model  # Only save the model it-self\n",
    "        torch.save({'epoch': epoch, 'model_state': model.state_dict(), 'valid_acc': valid_acc}, os.path.join(output_dir, 'layoutETD1k.pt'))\n",
    "        valid_acc_prev = valid_acc\n",
    "\n",
    "evaluate(model, test_dataloader, batch_size, total_train_epochs-1, 'Test_set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9351eb6e-f6c7-4181-8687-983b630c12ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded the model, epoch: 7 valid acc: 0.08888888888888889\n",
      "Epoch:7, Acc:8.89 on Test_set, Spend: 0.096 minutes for evaluation\n",
      "--------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.08888888888888889"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#%%\n",
    "'''\n",
    "Test_set prediction using the best epoch of layoutlmv2 model\n",
    "'''\n",
    "checkpoint = torch.load(output_dir+'/layoutETD1k.pt', map_location='cuda')\n",
    "epoch = checkpoint['epoch']\n",
    "valid_acc_prev = checkpoint['valid_acc']\n",
    "#valid_f1_prev = checkpoint['valid_f1']\n",
    "model = LayoutLMv2ForSequenceClassification.from_pretrained(\n",
    "    model_scale, state_dict=checkpoint['model_state'], num_labels=len(labels_test))\n",
    "\n",
    "model.to(device)\n",
    "print('Loaded the model, epoch:',checkpoint['epoch'],'valid acc:', checkpoint['valid_acc'])\n",
    "\n",
    "model.to(device)\n",
    "# evaluate(model, train_dataloader, batch_size, total_train_epochs-1, 'Train_set')\n",
    "evaluate(model, test_dataloader, batch_size, epoch, 'Test_set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9d93d674-6ccb-424d-b9b3-b095dfe9b2a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded the model, epoch: 0 valid acc: 0.15267175572519084\n",
      "Epoch:0, Acc:8.33 on Test_set, Spend: 0.094 minutes for evaluation\n",
      "--------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.08333333333333333"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#%%\n",
    "'''\n",
    "Test_set prediction using the best epoch of layoutlmv2 model\n",
    "'''\n",
    "checkpoint = torch.load(output_dir+'/layoutETD3k.pt', map_location='cuda')\n",
    "epoch = checkpoint['epoch']\n",
    "valid_acc_prev = checkpoint['valid_acc']\n",
    "#valid_f1_prev = checkpoint['valid_f1']\n",
    "model = LayoutLMv2ForSequenceClassification.from_pretrained(\n",
    "    model_scale, state_dict=checkpoint['model_state'], num_labels=len(labels_test))\n",
    "\n",
    "model.to(device)\n",
    "print('Loaded the model, epoch:',checkpoint['epoch'],'valid acc:', checkpoint['valid_acc'])\n",
    "\n",
    "model.to(device)\n",
    "# evaluate(model, train_dataloader, batch_size, total_train_epochs-1, 'Train_set')\n",
    "evaluate(model, test_dataloader, batch_size, epoch, 'Test_set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "c46f6323-b9e0-4eb1-b1bd-0ff62a8ef3c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=298x433 at 0x7F570C1AA890>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = Image.open(\"/home/mchou001/test/403/1.png\")\n",
    "image = image.convert(\"RGB\")\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "bf4a1ce6-9495-4d78-9200-ac1e9f42665b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare image for the model\n",
    "\n",
    "encoded_inputs = processor(image, return_tensors=\"pt\")\n",
    "\n",
    "# # make sure all keys of encoded_inputs are on the same device as the model\n",
    "for k,v in encoded_inputs.items():\n",
    "    encoded_inputs[k] = v.to(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "8197fed6-d47a-46c4-af7f-5cbc28fd805c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward pass\n",
    "outputs = model(**encoded_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "5bd8d567-e087-45a4-b8a9-852ca5b97437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 12])\n"
     ]
    }
   ],
   "source": [
    "logits = outputs.logits\n",
    "print(logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "1d3d58e6-5f34-4e0b-8443-89ae4a8012ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: Label-Appendices\n"
     ]
    }
   ],
   "source": [
    "predicted_class_idx = logits.argmax(-1).item()\n",
    "print(\"Predicted class:\", id2label_test[predicted_class_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9535bf86-1039-4035-910d-dfbca73bd40e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
