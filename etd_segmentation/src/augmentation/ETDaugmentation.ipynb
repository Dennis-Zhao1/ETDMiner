{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import glob\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import ast\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sentence_splitter import SentenceSplitter, split_text_into_sentences\n",
    "from transformers import PegasusForConditionalGeneration, PegasusTokenizerFast, PegasusTokenizer\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "import textwrap\n",
    "from PIL import Image, ImageDraw, ImageFont"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessor:\n",
    "    \n",
    "    # The function below will sort the files in numberical order \n",
    "    def numericalSort(self, value):\n",
    "        self.numbers = re.compile(r'(\\d+)')\n",
    "        self.value = value\n",
    "        parts = self.numbers.split(self.value)\n",
    "        parts[1::2] = map(int, parts[1::2])\n",
    "        return parts\n",
    "    \n",
    "    ## the function will return parsed lines from json files\n",
    "    def json_text_parser(self, json_files):\n",
    "        self.json_files = json_files\n",
    "        counter = 0\n",
    "        list_line = []\n",
    "        for filename in self.json_files:\n",
    "            with open(filename, encoding='utf-8', mode='r') as file:\n",
    "                data = json.loads(file.read())\n",
    "                counter = counter + 1\n",
    "                _text = []\n",
    "                for i in range(len(data)):\n",
    "                    text = data[i].get(\"Line_and_BB\")\n",
    "                    _text.append(text)\n",
    "                for i, data in enumerate(_text):\n",
    "                    line_text = [text['Line'].strip('\\n') for text in data]\n",
    "                    list_line.append(line_text)\n",
    "        \n",
    "        return list_line\n",
    "    \n",
    "    ## the function will return parsed bounding box information from json files\n",
    "    def json_bbox_parser(self, json_files):\n",
    "        self.json_files = json_files\n",
    "        counter = 0\n",
    "        list_bbox = []\n",
    "        for filename in self.json_files:\n",
    "            with open(filename, encoding='utf-8', mode='r') as file:\n",
    "                data = json.loads(file.read())\n",
    "                counter = counter + 1\n",
    "                _text = []\n",
    "                for i in range(len(data)):\n",
    "                    text = data[i].get(\"Line_and_BB\")\n",
    "                    _text.append(text)\n",
    "                for i, data in enumerate(_text):\n",
    "                    bbox_text = [bbox['Bounding Box'] for bbox in data]\n",
    "                    list_bbox.append(bbox_text)\n",
    "        \n",
    "        return list_bbox\n",
    "    \n",
    "    ## the function will convert label.txt file to .csv file and includes a header\n",
    "    def label_process(self, label_text_file):\n",
    "        self.label_text_file = label_text_file\n",
    "        dfList=[]\n",
    "        colname=['file_id', 'labels']\n",
    "        df = pd.read_csv(label_text_file, sep = \"\\t\", header = None)\n",
    "        dfList.append(df)\n",
    "        concatDf = pd.concat(dfList, axis =0)\n",
    "        concatDf.columns=colname\n",
    "        concatDf.to_csv(\"labels.csv\",index = None, encoding = 'utf-8')\n",
    "    \n",
    "        return concatDf\n",
    "    \n",
    "    '''\n",
    "    We are using 'ast' module for preprocessing task of the class labels.\n",
    "    The ast module helps Python applications to process trees of the Python abstract syntax grammar.\n",
    "    We utilized the ast module to grammatically identify the sentences and applied 'sentence splitter' module \n",
    "    on paragraphs to get a list of sentences.\n",
    "    The preprocessing functions for each classes returns a list.\n",
    "    '''\n",
    "    \n",
    "    def preprocess_dedication(self, dedication_):\n",
    "        self.dedication_ = dedication_\n",
    "        text_dedication = ast.literal_eval(self.dedication_)\n",
    "        text_dedication = (\" \").join(text_dedication)\n",
    "        splitter = SentenceSplitter(language='en')\n",
    "        dedication_list = splitter.split(text_dedication)\n",
    "        return dedication_list    \n",
    "    \n",
    "    def preprocess_ack(self, acknowledgement_):\n",
    "        self.acknowledgement_ = acknowledgement_\n",
    "        text_ack = ast.literal_eval(self.acknowledgement_)\n",
    "        text_ack = (\" \").join(text_ack)\n",
    "        splitter = SentenceSplitter(language='en')\n",
    "        ack_list = splitter.split(text_ack)\n",
    "        return ack_list\n",
    "    \n",
    "    def preprocess_gabs(self, general_abs):\n",
    "        self.general_abs = general_abs\n",
    "        splitter = SentenceSplitter(language='en')\n",
    "        gabs_list = splitter.split(self.general_abs)\n",
    "        return gabs_list\n",
    "    \n",
    "    def preprocess_abs(self, abstract_):\n",
    "        self.abstract_ = abstract_\n",
    "        text_abstract = ast.literal_eval(self.abstract_)\n",
    "        text_abstract = (\" \").join(text_abstract)\n",
    "        splitter = SentenceSplitter(language='en')\n",
    "        abstract_list = splitter.split(text_abstract)\n",
    "        return abstract_list  \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## loading the paraphrasing model\n",
    "\n",
    "#references: https://arxiv.org/abs/1912.08777\n",
    "\n",
    "model_name = 'tuner007/pegasus_paraphrase'\n",
    "torch_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
    "model = PegasusForConditionalGeneration.from_pretrained(model_name).to(torch_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Augmentation:\n",
    "    \n",
    "    def get_paraphrased_sentences(self, input_text, num_return_sequences):\n",
    "        self.input_text = input_text\n",
    "        self.num_return_sequences = num_return_sequences        \n",
    "        batch = tokenizer.prepare_seq2seq_batch([self.input_text],truncation=True,padding='longest',max_length=60, return_tensors=\"pt\").to(torch_device)\n",
    "        translated = model.generate(**batch,max_length=60,num_beams=10, num_return_sequences=self.num_return_sequences, temperature=1.5)\n",
    "        paraphrased_text = tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
    "        return paraphrased_text       \n",
    "    \n",
    "    def paraphrased_text(self, class_text_list):\n",
    "        self.class_text_list = class_text_list\n",
    "        phrases_text = []\n",
    "        for ele in self.class_text_list:\n",
    "            phrase = self.get_paraphrased_sentences(ele, 1)\n",
    "            phrases_text.append(phrase)\n",
    "        \n",
    "        para_phrases = [' '.join(x) for x in phrases_text]\n",
    "        paraphrase_ = [' '.join(x for x in para_phrases)] ## combine  the splitted lists into a paragraph\n",
    "        paraphrase_text = str(paraphrase_).strip('[]').strip(\"'\")\n",
    "        paraphrase_strip = paraphrase_text.strip('\"\"')\n",
    "        return paraphrase_strip\n",
    "    \n",
    "    def wrap_text(self, text):\n",
    "        self.text = text\n",
    "        new_phrase = textwrap.wrap(self.text, width=90)\n",
    "        string = ''\n",
    "        for ele in new_phrase[0:]:\n",
    "            string = string + ele + '\\n'\n",
    "        return string       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'numbers' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-050deccd572a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0metd_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/home/mchou001/etds_json/*.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumericalSort\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0metd_lines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson_text_parser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metd_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-bae3280b8a51>\u001b[0m in \u001b[0;36mnumericalSort\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mnumericalSort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumbers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumbers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumbers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mparts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'numbers' is not defined"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    parser = Preprocessor()\n",
    "    \n",
    "    \n",
    "    etd_path = sorted(glob.glob('/home/mchou001/etds_json/*.json'), key = parser.numericalSort)\n",
    "    \n",
    "    etd_lines = parser.json_text_parser(etd_path)\n",
    "    labels_ = parser.label_process(\"labels.txt\")\n",
    "    \n",
    "    etd_label = pd.read_csv(\"labels.csv\")\n",
    "    etd_label = labels_['labels']\n",
    "    file_id = labels_['file_id']\n",
    "    \n",
    "    res_list = [list(item) for item in list(zip(file_id, etd_lines, etd_label))]\n",
    "    dataframe = pd.DataFrame(res_list, columns = ['file_idx', 'text', 'class'])\n",
    "    dataframe.to_csv('ETD_aug.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
