{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipynb.fs.full.ETDaugmentation import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This function is taking all the acknowledgement text, processing it, and then calling paraphrasing model to\n",
    "## perform text based Augmentation\n",
    "\n",
    "def list_of_phrases(phrases):\n",
    "    \n",
    "    phrases_list = []\n",
    "    \n",
    "    for text in phrases_dedication:\n",
    "        dedication = parser.preprocess_dedication(text)\n",
    "        dedication_paraphrase = augmentation.paraphrased_text(dedication)\n",
    "        dedication_list = [dedication_paraphrase]\n",
    "        phrases_list.append(dedication_list)\n",
    "    \n",
    "    return phrases_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This function is taking all the augmented text and wrapping it so that it can fit into the page\n",
    "## it also adds heading to the top of the page\n",
    "\n",
    "def list_of_dedication(dedication_text):\n",
    "    list_dedication = []\n",
    "    for text in dedication_text:\n",
    "        for row in text:\n",
    "            dedication_text_wrap = augmentation.wrap_text(row)\n",
    "            dedication_title = \"\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tDedication\\n\" + dedication_text_wrap\n",
    "            list_dedication.append(dedication_title)\n",
    "    \n",
    "    return list_dedication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "W,H = (2360, 3200)\n",
    "def text_on_img(fileName, text, size):\n",
    "    font = ImageFont.truetype('NimbusMonoPS-Regular.otf', size)\n",
    "    image = Image.new(mode = \"RGB\", size = (W, H), color = \"white\")\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    w, h = draw.textsize(text)\n",
    "    \n",
    "    ## Adjust the texual position in a page and draw the text on a image\n",
    "    '''\n",
    "    dedication -- 10 width and height 6; \n",
    "    ack -- 15 width and 6 height; \n",
    "    gabs -- 13 width and height 16\n",
    "    abstract -- 20 width and height 6\n",
    "    \n",
    "    '''\n",
    "    draw.text(((W-w)/10,(H-h)/6), text, font=font, fill=(0,0,0), spacing=60)\n",
    "    image.save(fileName)\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This function is calling text_on_image function and save the image to the directory\n",
    "def dedication_aug(ack_text_title):\n",
    "    for n, row in enumerate(ack_text_title):\n",
    "        fileName = (\"/home/mchou001/Label-Dedication/aug_text{}.png\".format(n))\n",
    "        save_image = text_on_img(fileName, row, 40) ## dedication -- 43, ack and abstract - 40, gabs -- 38\n",
    "    \n",
    "    return save_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mchou001/.local/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:3524: FutureWarning: \n",
      "`prepare_seq2seq_batch` is deprecated and will be removed in version 5 of HuggingFace Transformers. Use the regular\n",
      "`__call__` method to prepare your inputs and the tokenizer under the `as_target_tokenizer` context manager to prepare\n",
      "your targets.\n",
      "\n",
      "Here is a short example:\n",
      "\n",
      "model_inputs = tokenizer(src_texts, ...)\n",
      "with tokenizer.as_target_tokenizer():\n",
      "    labels = tokenizer(tgt_texts, ...)\n",
      "model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
      "\n",
      "See the documentation of your specific tokenizer for more details on the specific arguments to the tokenizer of choice.\n",
      "For a more complete example, see the implementation of `prepare_seq2seq_batch`.\n",
      "\n",
      "  warnings.warn(formatted_warning, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 µs, sys: 1e+03 ns, total: 5 µs\n",
      "Wall time: 11.4 µs\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    parser = Preprocessor()\n",
    "    augmentation = Augmentation()\n",
    "    \n",
    "    etd_data = pd.read_csv('ETD_aug.csv')\n",
    "    etd_data.set_index(\"class\", inplace = True)\n",
    "\n",
    "    label_dedication = etd_data.loc[\"Label-Dedication\"]\n",
    "    phrases_dedication = label_dedication['text']\n",
    "    \n",
    "    dedication = list_of_phrases(phrases_dedication)\n",
    "    \n",
    "    dedication_text = list_of_dedication(dedication)\n",
    "    \n",
    "    dedication_text_aug = dedication_aug(dedication_text)\n",
    "    \n",
    "    %%time \n",
    "    [ x**2 for x in range(10000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/share/fonts\n"
     ]
    }
   ],
   "source": [
    "## To change the fonts check the available font list in the following directory\n",
    "#cd /usr/share/fonts/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
