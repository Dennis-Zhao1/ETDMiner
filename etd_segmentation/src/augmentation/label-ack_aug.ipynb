{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipynb.fs.full.ETDaugmentation import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This function is taking all the acknowledgement text, processing it, and then calling paraphrasing model to\n",
    "## perform text based Augmentation\n",
    "\n",
    "def list_of_phrases(phrases):\n",
    "    \n",
    "    phrases_list = []\n",
    "    \n",
    "    for text in phrases:\n",
    "        ack = parser.preprocess_ack(text)\n",
    "        ack_paraphrase = augmentation.paraphrased_text(ack)\n",
    "        ack_list = [ack_paraphrase]\n",
    "        phrases_list.append(ack_list)\n",
    "    \n",
    "    return phrases_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This function is taking all the augmented text and wrapping it so that it can fit into the page\n",
    "## it also adds heading to the top of the page\n",
    "\n",
    "def list_of_ack(ack_text):\n",
    "    list_ack = []\n",
    "    for text in ack_text:\n",
    "        for row in text:\n",
    "            ack_text_wrap = augmentation.wrap_text(row)\n",
    "            ack_title = \"\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tACKNOWLEDGEMENTS\\n\" + ack_text_wrap\n",
    "            list_ack.append(ack_title)\n",
    "    \n",
    "    return list_ack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This function is taking all the text and saving it on an empty image\n",
    "## we are choosing text font and text size \n",
    "\n",
    "W,H = (2360, 3200)\n",
    "def text_on_img(fileName, text, size):\n",
    "    font = ImageFont.truetype('NimbusMonoPS-Bold.otf', size)\n",
    "    image = Image.new(mode = \"RGB\", size = (W, H), color = \"white\")\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    w, h = draw.textsize(text)\n",
    "    \n",
    "    ## Adjust the texual position in a page and draw the text on a image\n",
    "    '''\n",
    "    dedication -- 10 width and height 6; \n",
    "    ack -- 15 width and 6 height; \n",
    "    gabs -- 13 width and height 16\n",
    "    abstract -- 20 width and height 6\n",
    "    \n",
    "    '''\n",
    "    draw.text(((W-w)/15,(H-h)/6), text, font=font, fill=(0,0,0), spacing=60)\n",
    "    image.save(fileName)\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This function is calling text_on_image function and save the image to the directory\n",
    "def ack_aug(ack_text_title):\n",
    "    for n, row in enumerate(ack_text_title):\n",
    "        fileName = (\"/home/mchou001/Label-Acknowledgement/aug_text{}.png\".format(n))\n",
    "        save_image = text_on_img(fileName, row, 40) ## dedication -- 43, ack and abstract - 40, gabs -- 38\n",
    "    \n",
    "    return save_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mchou001/.local/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:3524: FutureWarning: \n",
      "`prepare_seq2seq_batch` is deprecated and will be removed in version 5 of HuggingFace Transformers. Use the regular\n",
      "`__call__` method to prepare your inputs and the tokenizer under the `as_target_tokenizer` context manager to prepare\n",
      "your targets.\n",
      "\n",
      "Here is a short example:\n",
      "\n",
      "model_inputs = tokenizer(src_texts, ...)\n",
      "with tokenizer.as_target_tokenizer():\n",
      "    labels = tokenizer(tgt_texts, ...)\n",
      "model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
      "\n",
      "See the documentation of your specific tokenizer for more details on the specific arguments to the tokenizer of choice.\n",
      "For a more complete example, see the implementation of `prepare_seq2seq_batch`.\n",
      "\n",
      "  warnings.warn(formatted_warning, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    parser = Preprocessor()\n",
    "    augmentation = Augmentation()\n",
    "    \n",
    "    etd_data = pd.read_csv('ETD_aug.csv')\n",
    "    etd_data.set_index(\"class\", inplace = True)\n",
    "\n",
    "    label_ack = etd_data.loc[\"Label-Acknowledgement\"]\n",
    "    phrases_ack = label_ack['text']\n",
    "    \n",
    "    ack = list_of_phrases(phrases_ack)\n",
    "    \n",
    "    ack_text = list_of_ack(ack)\n",
    "    \n",
    "    ack_text_aug = ack_aug(ack_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File count: 543\n"
     ]
    }
   ],
   "source": [
    "dir_path = '/home/mchou001/Label-Acknowledgement/'\n",
    "count = 0\n",
    "# Iterate directory\n",
    "for path in os.listdir(dir_path):\n",
    "    # check if current path is a file\n",
    "    if os.path.isfile(os.path.join(dir_path, path)):\n",
    "        count += 1\n",
    "print('File count:', count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!zip -r aug_image.zip Label-Acknowledgement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
